{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RL-IDS Adaptive System A Reinforcement Learning-driven Adaptive Intrusion Detection System with real-time threat detection capabilities using Deep Q-Network (DQN) agents trained on CICIDS2017 dataset. Overview RL-IDS is an advanced network security system that combines reinforcement learning with intrusion detection to provide adaptive, real-time threat detection. The system uses Deep Q-Network (DQN) agents trained on the CICIDS2017 dataset to classify network traffic and detect various types of attacks including DoS, DDoS, PortScan, Brute Force, XSS, SQL Injection, and Infiltration attacks. Key Features Real-time Network Monitoring - Monitor live network traffic with packet-level analysis using Scapy Deep Q-Network (DQN) Models - Trained RL agents with configurable architectures and optimization techniques REST API - FastAPI-based service for real-time predictions and model information CICIDS2017 Feature Extraction - Extract 78 standardized network flow features from live traffic Multiple Monitoring Modes - Network interface monitoring and website-specific traffic analysis Advanced Training Pipeline - Curriculum learning, early stopping, learning rate scheduling Comprehensive Evaluation - Detailed performance metrics, confusion matrices, and prediction analysis Architecture The system consists of several key components: RL Agents ( rl_ids/agents/ ): DQN implementation with configurable network architectures Training Environment ( rl_ids/environments/ ): Gymnasium-compatible environment for IDS training Data Processing ( rl_ids/make_dataset.py ): CICIDS2017 dataset preprocessing and normalization FastAPI Service ( api/ ): Production-ready API for real-time threat detection Real-time Monitors ( network_monitor.py , website_monitor.py ): Live traffic analysis tools Quick Start Prerequisites Python 3.9+ Network capture permissions (sudo for packet capture) CICIDS2017 dataset (place CSV files in data/raw/ ) Installation # Install dependencies pip install -r requirements.txt # Process CICIDS2017 dataset python -m rl_ids.make_dataset # Train DQN model python -m rl_ids.modeling.train # Evaluate trained model python -m rl_ids.modeling.evaluate Basic Usage Start the API server: python run_api.py Monitor network interface: sudo python network_monitor.py wlan0 Monitor specific website: python website_monitor.py example.com Supported Attack Types The system can detect the following attack types from the CICIDS2017 dataset: Benign Traffic - Normal network activity DoS Attacks - DoS Hulk, DoS Slowloris, DoS Slowhttptest, DoS GoldenEye DDoS Attacks - Distributed Denial of Service Port Scan - Network reconnaissance attacks Brute Force - FTP-Patator, SSH-Patator Web Attacks - SQL Injection, XSS, Brute Force Bot - Botnet traffic Infiltration - Network infiltration attacks Heartbleed - SSL/TLS vulnerability exploitation Project Structure rl_ids/ \u251c\u2500\u2500 rl_ids/ # Core RL-IDS modules \u2502 \u251c\u2500\u2500 agents/ # DQN agent implementation \u2502 \u2502 \u2514\u2500\u2500 dqn_agent.py # DQN agent with training and inference \u2502 \u251c\u2500\u2500 environments/ # Training environments \u2502 \u2502 \u2514\u2500\u2500 ids_env.py # IDS detection Gymnasium environment \u2502 \u251c\u2500\u2500 modeling/ # Training and evaluation pipeline \u2502 \u2502 \u251c\u2500\u2500 train.py # Enhanced DQN training with curriculum learning \u2502 \u2502 \u2514\u2500\u2500 evaluate.py # Comprehensive model evaluation \u2502 \u251c\u2500\u2500 config.py # Project configuration and paths \u2502 \u251c\u2500\u2500 make_dataset.py # CICIDS2017 data preprocessing \u2502 \u2514\u2500\u2500 plots.py # Visualization utilities \u251c\u2500\u2500 api/ # FastAPI application \u2502 \u251c\u2500\u2500 main.py # FastAPI application with endpoints \u2502 \u251c\u2500\u2500 models.py # Pydantic request/response models \u2502 \u251c\u2500\u2500 services.py # Prediction service implementation \u2502 \u251c\u2500\u2500 client.py # API client for testing \u2502 \u2514\u2500\u2500 config.py # API configuration \u251c\u2500\u2500 models/ # Trained model files \u2502 \u251c\u2500\u2500 dqn_model_best.pt # Best performing model \u2502 \u251c\u2500\u2500 dqn_model_final.pt # Final training epoch model \u2502 \u2514\u2500\u2500 episodes/ # Episodic model checkpoints \u251c\u2500\u2500 data/ # Dataset storage \u2502 \u251c\u2500\u2500 raw/ # Original CICIDS2017 CSV files \u2502 \u2514\u2500\u2500 processed/ # Preprocessed train/val/test splits \u251c\u2500\u2500 reports/ # Training and evaluation reports \u2502 \u251c\u2500\u2500 training_metrics.csv # Training progress metrics \u2502 \u251c\u2500\u2500 evaluation_*.csv # Evaluation results \u2502 \u2514\u2500\u2500 figures/ # Generated plots and visualizations \u251c\u2500\u2500 logs/ # Runtime logs \u251c\u2500\u2500 network_monitor.py # Real-time network interface monitoring \u251c\u2500\u2500 website_monitor.py # Website-specific traffic monitoring \u2514\u2500\u2500 run_api.py # API server startup script Getting Started Visit the User Guide for detailed installation and usage instructions, API Reference for integration details, or Modules for technical implementation details. License This project is licensed under the terms specified in LICENSE .","title":"Home"},{"location":"#rl-ids-adaptive-system","text":"A Reinforcement Learning-driven Adaptive Intrusion Detection System with real-time threat detection capabilities using Deep Q-Network (DQN) agents trained on CICIDS2017 dataset.","title":"RL-IDS Adaptive System"},{"location":"#overview","text":"RL-IDS is an advanced network security system that combines reinforcement learning with intrusion detection to provide adaptive, real-time threat detection. The system uses Deep Q-Network (DQN) agents trained on the CICIDS2017 dataset to classify network traffic and detect various types of attacks including DoS, DDoS, PortScan, Brute Force, XSS, SQL Injection, and Infiltration attacks.","title":"Overview"},{"location":"#key-features","text":"Real-time Network Monitoring - Monitor live network traffic with packet-level analysis using Scapy Deep Q-Network (DQN) Models - Trained RL agents with configurable architectures and optimization techniques REST API - FastAPI-based service for real-time predictions and model information CICIDS2017 Feature Extraction - Extract 78 standardized network flow features from live traffic Multiple Monitoring Modes - Network interface monitoring and website-specific traffic analysis Advanced Training Pipeline - Curriculum learning, early stopping, learning rate scheduling Comprehensive Evaluation - Detailed performance metrics, confusion matrices, and prediction analysis","title":"Key Features"},{"location":"#architecture","text":"The system consists of several key components: RL Agents ( rl_ids/agents/ ): DQN implementation with configurable network architectures Training Environment ( rl_ids/environments/ ): Gymnasium-compatible environment for IDS training Data Processing ( rl_ids/make_dataset.py ): CICIDS2017 dataset preprocessing and normalization FastAPI Service ( api/ ): Production-ready API for real-time threat detection Real-time Monitors ( network_monitor.py , website_monitor.py ): Live traffic analysis tools","title":"Architecture"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#prerequisites","text":"Python 3.9+ Network capture permissions (sudo for packet capture) CICIDS2017 dataset (place CSV files in data/raw/ )","title":"Prerequisites"},{"location":"#installation","text":"# Install dependencies pip install -r requirements.txt # Process CICIDS2017 dataset python -m rl_ids.make_dataset # Train DQN model python -m rl_ids.modeling.train # Evaluate trained model python -m rl_ids.modeling.evaluate","title":"Installation"},{"location":"#basic-usage","text":"Start the API server: python run_api.py Monitor network interface: sudo python network_monitor.py wlan0 Monitor specific website: python website_monitor.py example.com","title":"Basic Usage"},{"location":"#supported-attack-types","text":"The system can detect the following attack types from the CICIDS2017 dataset: Benign Traffic - Normal network activity DoS Attacks - DoS Hulk, DoS Slowloris, DoS Slowhttptest, DoS GoldenEye DDoS Attacks - Distributed Denial of Service Port Scan - Network reconnaissance attacks Brute Force - FTP-Patator, SSH-Patator Web Attacks - SQL Injection, XSS, Brute Force Bot - Botnet traffic Infiltration - Network infiltration attacks Heartbleed - SSL/TLS vulnerability exploitation","title":"Supported Attack Types"},{"location":"#project-structure","text":"rl_ids/ \u251c\u2500\u2500 rl_ids/ # Core RL-IDS modules \u2502 \u251c\u2500\u2500 agents/ # DQN agent implementation \u2502 \u2502 \u2514\u2500\u2500 dqn_agent.py # DQN agent with training and inference \u2502 \u251c\u2500\u2500 environments/ # Training environments \u2502 \u2502 \u2514\u2500\u2500 ids_env.py # IDS detection Gymnasium environment \u2502 \u251c\u2500\u2500 modeling/ # Training and evaluation pipeline \u2502 \u2502 \u251c\u2500\u2500 train.py # Enhanced DQN training with curriculum learning \u2502 \u2502 \u2514\u2500\u2500 evaluate.py # Comprehensive model evaluation \u2502 \u251c\u2500\u2500 config.py # Project configuration and paths \u2502 \u251c\u2500\u2500 make_dataset.py # CICIDS2017 data preprocessing \u2502 \u2514\u2500\u2500 plots.py # Visualization utilities \u251c\u2500\u2500 api/ # FastAPI application \u2502 \u251c\u2500\u2500 main.py # FastAPI application with endpoints \u2502 \u251c\u2500\u2500 models.py # Pydantic request/response models \u2502 \u251c\u2500\u2500 services.py # Prediction service implementation \u2502 \u251c\u2500\u2500 client.py # API client for testing \u2502 \u2514\u2500\u2500 config.py # API configuration \u251c\u2500\u2500 models/ # Trained model files \u2502 \u251c\u2500\u2500 dqn_model_best.pt # Best performing model \u2502 \u251c\u2500\u2500 dqn_model_final.pt # Final training epoch model \u2502 \u2514\u2500\u2500 episodes/ # Episodic model checkpoints \u251c\u2500\u2500 data/ # Dataset storage \u2502 \u251c\u2500\u2500 raw/ # Original CICIDS2017 CSV files \u2502 \u2514\u2500\u2500 processed/ # Preprocessed train/val/test splits \u251c\u2500\u2500 reports/ # Training and evaluation reports \u2502 \u251c\u2500\u2500 training_metrics.csv # Training progress metrics \u2502 \u251c\u2500\u2500 evaluation_*.csv # Evaluation results \u2502 \u2514\u2500\u2500 figures/ # Generated plots and visualizations \u251c\u2500\u2500 logs/ # Runtime logs \u251c\u2500\u2500 network_monitor.py # Real-time network interface monitoring \u251c\u2500\u2500 website_monitor.py # Website-specific traffic monitoring \u2514\u2500\u2500 run_api.py # API server startup script","title":"Project Structure"},{"location":"#getting-started","text":"Visit the User Guide for detailed installation and usage instructions, API Reference for integration details, or Modules for technical implementation details.","title":"Getting Started"},{"location":"#license","text":"This project is licensed under the terms specified in LICENSE .","title":"License"},{"location":"changelog/","text":"Changelog All notable changes to the RL-IDS (Reinforcement Learning Intrusion Detection System) are documented here. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [1.2.0] - 2025-06-27 Added Marketing-Quality README.md Stunning visual design with professional branding and hero section Interactive feature comparison tables with visual elements Mermaid architecture diagrams for system visualization Performance metrics dashboard with real statistics (95.3% accuracy, <100ms response time) Comprehensive quick start guide with 30-second demo Attack type detection matrix with detailed capabilities Professional project structure visualization with directory tree Enterprise-ready presentation with Docker deployment examples Advanced API Integration Examples Real-world SIEM integration patterns with security tool connectivity High-performance batch processing examples with connection pooling Enterprise monitoring and alerting implementation guides Production-ready error handling and retry patterns Scalability examples for distributed deployments Security best practices for API integration Enhanced User Experience Beautiful visual hierarchy with consistent emoji usage Collapsible sections for detailed technical information Comprehensive benchmarks and performance characteristics Development roadmap with planned feature enhancements Community and support section with clear contact channels Professional call-to-action elements for GitHub engagement Complete Documentation Coverage Installation guide with multiple deployment methods Architecture documentation with system design principles Testing guide with comprehensive examples and best practices FAQ section covering all common use cases and troubleshooting Module documentation for all core components API examples with real-world integration scenarios Changed Visual Design Revolution Complete README overhaul with marketing-quality presentation Enhanced documentation structure for better user journey Improved code examples with proper syntax highlighting Better navigation with linked sections and table of contents Consistent branding and professional appearance throughout Content Quality Enhancement Updated all documentation for accuracy and completeness Improved technical examples with real-world applicability Enhanced cross-references and navigation between sections Better organization of complex technical information Fixed Documentation consistency across all files and sections Code example accuracy and comprehensive testing Installation instructions for all supported platforms Link validation and cross-reference accuracy throughout documentation [1.1.0] - 2025-06-21 Added Comprehensive Documentation Suite Complete MkDocs-based documentation with shadcn theme Interactive API reference with OpenAPI integration Step-by-step installation and setup guides Advanced user guides for network and website monitoring Production deployment guides and best practices FAQ and troubleshooting section with detailed solutions Module-level documentation for agents, features, and environments Development & Contribution Infrastructure GitHub issue templates for bugs and feature requests Comprehensive contributing guidelines with development setup Pre-commit hooks configuration for code quality Automated testing and CI/CD pipeline documentation GitHub Actions workflows for documentation deployment Enhanced API Documentation Detailed endpoint documentation with examples Python client library usage guides Request/response model specifications Error handling and status code documentation Security and authentication guidelines Architecture Documentation System architecture overview with diagrams Component interaction documentation Data flow and processing pipelines Configuration management guides Performance characteristics and optimization Changed Improved logging configuration with loguru integration Enhanced model checkpoint saving strategies Restructured documentation for better user experience Updated API documentation with comprehensive examples Reorganized project structure for better maintainability Fixed Training progress monitoring accuracy API health check endpoint reliability Documentation links and cross-references Code examples and snippets consistency Installation instructions for different platforms [1.0.0] - 2025-06-21 Added Core RL-IDS System DQN agent with Deep Q-Network implementation Custom Gymnasium environment for CICIDS2017 dataset Feature extraction pipeline with 78 network flow features Real-time network packet capture and analysis Comprehensive training and evaluation framework Network Monitoring Live packet capture using raw sockets Protocol analysis (TCP, UDP, HTTP, HTTPS) Flow-based traffic aggregation Statistical feature computation Real-time threat detection and alerting Website Monitoring Automated web request generation Traffic pattern simulation Packet capture for generated traffic Integration with network monitoring pipeline FastAPI Web Service RESTful API for predictions and model information Health check endpoints for monitoring Batch prediction capabilities Comprehensive error handling and validation OpenAPI/Swagger documentation Python Client Library Synchronous and asynchronous client implementations Built-in retry mechanisms and error handling Comprehensive type hints and documentation Integration examples and best practices Machine Learning Pipeline CICIDS2017 dataset processing and feature extraction DQN training with experience replay Model evaluation and performance metrics Hyperparameter optimization support Model checkpointing and versioning Data Processing Flow-based feature extraction from network packets Statistical analysis of traffic patterns Data preprocessing and normalization Train/validation/test dataset splitting Feature importance analysis Configuration Management Environment-based configuration Flexible model and training parameters Network interface and monitoring settings API server configuration Logging and debugging options Technical Features Reinforcement Learning Deep Q-Network (DQN) implementation Experience replay buffer for stable training Target network for improved convergence Epsilon-greedy exploration strategy Reward-based learning for threat detection Network Analysis 78 CICIDS2017-compatible features Flow duration and packet timing analysis Protocol-specific feature extraction Bidirectional flow analysis Statistical traffic characterization Real-time Processing Live packet capture and processing Streaming feature extraction Real-time prediction pipeline Configurable monitoring intervals Efficient memory management Performance Optimization Vectorized operations for feature extraction Efficient data structures for packet processing Optimized model inference Configurable batch processing Memory-efficient data handling Supported Attack Types DDoS Attacks : Distributed Denial of Service detection Port Scanning : Network reconnaissance identification Web Attacks : SQL injection, XSS, and web-based threats Infiltration : Advanced persistent threat detection Brute Force : Authentication and password attacks Botnet : Command and control communication detection Dependencies Core : Python 3.13+, PyTorch, Pandas, Scikit-learn RL Framework : Gymnasium for environment interface API : FastAPI, Uvicorn, Pydantic for web services Monitoring : Scapy, Psutil for network analysis Utilities : Loguru, Typer, Tqdm for enhanced functionality Initial Release Features Complete intrusion detection system Pre-trained models for immediate use Comprehensive API for integration Real-time monitoring capabilities Extensive documentation and examples Development History Project Inception The RL-IDS project was initiated to address the need for adaptive intrusion detection systems that can learn and evolve with changing threat landscapes. Traditional signature-based systems often fail to detect novel attacks, while rule-based systems require constant manual updates. Technology Choices Reinforcement Learning : Chosen for its ability to adapt and learn from feedback Deep Q-Networks : Selected for their proven effectiveness in decision-making tasks CICIDS2017 Dataset : Used as the standard benchmark for network intrusion detection FastAPI : Selected for high-performance API development with automatic documentation Future Roadmap Enhanced Model Architectures : Exploration of transformer-based models Multi-Agent Systems : Distributed detection across network segments Federated Learning : Privacy-preserving collaborative learning Real-time Adaptation : Online learning capabilities Extended Protocol Support : IPv6, QUIC, and emerging protocols Contributing We welcome contributions to RL-IDS! Please see our Contributing Guide for details on: - Development setup and environment - Code style and quality standards - Testing requirements and procedures - Pull request process and guidelines - Issue reporting and feature requests License This project is licensed under the MIT License - see the LICENSE file for details. Acknowledgments CICIDS2017 Dataset : University of New Brunswick for the comprehensive dataset PyTorch Team : For the excellent deep learning framework FastAPI : For the modern, high-performance web framework Gymnasium : For the standardized RL environment interface Open Source Community : For the countless libraries and tools that made this project possible","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to the RL-IDS (Reinforcement Learning Intrusion Detection System) are documented here. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#120-2025-06-27","text":"","title":"[1.2.0] - 2025-06-27"},{"location":"changelog/#added","text":"Marketing-Quality README.md Stunning visual design with professional branding and hero section Interactive feature comparison tables with visual elements Mermaid architecture diagrams for system visualization Performance metrics dashboard with real statistics (95.3% accuracy, <100ms response time) Comprehensive quick start guide with 30-second demo Attack type detection matrix with detailed capabilities Professional project structure visualization with directory tree Enterprise-ready presentation with Docker deployment examples Advanced API Integration Examples Real-world SIEM integration patterns with security tool connectivity High-performance batch processing examples with connection pooling Enterprise monitoring and alerting implementation guides Production-ready error handling and retry patterns Scalability examples for distributed deployments Security best practices for API integration Enhanced User Experience Beautiful visual hierarchy with consistent emoji usage Collapsible sections for detailed technical information Comprehensive benchmarks and performance characteristics Development roadmap with planned feature enhancements Community and support section with clear contact channels Professional call-to-action elements for GitHub engagement Complete Documentation Coverage Installation guide with multiple deployment methods Architecture documentation with system design principles Testing guide with comprehensive examples and best practices FAQ section covering all common use cases and troubleshooting Module documentation for all core components API examples with real-world integration scenarios","title":"Added"},{"location":"changelog/#changed","text":"Visual Design Revolution Complete README overhaul with marketing-quality presentation Enhanced documentation structure for better user journey Improved code examples with proper syntax highlighting Better navigation with linked sections and table of contents Consistent branding and professional appearance throughout Content Quality Enhancement Updated all documentation for accuracy and completeness Improved technical examples with real-world applicability Enhanced cross-references and navigation between sections Better organization of complex technical information","title":"Changed"},{"location":"changelog/#fixed","text":"Documentation consistency across all files and sections Code example accuracy and comprehensive testing Installation instructions for all supported platforms Link validation and cross-reference accuracy throughout documentation","title":"Fixed"},{"location":"changelog/#110-2025-06-21","text":"","title":"[1.1.0] - 2025-06-21"},{"location":"changelog/#added_1","text":"Comprehensive Documentation Suite Complete MkDocs-based documentation with shadcn theme Interactive API reference with OpenAPI integration Step-by-step installation and setup guides Advanced user guides for network and website monitoring Production deployment guides and best practices FAQ and troubleshooting section with detailed solutions Module-level documentation for agents, features, and environments Development & Contribution Infrastructure GitHub issue templates for bugs and feature requests Comprehensive contributing guidelines with development setup Pre-commit hooks configuration for code quality Automated testing and CI/CD pipeline documentation GitHub Actions workflows for documentation deployment Enhanced API Documentation Detailed endpoint documentation with examples Python client library usage guides Request/response model specifications Error handling and status code documentation Security and authentication guidelines Architecture Documentation System architecture overview with diagrams Component interaction documentation Data flow and processing pipelines Configuration management guides Performance characteristics and optimization","title":"Added"},{"location":"changelog/#changed_1","text":"Improved logging configuration with loguru integration Enhanced model checkpoint saving strategies Restructured documentation for better user experience Updated API documentation with comprehensive examples Reorganized project structure for better maintainability","title":"Changed"},{"location":"changelog/#fixed_1","text":"Training progress monitoring accuracy API health check endpoint reliability Documentation links and cross-references Code examples and snippets consistency Installation instructions for different platforms","title":"Fixed"},{"location":"changelog/#100-2025-06-21","text":"","title":"[1.0.0] - 2025-06-21"},{"location":"changelog/#added_2","text":"Core RL-IDS System DQN agent with Deep Q-Network implementation Custom Gymnasium environment for CICIDS2017 dataset Feature extraction pipeline with 78 network flow features Real-time network packet capture and analysis Comprehensive training and evaluation framework Network Monitoring Live packet capture using raw sockets Protocol analysis (TCP, UDP, HTTP, HTTPS) Flow-based traffic aggregation Statistical feature computation Real-time threat detection and alerting Website Monitoring Automated web request generation Traffic pattern simulation Packet capture for generated traffic Integration with network monitoring pipeline FastAPI Web Service RESTful API for predictions and model information Health check endpoints for monitoring Batch prediction capabilities Comprehensive error handling and validation OpenAPI/Swagger documentation Python Client Library Synchronous and asynchronous client implementations Built-in retry mechanisms and error handling Comprehensive type hints and documentation Integration examples and best practices Machine Learning Pipeline CICIDS2017 dataset processing and feature extraction DQN training with experience replay Model evaluation and performance metrics Hyperparameter optimization support Model checkpointing and versioning Data Processing Flow-based feature extraction from network packets Statistical analysis of traffic patterns Data preprocessing and normalization Train/validation/test dataset splitting Feature importance analysis Configuration Management Environment-based configuration Flexible model and training parameters Network interface and monitoring settings API server configuration Logging and debugging options","title":"Added"},{"location":"changelog/#technical-features","text":"Reinforcement Learning Deep Q-Network (DQN) implementation Experience replay buffer for stable training Target network for improved convergence Epsilon-greedy exploration strategy Reward-based learning for threat detection Network Analysis 78 CICIDS2017-compatible features Flow duration and packet timing analysis Protocol-specific feature extraction Bidirectional flow analysis Statistical traffic characterization Real-time Processing Live packet capture and processing Streaming feature extraction Real-time prediction pipeline Configurable monitoring intervals Efficient memory management Performance Optimization Vectorized operations for feature extraction Efficient data structures for packet processing Optimized model inference Configurable batch processing Memory-efficient data handling","title":"Technical Features"},{"location":"changelog/#supported-attack-types","text":"DDoS Attacks : Distributed Denial of Service detection Port Scanning : Network reconnaissance identification Web Attacks : SQL injection, XSS, and web-based threats Infiltration : Advanced persistent threat detection Brute Force : Authentication and password attacks Botnet : Command and control communication detection","title":"Supported Attack Types"},{"location":"changelog/#dependencies","text":"Core : Python 3.13+, PyTorch, Pandas, Scikit-learn RL Framework : Gymnasium for environment interface API : FastAPI, Uvicorn, Pydantic for web services Monitoring : Scapy, Psutil for network analysis Utilities : Loguru, Typer, Tqdm for enhanced functionality","title":"Dependencies"},{"location":"changelog/#initial-release-features","text":"Complete intrusion detection system Pre-trained models for immediate use Comprehensive API for integration Real-time monitoring capabilities Extensive documentation and examples","title":"Initial Release Features"},{"location":"changelog/#development-history","text":"","title":"Development History"},{"location":"changelog/#project-inception","text":"The RL-IDS project was initiated to address the need for adaptive intrusion detection systems that can learn and evolve with changing threat landscapes. Traditional signature-based systems often fail to detect novel attacks, while rule-based systems require constant manual updates.","title":"Project Inception"},{"location":"changelog/#technology-choices","text":"Reinforcement Learning : Chosen for its ability to adapt and learn from feedback Deep Q-Networks : Selected for their proven effectiveness in decision-making tasks CICIDS2017 Dataset : Used as the standard benchmark for network intrusion detection FastAPI : Selected for high-performance API development with automatic documentation","title":"Technology Choices"},{"location":"changelog/#future-roadmap","text":"Enhanced Model Architectures : Exploration of transformer-based models Multi-Agent Systems : Distributed detection across network segments Federated Learning : Privacy-preserving collaborative learning Real-time Adaptation : Online learning capabilities Extended Protocol Support : IPv6, QUIC, and emerging protocols","title":"Future Roadmap"},{"location":"changelog/#contributing","text":"We welcome contributions to RL-IDS! Please see our Contributing Guide for details on: - Development setup and environment - Code style and quality standards - Testing requirements and procedures - Pull request process and guidelines - Issue reporting and feature requests","title":"Contributing"},{"location":"changelog/#license","text":"This project is licensed under the MIT License - see the LICENSE file for details.","title":"License"},{"location":"changelog/#acknowledgments","text":"CICIDS2017 Dataset : University of New Brunswick for the comprehensive dataset PyTorch Team : For the excellent deep learning framework FastAPI : For the modern, high-performance web framework Gymnasium : For the standardized RL environment interface Open Source Community : For the countless libraries and tools that made this project possible","title":"Acknowledgments"},{"location":"faq/","text":"Frequently Asked Questions This FAQ addresses common questions about the RL-IDS (Reinforcement Learning Intrusion Detection System). General Questions What is RL-IDS? RL-IDS is a reinforcement learning-driven adaptive intrusion detection system that uses Deep Q-Network (DQN) algorithms to detect network intrusions in real-time. It combines traditional network monitoring with modern machine learning techniques to provide intelligent threat detection. What makes RL-IDS different from traditional IDS? Adaptive Learning : Continuously learns from new data and adapts to evolving threats Reinforcement Learning : Uses reward-based learning to improve detection accuracy Real-time Processing : Provides immediate threat detection and response Feature-rich Analysis : Analyzes 78 different network flow features API Integration : Easy integration with existing security infrastructure What types of attacks can RL-IDS detect? Based on the CICIDS2017 dataset, RL-IDS can detect: - DDoS Attacks : Distributed Denial of Service - Port Scan : Network reconnaissance activities - Web Attacks : SQL injection, XSS, and other web-based attacks - Infiltration : Advanced persistent threats - Brute Force : Password and authentication attacks - Botnet : Command and control communications Installation and Setup What are the system requirements? Minimum Requirements: - Python 3.13.0+ - 8GB RAM (16GB recommended) - 10GB free disk space - Network interface with administrative privileges Recommended: - Multi-core CPU (8+ cores for training) - NVIDIA GPU with CUDA support - Fast SSD storage Do I need root/administrator privileges? For network monitoring features, you need: - Linux : Either root privileges or set capabilities: sudo setcap cap_net_raw,cap_net_admin=eip $(which python) - Windows : Run as administrator - macOS : Use sudo or configure appropriate permissions For API-only usage, no special privileges are required. Can I run RL-IDS without a pre-trained model? Yes, but with limitations: - You can train a new model using the provided training scripts - Training requires the CICIDS2017 dataset (or similar labeled data) - Initial training may take several hours depending on hardware - Pre-trained models provide immediate detection capabilities Usage and Configuration How do I configure network monitoring? Identify Network Interface : python import psutil print(list(psutil.net_if_addrs().keys())) Update Configuration : python # In network_monitor.py or configuration file INTERFACE = \"eth0\" # Replace with your interface Set Permissions (Linux): bash sudo setcap cap_net_raw,cap_net_admin=eip $(which python) How do I integrate RL-IDS with my existing security tools? API Integration: from api.client import RLIDSClient client = RLIDSClient(\"http://localhost:8000\") result = client.predict(network_features) Webhook Integration: - Configure webhooks in your security tools - Send network data to RL-IDS API endpoints - Process responses for alerts and actions Log Integration: - Configure RL-IDS to output to standard security log formats - Integrate with SIEM systems through log forwarding Can I customize the detection model? Yes, several customization options are available: Model Parameters: - Adjust neural network architecture in rl_ids/agents/dqn_agent.py - Modify training hyperparameters in rl_ids/config.py - Change reward functions in the environment Feature Engineering: - Add new features to rl_ids/make_dataset.py - Modify existing feature calculations - Implement custom preprocessing pipelines Training Data: - Use your own labeled dataset - Combine multiple datasets - Implement active learning strategies Performance and Scalability What is the expected performance? Detection Speed: - Single prediction: < 100ms - Batch predictions: > 1000 predictions/second - Real-time monitoring: Handles typical enterprise network loads Accuracy: - Trained on CICIDS2017: > 95% accuracy - False positive rate: < 5% - Performance may vary with different network environments How can I improve performance? Hardware Optimizations: - Use GPU acceleration for training and large-scale inference - Increase RAM for larger batch processing - Use SSD storage for faster data access Software Optimizations: - Adjust batch sizes based on available memory - Use multiple worker processes for API deployment - Implement caching for frequently accessed data Network Optimizations: - Optimize packet capture buffer sizes - Use appropriate network interface configurations - Consider distributed deployment for high-traffic environments Can RL-IDS scale horizontally? Yes, through several approaches: API Scaling: - Deploy multiple API instances behind a load balancer - Use container orchestration (Docker, Kubernetes) - Implement distributed caching (Redis) Data Processing: - Distribute packet capture across multiple interfaces - Use message queues for processing pipelines - Implement microservices architecture Troubleshooting \"Permission denied\" errors during network monitoring Solution: # Linux - Set capabilities sudo setcap cap_net_raw,cap_net_admin=eip $(which python) # Or run with sudo (not recommended for production) sudo python network_monitor.py # Windows - Run as administrator # Right-click Command Prompt \u2192 \"Run as administrator\" \"ModuleNotFoundError\" when importing rl_ids Common Causes: 1. Virtual environment not activated 2. Package not installed properly 3. Python path issues Solutions: # Activate virtual environment source venv/bin/activate # Linux/macOS venv\\Scripts\\activate # Windows # Reinstall package pip install -e . # Check installation python -c \"import rl_ids; print('Success')\" High memory usage during training Solutions: 1. Reduce Batch Size: python # In training configuration BATCH_SIZE = 32 # Reduce from default Enable Gradient Checkpointing: python # In model configuration USE_CHECKPOINT = True Use CPU Training: bash export CUDA_VISIBLE_DEVICES=\"\" Model not loading or giving errors Check Model File: import torch try: model = torch.load(\"models/dqn_model_best.pt\") print(\"Model loaded successfully\") except Exception as e: print(f\"Model loading error: {e}\") Regenerate Model: # Retrain model python rl_ids/modeling/train.py # Or download pre-trained model (if available) python -c \"from rl_ids.modeling.train import download_pretrained_model; download_pretrained_model()\" API connection issues Check API Status: # Test API connectivity curl http://localhost:8000/health # Check if API is running ps aux | grep \"uvicorn\\|python.*api\" Common Solutions: 1. Ensure API server is running 2. Check firewall settings 3. Verify correct host/port configuration 4. Test with different client (curl, browser) Development and Contribution How can I contribute to RL-IDS? Fork the Repository : Create your own fork on GitHub Set Up Development Environment : Follow the development setup guide Make Changes : Implement features or fix bugs Write Tests : Ensure your changes are tested Submit Pull Request : Follow the contribution guidelines How do I add new attack detection capabilities? Data Collection : Gather labeled examples of the new attack type Feature Analysis : Identify distinguishing features Model Training : Retrain with expanded dataset Validation : Test detection accuracy Integration : Update classification labels and API responses How do I extend the API? Add Endpoints : Define new routes in api/main.py Create Models : Add Pydantic models in api/models.py Implement Logic : Add business logic in api/services.py Update Client : Extend the Python client library Document : Update API documentation Advanced Usage Can I use RL-IDS with custom datasets? Yes, follow these steps: Format Data : Ensure data matches CICIDS2017 feature format (78 features) Update Labels : Map your attack types to the expected categories Preprocessing : Apply the same preprocessing pipeline Training : Retrain the model with your dataset Validation : Test performance on representative data How do I implement custom reward functions? Modify the reward function in the IDS environment: # In rl_ids/environments/ids_env.py def _calculate_reward(self, predicted_class, actual_class): # Custom reward logic if predicted_class == actual_class: if actual_class == 'BENIGN': return 1.0 # Correct benign classification else: return 2.0 # Correct attack detection (higher reward) else: if actual_class == 'BENIGN': return -2.0 # False positive (high penalty) else: return -1.0 # False negative Can I deploy RL-IDS in a distributed environment? Yes, consider these approaches: Microservices Deployment: - Separate services for data collection, processing, and detection - Use message queues (RabbitMQ, Kafka) for communication - Implement service discovery and load balancing Container Orchestration: # docker-compose.yml example version: '3.8' services: rl-ids-api: image: rl-ids:latest ports: - \"8000:8000\" environment: - MODEL_PATH=/app/models/dqn_model_best.pt rl-ids-monitor: image: rl-ids:latest command: python network_monitor.py network_mode: host privileged: true Getting Help Where can I find more information? Documentation : Complete documentation is available in this site GitHub Repository : Source code and issue tracking API Reference : Interactive API documentation at /docs endpoint Code Examples : Sample implementations in the repository How do I report bugs or request features? Check Existing Issues : Search GitHub issues for similar problems Create Detailed Report : Include error messages, environment details, and steps to reproduce Provide Context : Explain your use case and expected behavior Follow Up : Respond to questions and provide additional information How do I get support for production deployment? For production deployments: 1. Review Best Practices : Follow the deployment and security guidelines 2. Performance Testing : Conduct thorough testing in your environment 3. Monitoring Setup : Implement comprehensive logging and monitoring 4. Backup Strategy : Ensure model and configuration backup procedures 5. Update Plan : Establish procedures for updates and maintenance Remember to never include sensitive network data or security configurations in support requests.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"This FAQ addresses common questions about the RL-IDS (Reinforcement Learning Intrusion Detection System).","title":"Frequently Asked Questions"},{"location":"faq/#general-questions","text":"","title":"General Questions"},{"location":"faq/#what-is-rl-ids","text":"RL-IDS is a reinforcement learning-driven adaptive intrusion detection system that uses Deep Q-Network (DQN) algorithms to detect network intrusions in real-time. It combines traditional network monitoring with modern machine learning techniques to provide intelligent threat detection.","title":"What is RL-IDS?"},{"location":"faq/#what-makes-rl-ids-different-from-traditional-ids","text":"Adaptive Learning : Continuously learns from new data and adapts to evolving threats Reinforcement Learning : Uses reward-based learning to improve detection accuracy Real-time Processing : Provides immediate threat detection and response Feature-rich Analysis : Analyzes 78 different network flow features API Integration : Easy integration with existing security infrastructure","title":"What makes RL-IDS different from traditional IDS?"},{"location":"faq/#what-types-of-attacks-can-rl-ids-detect","text":"Based on the CICIDS2017 dataset, RL-IDS can detect: - DDoS Attacks : Distributed Denial of Service - Port Scan : Network reconnaissance activities - Web Attacks : SQL injection, XSS, and other web-based attacks - Infiltration : Advanced persistent threats - Brute Force : Password and authentication attacks - Botnet : Command and control communications","title":"What types of attacks can RL-IDS detect?"},{"location":"faq/#installation-and-setup","text":"","title":"Installation and Setup"},{"location":"faq/#what-are-the-system-requirements","text":"Minimum Requirements: - Python 3.13.0+ - 8GB RAM (16GB recommended) - 10GB free disk space - Network interface with administrative privileges Recommended: - Multi-core CPU (8+ cores for training) - NVIDIA GPU with CUDA support - Fast SSD storage","title":"What are the system requirements?"},{"location":"faq/#do-i-need-rootadministrator-privileges","text":"For network monitoring features, you need: - Linux : Either root privileges or set capabilities: sudo setcap cap_net_raw,cap_net_admin=eip $(which python) - Windows : Run as administrator - macOS : Use sudo or configure appropriate permissions For API-only usage, no special privileges are required.","title":"Do I need root/administrator privileges?"},{"location":"faq/#can-i-run-rl-ids-without-a-pre-trained-model","text":"Yes, but with limitations: - You can train a new model using the provided training scripts - Training requires the CICIDS2017 dataset (or similar labeled data) - Initial training may take several hours depending on hardware - Pre-trained models provide immediate detection capabilities","title":"Can I run RL-IDS without a pre-trained model?"},{"location":"faq/#usage-and-configuration","text":"","title":"Usage and Configuration"},{"location":"faq/#how-do-i-configure-network-monitoring","text":"Identify Network Interface : python import psutil print(list(psutil.net_if_addrs().keys())) Update Configuration : python # In network_monitor.py or configuration file INTERFACE = \"eth0\" # Replace with your interface Set Permissions (Linux): bash sudo setcap cap_net_raw,cap_net_admin=eip $(which python)","title":"How do I configure network monitoring?"},{"location":"faq/#how-do-i-integrate-rl-ids-with-my-existing-security-tools","text":"API Integration: from api.client import RLIDSClient client = RLIDSClient(\"http://localhost:8000\") result = client.predict(network_features) Webhook Integration: - Configure webhooks in your security tools - Send network data to RL-IDS API endpoints - Process responses for alerts and actions Log Integration: - Configure RL-IDS to output to standard security log formats - Integrate with SIEM systems through log forwarding","title":"How do I integrate RL-IDS with my existing security tools?"},{"location":"faq/#can-i-customize-the-detection-model","text":"Yes, several customization options are available: Model Parameters: - Adjust neural network architecture in rl_ids/agents/dqn_agent.py - Modify training hyperparameters in rl_ids/config.py - Change reward functions in the environment Feature Engineering: - Add new features to rl_ids/make_dataset.py - Modify existing feature calculations - Implement custom preprocessing pipelines Training Data: - Use your own labeled dataset - Combine multiple datasets - Implement active learning strategies","title":"Can I customize the detection model?"},{"location":"faq/#performance-and-scalability","text":"","title":"Performance and Scalability"},{"location":"faq/#what-is-the-expected-performance","text":"Detection Speed: - Single prediction: < 100ms - Batch predictions: > 1000 predictions/second - Real-time monitoring: Handles typical enterprise network loads Accuracy: - Trained on CICIDS2017: > 95% accuracy - False positive rate: < 5% - Performance may vary with different network environments","title":"What is the expected performance?"},{"location":"faq/#how-can-i-improve-performance","text":"Hardware Optimizations: - Use GPU acceleration for training and large-scale inference - Increase RAM for larger batch processing - Use SSD storage for faster data access Software Optimizations: - Adjust batch sizes based on available memory - Use multiple worker processes for API deployment - Implement caching for frequently accessed data Network Optimizations: - Optimize packet capture buffer sizes - Use appropriate network interface configurations - Consider distributed deployment for high-traffic environments","title":"How can I improve performance?"},{"location":"faq/#can-rl-ids-scale-horizontally","text":"Yes, through several approaches: API Scaling: - Deploy multiple API instances behind a load balancer - Use container orchestration (Docker, Kubernetes) - Implement distributed caching (Redis) Data Processing: - Distribute packet capture across multiple interfaces - Use message queues for processing pipelines - Implement microservices architecture","title":"Can RL-IDS scale horizontally?"},{"location":"faq/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"faq/#permission-denied-errors-during-network-monitoring","text":"Solution: # Linux - Set capabilities sudo setcap cap_net_raw,cap_net_admin=eip $(which python) # Or run with sudo (not recommended for production) sudo python network_monitor.py # Windows - Run as administrator # Right-click Command Prompt \u2192 \"Run as administrator\"","title":"\"Permission denied\" errors during network monitoring"},{"location":"faq/#modulenotfounderror-when-importing-rl_ids","text":"Common Causes: 1. Virtual environment not activated 2. Package not installed properly 3. Python path issues Solutions: # Activate virtual environment source venv/bin/activate # Linux/macOS venv\\Scripts\\activate # Windows # Reinstall package pip install -e . # Check installation python -c \"import rl_ids; print('Success')\"","title":"\"ModuleNotFoundError\" when importing rl_ids"},{"location":"faq/#high-memory-usage-during-training","text":"Solutions: 1. Reduce Batch Size: python # In training configuration BATCH_SIZE = 32 # Reduce from default Enable Gradient Checkpointing: python # In model configuration USE_CHECKPOINT = True Use CPU Training: bash export CUDA_VISIBLE_DEVICES=\"\"","title":"High memory usage during training"},{"location":"faq/#model-not-loading-or-giving-errors","text":"Check Model File: import torch try: model = torch.load(\"models/dqn_model_best.pt\") print(\"Model loaded successfully\") except Exception as e: print(f\"Model loading error: {e}\") Regenerate Model: # Retrain model python rl_ids/modeling/train.py # Or download pre-trained model (if available) python -c \"from rl_ids.modeling.train import download_pretrained_model; download_pretrained_model()\"","title":"Model not loading or giving errors"},{"location":"faq/#api-connection-issues","text":"Check API Status: # Test API connectivity curl http://localhost:8000/health # Check if API is running ps aux | grep \"uvicorn\\|python.*api\" Common Solutions: 1. Ensure API server is running 2. Check firewall settings 3. Verify correct host/port configuration 4. Test with different client (curl, browser)","title":"API connection issues"},{"location":"faq/#development-and-contribution","text":"","title":"Development and Contribution"},{"location":"faq/#how-can-i-contribute-to-rl-ids","text":"Fork the Repository : Create your own fork on GitHub Set Up Development Environment : Follow the development setup guide Make Changes : Implement features or fix bugs Write Tests : Ensure your changes are tested Submit Pull Request : Follow the contribution guidelines","title":"How can I contribute to RL-IDS?"},{"location":"faq/#how-do-i-add-new-attack-detection-capabilities","text":"Data Collection : Gather labeled examples of the new attack type Feature Analysis : Identify distinguishing features Model Training : Retrain with expanded dataset Validation : Test detection accuracy Integration : Update classification labels and API responses","title":"How do I add new attack detection capabilities?"},{"location":"faq/#how-do-i-extend-the-api","text":"Add Endpoints : Define new routes in api/main.py Create Models : Add Pydantic models in api/models.py Implement Logic : Add business logic in api/services.py Update Client : Extend the Python client library Document : Update API documentation","title":"How do I extend the API?"},{"location":"faq/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"faq/#can-i-use-rl-ids-with-custom-datasets","text":"Yes, follow these steps: Format Data : Ensure data matches CICIDS2017 feature format (78 features) Update Labels : Map your attack types to the expected categories Preprocessing : Apply the same preprocessing pipeline Training : Retrain the model with your dataset Validation : Test performance on representative data","title":"Can I use RL-IDS with custom datasets?"},{"location":"faq/#how-do-i-implement-custom-reward-functions","text":"Modify the reward function in the IDS environment: # In rl_ids/environments/ids_env.py def _calculate_reward(self, predicted_class, actual_class): # Custom reward logic if predicted_class == actual_class: if actual_class == 'BENIGN': return 1.0 # Correct benign classification else: return 2.0 # Correct attack detection (higher reward) else: if actual_class == 'BENIGN': return -2.0 # False positive (high penalty) else: return -1.0 # False negative","title":"How do I implement custom reward functions?"},{"location":"faq/#can-i-deploy-rl-ids-in-a-distributed-environment","text":"Yes, consider these approaches: Microservices Deployment: - Separate services for data collection, processing, and detection - Use message queues (RabbitMQ, Kafka) for communication - Implement service discovery and load balancing Container Orchestration: # docker-compose.yml example version: '3.8' services: rl-ids-api: image: rl-ids:latest ports: - \"8000:8000\" environment: - MODEL_PATH=/app/models/dqn_model_best.pt rl-ids-monitor: image: rl-ids:latest command: python network_monitor.py network_mode: host privileged: true","title":"Can I deploy RL-IDS in a distributed environment?"},{"location":"faq/#getting-help","text":"","title":"Getting Help"},{"location":"faq/#where-can-i-find-more-information","text":"Documentation : Complete documentation is available in this site GitHub Repository : Source code and issue tracking API Reference : Interactive API documentation at /docs endpoint Code Examples : Sample implementations in the repository","title":"Where can I find more information?"},{"location":"faq/#how-do-i-report-bugs-or-request-features","text":"Check Existing Issues : Search GitHub issues for similar problems Create Detailed Report : Include error messages, environment details, and steps to reproduce Provide Context : Explain your use case and expected behavior Follow Up : Respond to questions and provide additional information","title":"How do I report bugs or request features?"},{"location":"faq/#how-do-i-get-support-for-production-deployment","text":"For production deployments: 1. Review Best Practices : Follow the deployment and security guidelines 2. Performance Testing : Conduct thorough testing in your environment 3. Monitoring Setup : Implement comprehensive logging and monitoring 4. Backup Strategy : Ensure model and configuration backup procedures 5. Update Plan : Establish procedures for updates and maintenance Remember to never include sensitive network data or security configurations in support requests.","title":"How do I get support for production deployment?"},{"location":"license/","text":"The MIT License (MIT) Copyright (c) 2025, Yash Potdar Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"LICENSE"},{"location":"api/","text":"API Reference RL-IDS provides a FastAPI-based REST API for integration with other systems and programmatic access to threat detection capabilities. Overview The RL-IDS API ( api/main.py ) provides a production-ready FastAPI service for real-time network intrusion detection using trained DQN models. The API is designed for high-performance, scalable threat detection with comprehensive error handling and monitoring capabilities. Quick Start Start the API Server # Start with default settings python run_api.py # Start with custom host/port python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 The API will be available at http://localhost:8000 with interactive documentation at http://localhost:8000/docs . Base Configuration The API is configured through api/config.py and can be customized via environment variables: # Default settings HOST = \"0.0.0.0\" PORT = 8000 MODEL_PATH = \"models/dqn_model_final.pt\" API Endpoints Root Endpoint Get basic service information: GET / Response: { \"service\": \"RL-based Intrusion Detection System API\", \"version\": \"1.2.0\", \"status\": \"running\", \"docs\": \"/docs\" } Health Check Check API status and model information: GET /health Response: { \"status\": \"healthy\", \"timestamp\": \"2025-06-27T10:30:00.123456\", \"details\": { \"model_loaded\": true, \"model_path\": \"models/dqn_model_final.pt\", \"predictions_served\": 1234, \"uptime_seconds\": 3600.5, \"memory_usage_mb\": 256.7 } } Error Response (503): { \"detail\": \"Prediction service not initialized\" } Model Information Get detailed information about the loaded model: GET /model/info Response: { \"model_name\": \"DQN_IDS_Model\", \"model_version\": \"1.0.0\", \"model_type\": \"Deep Q-Network\", \"input_features\": 78, \"output_classes\": 15, \"training_episodes\": null, \"model_size_mb\": 2.5, \"class_names\": [ \"BENIGN\", \"Web Attack \u2013 Brute Force\", \"Web Attack \u2013 XSS\", \"Web Attack \u2013 Sql Injection\", \"FTP-Patator\", \"SSH-Patator\", \"PortScan\", \"DoS slowloris\", \"DoS Slowhttptest\", \"DoS Hulk\", \"DoS GoldenEye\", \"Heartbleed\", \"Bot\", \"DDoS\", \"Infiltration\" ], \"classification_type\": \"multi-class\" } Single Prediction Analyze network features for threats: POST /predict Content-Type: application/json { \"features\": [ 0.123, 0.456, 0.789, 0.321, 0.654, 0.987, 0.147, 0.258, 0.369, 0.741, ... // 78 total features ] } Success Response (200): { \"prediction\": 9, \"confidence\": 0.87, \"class_probabilities\": [ 0.01, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.01, 0.02, 0.87, 0.01, 0.01, 0.01, 0.01, 0.01 ], \"predicted_class\": \"DoS Hulk\", \"is_attack\": true, \"processing_time_ms\": 12.5, \"timestamp\": \"2025-06-27T10:30:45.123456\" } Validation Error (422): { \"detail\": \"Invalid input data: Expected 78 features, got 77\" } Batch Prediction Analyze multiple feature sets: POST /predict/batch Content-Type: application/json [ { \"features\": [0.1, 0.2, 0.3, ...] }, { \"features\": [0.4, 0.5, 0.6, ...] } ] Response: [ { \"prediction\": 0, \"confidence\": 0.92, \"class_probabilities\": [0.92, 0.01, 0.01, ...], \"predicted_class\": \"BENIGN\", \"is_attack\": false, \"processing_time_ms\": 8.3, \"timestamp\": \"2025-06-27T10:30:45.123456\" }, { \"prediction\": 6, \"confidence\": 0.78, \"class_probabilities\": [0.05, 0.02, 0.01, 0.01, 0.02, 0.01, 0.78, ...], \"predicted_class\": \"PortScan\", \"is_attack\": true, \"processing_time_ms\": 9.1, \"timestamp\": \"2025-06-27T10:30:45.134567\" } ] Batch Size Limit (413): { \"detail\": \"Batch size too large. Maximum 100 requests allowed.\" } Request/Response Models IDSPredictionRequest class IDSPredictionRequest(BaseModel): features: List[float] = Field( ..., description=\"Network traffic features for prediction\", min_items=1 ) Validation Rules: - Must contain exactly 78 features (CICIDS2017 standard) - All features must be numeric (int or float) - Features list cannot be empty IDSPredictionResponse class IDSPredictionResponse(BaseModel): prediction: int # Predicted class (0-14) confidence: float # Confidence score (0.0-1.0) class_probabilities: List[float] # Probability for each class predicted_class: str # Human-readable class name is_attack: bool # True if attack detected (non-benign) processing_time_ms: float # Processing time in milliseconds timestamp: str # Prediction timestamp (ISO format) Error Handling The API returns standard HTTP status codes with detailed error information: Status Codes 200 - Success 422 - Validation Error (invalid input) 500 - Internal Server Error 503 - Service Unavailable (model not loaded) 413 - Request Entity Too Large (batch size exceeded) Error Response Format { \"detail\": \"Error description\", \"timestamp\": \"2025-06-27T10:30:45.123456\" } Common Errors Feature Count Mismatch: { \"detail\": \"Invalid input data: Expected 78 features, got 75\" } Invalid Feature Values: { \"detail\": \"Invalid input data: All features must be numeric\" } Model Not Loaded: { \"detail\": \"Prediction service not initialized\" } API Configuration Environment Variables Configure the API through environment variables: # API server settings RLIDS_API_HOST=0.0.0.0 RLIDS_API_PORT=8000 # Model settings RLIDS_MODEL_PATH=models/dqn_model_best.pt # Logging RLIDS_LOG_LEVEL=INFO RLIDS_DEBUG=false CORS Configuration The API includes CORS middleware for cross-origin requests: app.add_middleware( CORSMiddleware, allow_origins=[\"*\"], # Configure for production allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"], ) Service Architecture Application Lifecycle The API uses FastAPI's lifespan events for proper service management: @asynccontextmanager async def lifespan(app: FastAPI): # Startup global prediction_service model_path = MODELS_DIR / \"dqn_model_final.pt\" prediction_service = IDSPredictionService(model_path=model_path) await prediction_service.initialize() yield # Shutdown if prediction_service: await prediction_service.cleanup() Prediction Service The IDSPredictionService class handles: Model Loading : Loads trained DQN models with configuration Feature Validation : Ensures input features match expected format Prediction Processing : Runs inference with timing metrics Resource Management : Handles GPU/CPU memory and cleanup Performance Characteristics Model Loading Time : ~2-5 seconds on startup Prediction Latency : ~8-15ms per prediction Memory Usage : ~200-500MB depending on model size Throughput : ~100-500 predictions/second (depends on hardware) OpenAPI Documentation The API automatically generates comprehensive documentation: Swagger UI : http://localhost:8000/docs ReDoc : http://localhost:8000/redoc OpenAPI Spec : http://localhost:8000/openapi.json Interactive Testing The Swagger UI provides interactive testing capabilities: 1. Navigate to http://localhost:8000/docs 2. Expand endpoint sections 3. Click \"Try it out\" 4. Input test data 5. Execute requests and view responses Security Considerations Production Deployment For production environments, consider: Authentication : Add API key or JWT token authentication from fastapi.security import HTTPBearer security = HTTPBearer() @app.post(\"/predict\") async def predict(request: IDSPredictionRequest, token: str = Depends(security)): # Verify token pass Rate Limiting : Implement request rate limiting from slowapi import Limiter limiter = Limiter(key_func=get_remote_address) @limiter.limit(\"100/minute\") @app.post(\"/predict\") async def predict(request: Request, ...): pass HTTPS : Use TLS/SSL in production uvicorn api.main:app --host 0.0.0.0 --port 443 --ssl-keyfile key.pem --ssl-certfile cert.pem CORS : Restrict origins for production app.add_middleware( CORSMiddleware, allow_origins=[\"https://yourdomain.com\"], allow_credentials=True, allow_methods=[\"GET\", \"POST\"], allow_headers=[\"*\"], ) Input Validation The API includes comprehensive input validation: - Feature count validation (must be exactly 78) - Numeric type checking for all features - Range validation to prevent extreme values - Batch size limits to prevent resource exhaustion Monitoring and Logging Health Monitoring The /health endpoint provides detailed service status: - Model loading status - Memory usage metrics - Prediction statistics - Service uptime Logging Configuration Configure logging levels and outputs: # In api/config.py LOG_LEVEL = os.getenv(\"RLIDS_LOG_LEVEL\", \"INFO\") Log Levels: - DEBUG : Detailed debugging information - INFO : General operational messages - WARNING : Warning conditions - ERROR : Error conditions - CRITICAL : Critical error conditions Performance Metrics The API tracks performance metrics: - Prediction processing time - Total predictions served - Error rates and types - Memory usage patterns RL-IDS provides a FastAPI-based REST API for integration with other systems and programmatic access to threat detection capabilities. Quick Start Start the API server: python run_api.py The API will be available at http://localhost:8000 with interactive documentation at http://localhost:8000/docs . Base Configuration The API is configured through api/config.py and can be customized via environment variables. Default settings: - Host : 0.0.0.0 - Port : 8000 - Model Path : models/dqn_model_best.pt Core Endpoints Health Check Check API status and model information: GET /health Response: { \"status\": \"healthy\", \"timestamp\": \"2025-01-27T10:30:00Z\", \"model_loaded\": true, \"model_path\": \"models/dqn_model_best.pt\", \"api_version\": \"1.2.0\" } Threat Prediction Analyze network features for threats: POST /predict Content-Type: application/json { \"features\": [0.1, 0.2, 0.3, ...], // 78 CICIDS2017 features \"metadata\": { \"source_ip\": \"192.168.1.100\", \"timestamp\": \"2025-01-27T10:30:00Z\" } } Response: { \"is_attack\": true, \"predicted_class\": \"DoS Hulk\", \"confidence\": 0.87, \"model_version\": \"dqn_v1\", \"timestamp\": \"2025-01-27T10:30:00Z\", \"features_count\": 78 } Batch Prediction Analyze multiple feature sets: POST /predict_batch Content-Type: application/json { \"batch\": [ {\"features\": [0.1, 0.2, ...], \"id\": \"sample_1\"}, {\"features\": [0.3, 0.4, ...], \"id\": \"sample_2\"} ] } Response: { \"predictions\": [ { \"id\": \"sample_1\", \"is_attack\": false, \"predicted_class\": \"benign\", \"confidence\": 0.92 }, { \"id\": \"sample_2\", \"is_attack\": true, \"predicted_class\": \"Port Scan\", \"confidence\": 0.78 } ], \"processed_count\": 2 } Python Client RL-IDS includes a Python client for easy integration: from api.client import IDSAPIClient # Initialize client client = IDSAPIClient(\"http://localhost:8000\") # Health check health = await client.health_check() print(f\"API Status: {health['status']}\") # Single prediction features = [0.1, 0.2, 0.3, ...] # 78 features prediction = await client.predict(features) if prediction['is_attack']: print(f\"Attack detected: {prediction['predicted_class']}\") print(f\"Confidence: {prediction['confidence']:.1%}\") # Close client await client.close() Client Methods The IDSAPIClient provides: health_check() - Check API health predict(features) - Single prediction predict_batch(batch) - Batch predictions close() - Close HTTP connections Performance Testing Test API performance: import asyncio from api.client import benchmark_api_performance # Benchmark with 100 requests results = await benchmark_api_performance(100) print(f\"Average response time: {results['avg_time']:.3f}s\") print(f\"Requests per second: {results['rps']:.1f}\") Error Handling The API returns standard HTTP status codes: 200 - Success 400 - Bad Request (invalid features) 422 - Validation Error 500 - Internal Server Error Error Response Format: { \"error\": \"Invalid feature count\", \"details\": \"Expected 78 features, received 77\", \"timestamp\": \"2025-01-27T10:30:00Z\" } Rate Limiting For production deployment, implement rate limiting: # Example with slowapi from slowapi import Limiter, _rate_limit_exceeded_handler from slowapi.util import get_remote_address limiter = Limiter(key_func=get_remote_address) app.state.limiter = limiter @limiter.limit(\"100/minute\") async def predict_endpoint(request: Request, ...): # ... endpoint logic Authentication For production, add authentication: from fastapi import Depends, HTTPException, status from fastapi.security import HTTPBearer security = HTTPBearer() async def verify_token(token: str = Depends(security)): # Implement token verification if not verify_jwt_token(token.credentials): raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid authentication token\" ) OpenAPI Documentation The API automatically generates OpenAPI documentation available at: Swagger UI : http://localhost:8000/docs ReDoc : http://localhost:8000/redoc OpenAPI JSON : http://localhost:8000/openapi.json Configuration Configure the API through environment variables or api/config.py : # api/config.py class Settings: api_host: str = \"0.0.0.0\" api_port: int = 8000 model_path: str = \"models/dqn_model_best.pt\" log_level: str = \"INFO\" cors_origins: List[str] = [\"*\"] Docker Deployment Deploy using Docker: FROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 8000 CMD [\"python\", \"run_api.py\"] # Build and run docker build -t rl-ids-api . docker run -p 8000:8000 rl-ids-api","title":"Overview"},{"location":"api/#api-reference","text":"RL-IDS provides a FastAPI-based REST API for integration with other systems and programmatic access to threat detection capabilities.","title":"API Reference"},{"location":"api/#overview","text":"The RL-IDS API ( api/main.py ) provides a production-ready FastAPI service for real-time network intrusion detection using trained DQN models. The API is designed for high-performance, scalable threat detection with comprehensive error handling and monitoring capabilities.","title":"Overview"},{"location":"api/#quick-start","text":"","title":"Quick Start"},{"location":"api/#start-the-api-server","text":"# Start with default settings python run_api.py # Start with custom host/port python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 The API will be available at http://localhost:8000 with interactive documentation at http://localhost:8000/docs .","title":"Start the API Server"},{"location":"api/#base-configuration","text":"The API is configured through api/config.py and can be customized via environment variables: # Default settings HOST = \"0.0.0.0\" PORT = 8000 MODEL_PATH = \"models/dqn_model_final.pt\"","title":"Base Configuration"},{"location":"api/#api-endpoints","text":"","title":"API Endpoints"},{"location":"api/#root-endpoint","text":"Get basic service information: GET / Response: { \"service\": \"RL-based Intrusion Detection System API\", \"version\": \"1.2.0\", \"status\": \"running\", \"docs\": \"/docs\" }","title":"Root Endpoint"},{"location":"api/#health-check","text":"Check API status and model information: GET /health Response: { \"status\": \"healthy\", \"timestamp\": \"2025-06-27T10:30:00.123456\", \"details\": { \"model_loaded\": true, \"model_path\": \"models/dqn_model_final.pt\", \"predictions_served\": 1234, \"uptime_seconds\": 3600.5, \"memory_usage_mb\": 256.7 } } Error Response (503): { \"detail\": \"Prediction service not initialized\" }","title":"Health Check"},{"location":"api/#model-information","text":"Get detailed information about the loaded model: GET /model/info Response: { \"model_name\": \"DQN_IDS_Model\", \"model_version\": \"1.0.0\", \"model_type\": \"Deep Q-Network\", \"input_features\": 78, \"output_classes\": 15, \"training_episodes\": null, \"model_size_mb\": 2.5, \"class_names\": [ \"BENIGN\", \"Web Attack \u2013 Brute Force\", \"Web Attack \u2013 XSS\", \"Web Attack \u2013 Sql Injection\", \"FTP-Patator\", \"SSH-Patator\", \"PortScan\", \"DoS slowloris\", \"DoS Slowhttptest\", \"DoS Hulk\", \"DoS GoldenEye\", \"Heartbleed\", \"Bot\", \"DDoS\", \"Infiltration\" ], \"classification_type\": \"multi-class\" }","title":"Model Information"},{"location":"api/#single-prediction","text":"Analyze network features for threats: POST /predict Content-Type: application/json { \"features\": [ 0.123, 0.456, 0.789, 0.321, 0.654, 0.987, 0.147, 0.258, 0.369, 0.741, ... // 78 total features ] } Success Response (200): { \"prediction\": 9, \"confidence\": 0.87, \"class_probabilities\": [ 0.01, 0.02, 0.01, 0.02, 0.01, 0.02, 0.01, 0.01, 0.02, 0.87, 0.01, 0.01, 0.01, 0.01, 0.01 ], \"predicted_class\": \"DoS Hulk\", \"is_attack\": true, \"processing_time_ms\": 12.5, \"timestamp\": \"2025-06-27T10:30:45.123456\" } Validation Error (422): { \"detail\": \"Invalid input data: Expected 78 features, got 77\" }","title":"Single Prediction"},{"location":"api/#batch-prediction","text":"Analyze multiple feature sets: POST /predict/batch Content-Type: application/json [ { \"features\": [0.1, 0.2, 0.3, ...] }, { \"features\": [0.4, 0.5, 0.6, ...] } ] Response: [ { \"prediction\": 0, \"confidence\": 0.92, \"class_probabilities\": [0.92, 0.01, 0.01, ...], \"predicted_class\": \"BENIGN\", \"is_attack\": false, \"processing_time_ms\": 8.3, \"timestamp\": \"2025-06-27T10:30:45.123456\" }, { \"prediction\": 6, \"confidence\": 0.78, \"class_probabilities\": [0.05, 0.02, 0.01, 0.01, 0.02, 0.01, 0.78, ...], \"predicted_class\": \"PortScan\", \"is_attack\": true, \"processing_time_ms\": 9.1, \"timestamp\": \"2025-06-27T10:30:45.134567\" } ] Batch Size Limit (413): { \"detail\": \"Batch size too large. Maximum 100 requests allowed.\" }","title":"Batch Prediction"},{"location":"api/#requestresponse-models","text":"","title":"Request/Response Models"},{"location":"api/#idspredictionrequest","text":"class IDSPredictionRequest(BaseModel): features: List[float] = Field( ..., description=\"Network traffic features for prediction\", min_items=1 ) Validation Rules: - Must contain exactly 78 features (CICIDS2017 standard) - All features must be numeric (int or float) - Features list cannot be empty","title":"IDSPredictionRequest"},{"location":"api/#idspredictionresponse","text":"class IDSPredictionResponse(BaseModel): prediction: int # Predicted class (0-14) confidence: float # Confidence score (0.0-1.0) class_probabilities: List[float] # Probability for each class predicted_class: str # Human-readable class name is_attack: bool # True if attack detected (non-benign) processing_time_ms: float # Processing time in milliseconds timestamp: str # Prediction timestamp (ISO format)","title":"IDSPredictionResponse"},{"location":"api/#error-handling","text":"The API returns standard HTTP status codes with detailed error information:","title":"Error Handling"},{"location":"api/#status-codes","text":"200 - Success 422 - Validation Error (invalid input) 500 - Internal Server Error 503 - Service Unavailable (model not loaded) 413 - Request Entity Too Large (batch size exceeded)","title":"Status Codes"},{"location":"api/#error-response-format","text":"{ \"detail\": \"Error description\", \"timestamp\": \"2025-06-27T10:30:45.123456\" }","title":"Error Response Format"},{"location":"api/#common-errors","text":"Feature Count Mismatch: { \"detail\": \"Invalid input data: Expected 78 features, got 75\" } Invalid Feature Values: { \"detail\": \"Invalid input data: All features must be numeric\" } Model Not Loaded: { \"detail\": \"Prediction service not initialized\" }","title":"Common Errors"},{"location":"api/#api-configuration","text":"","title":"API Configuration"},{"location":"api/#environment-variables","text":"Configure the API through environment variables: # API server settings RLIDS_API_HOST=0.0.0.0 RLIDS_API_PORT=8000 # Model settings RLIDS_MODEL_PATH=models/dqn_model_best.pt # Logging RLIDS_LOG_LEVEL=INFO RLIDS_DEBUG=false","title":"Environment Variables"},{"location":"api/#cors-configuration","text":"The API includes CORS middleware for cross-origin requests: app.add_middleware( CORSMiddleware, allow_origins=[\"*\"], # Configure for production allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"], )","title":"CORS Configuration"},{"location":"api/#service-architecture","text":"","title":"Service Architecture"},{"location":"api/#application-lifecycle","text":"The API uses FastAPI's lifespan events for proper service management: @asynccontextmanager async def lifespan(app: FastAPI): # Startup global prediction_service model_path = MODELS_DIR / \"dqn_model_final.pt\" prediction_service = IDSPredictionService(model_path=model_path) await prediction_service.initialize() yield # Shutdown if prediction_service: await prediction_service.cleanup()","title":"Application Lifecycle"},{"location":"api/#prediction-service","text":"The IDSPredictionService class handles: Model Loading : Loads trained DQN models with configuration Feature Validation : Ensures input features match expected format Prediction Processing : Runs inference with timing metrics Resource Management : Handles GPU/CPU memory and cleanup","title":"Prediction Service"},{"location":"api/#performance-characteristics","text":"Model Loading Time : ~2-5 seconds on startup Prediction Latency : ~8-15ms per prediction Memory Usage : ~200-500MB depending on model size Throughput : ~100-500 predictions/second (depends on hardware)","title":"Performance Characteristics"},{"location":"api/#openapi-documentation","text":"The API automatically generates comprehensive documentation: Swagger UI : http://localhost:8000/docs ReDoc : http://localhost:8000/redoc OpenAPI Spec : http://localhost:8000/openapi.json","title":"OpenAPI Documentation"},{"location":"api/#interactive-testing","text":"The Swagger UI provides interactive testing capabilities: 1. Navigate to http://localhost:8000/docs 2. Expand endpoint sections 3. Click \"Try it out\" 4. Input test data 5. Execute requests and view responses","title":"Interactive Testing"},{"location":"api/#security-considerations","text":"","title":"Security Considerations"},{"location":"api/#production-deployment","text":"For production environments, consider: Authentication : Add API key or JWT token authentication from fastapi.security import HTTPBearer security = HTTPBearer() @app.post(\"/predict\") async def predict(request: IDSPredictionRequest, token: str = Depends(security)): # Verify token pass Rate Limiting : Implement request rate limiting from slowapi import Limiter limiter = Limiter(key_func=get_remote_address) @limiter.limit(\"100/minute\") @app.post(\"/predict\") async def predict(request: Request, ...): pass HTTPS : Use TLS/SSL in production uvicorn api.main:app --host 0.0.0.0 --port 443 --ssl-keyfile key.pem --ssl-certfile cert.pem CORS : Restrict origins for production app.add_middleware( CORSMiddleware, allow_origins=[\"https://yourdomain.com\"], allow_credentials=True, allow_methods=[\"GET\", \"POST\"], allow_headers=[\"*\"], )","title":"Production Deployment"},{"location":"api/#input-validation","text":"The API includes comprehensive input validation: - Feature count validation (must be exactly 78) - Numeric type checking for all features - Range validation to prevent extreme values - Batch size limits to prevent resource exhaustion","title":"Input Validation"},{"location":"api/#monitoring-and-logging","text":"","title":"Monitoring and Logging"},{"location":"api/#health-monitoring","text":"The /health endpoint provides detailed service status: - Model loading status - Memory usage metrics - Prediction statistics - Service uptime","title":"Health Monitoring"},{"location":"api/#logging-configuration","text":"Configure logging levels and outputs: # In api/config.py LOG_LEVEL = os.getenv(\"RLIDS_LOG_LEVEL\", \"INFO\") Log Levels: - DEBUG : Detailed debugging information - INFO : General operational messages - WARNING : Warning conditions - ERROR : Error conditions - CRITICAL : Critical error conditions","title":"Logging Configuration"},{"location":"api/#performance-metrics","text":"The API tracks performance metrics: - Prediction processing time - Total predictions served - Error rates and types - Memory usage patterns RL-IDS provides a FastAPI-based REST API for integration with other systems and programmatic access to threat detection capabilities.","title":"Performance Metrics"},{"location":"api/#quick-start_1","text":"Start the API server: python run_api.py The API will be available at http://localhost:8000 with interactive documentation at http://localhost:8000/docs .","title":"Quick Start"},{"location":"api/#base-configuration_1","text":"The API is configured through api/config.py and can be customized via environment variables. Default settings: - Host : 0.0.0.0 - Port : 8000 - Model Path : models/dqn_model_best.pt","title":"Base Configuration"},{"location":"api/#core-endpoints","text":"","title":"Core Endpoints"},{"location":"api/#health-check_1","text":"Check API status and model information: GET /health Response: { \"status\": \"healthy\", \"timestamp\": \"2025-01-27T10:30:00Z\", \"model_loaded\": true, \"model_path\": \"models/dqn_model_best.pt\", \"api_version\": \"1.2.0\" }","title":"Health Check"},{"location":"api/#threat-prediction","text":"Analyze network features for threats: POST /predict Content-Type: application/json { \"features\": [0.1, 0.2, 0.3, ...], // 78 CICIDS2017 features \"metadata\": { \"source_ip\": \"192.168.1.100\", \"timestamp\": \"2025-01-27T10:30:00Z\" } } Response: { \"is_attack\": true, \"predicted_class\": \"DoS Hulk\", \"confidence\": 0.87, \"model_version\": \"dqn_v1\", \"timestamp\": \"2025-01-27T10:30:00Z\", \"features_count\": 78 }","title":"Threat Prediction"},{"location":"api/#batch-prediction_1","text":"Analyze multiple feature sets: POST /predict_batch Content-Type: application/json { \"batch\": [ {\"features\": [0.1, 0.2, ...], \"id\": \"sample_1\"}, {\"features\": [0.3, 0.4, ...], \"id\": \"sample_2\"} ] } Response: { \"predictions\": [ { \"id\": \"sample_1\", \"is_attack\": false, \"predicted_class\": \"benign\", \"confidence\": 0.92 }, { \"id\": \"sample_2\", \"is_attack\": true, \"predicted_class\": \"Port Scan\", \"confidence\": 0.78 } ], \"processed_count\": 2 }","title":"Batch Prediction"},{"location":"api/#python-client","text":"RL-IDS includes a Python client for easy integration: from api.client import IDSAPIClient # Initialize client client = IDSAPIClient(\"http://localhost:8000\") # Health check health = await client.health_check() print(f\"API Status: {health['status']}\") # Single prediction features = [0.1, 0.2, 0.3, ...] # 78 features prediction = await client.predict(features) if prediction['is_attack']: print(f\"Attack detected: {prediction['predicted_class']}\") print(f\"Confidence: {prediction['confidence']:.1%}\") # Close client await client.close()","title":"Python Client"},{"location":"api/#client-methods","text":"The IDSAPIClient provides: health_check() - Check API health predict(features) - Single prediction predict_batch(batch) - Batch predictions close() - Close HTTP connections","title":"Client Methods"},{"location":"api/#performance-testing","text":"Test API performance: import asyncio from api.client import benchmark_api_performance # Benchmark with 100 requests results = await benchmark_api_performance(100) print(f\"Average response time: {results['avg_time']:.3f}s\") print(f\"Requests per second: {results['rps']:.1f}\")","title":"Performance Testing"},{"location":"api/#error-handling_1","text":"The API returns standard HTTP status codes: 200 - Success 400 - Bad Request (invalid features) 422 - Validation Error 500 - Internal Server Error Error Response Format: { \"error\": \"Invalid feature count\", \"details\": \"Expected 78 features, received 77\", \"timestamp\": \"2025-01-27T10:30:00Z\" }","title":"Error Handling"},{"location":"api/#rate-limiting","text":"For production deployment, implement rate limiting: # Example with slowapi from slowapi import Limiter, _rate_limit_exceeded_handler from slowapi.util import get_remote_address limiter = Limiter(key_func=get_remote_address) app.state.limiter = limiter @limiter.limit(\"100/minute\") async def predict_endpoint(request: Request, ...): # ... endpoint logic","title":"Rate Limiting"},{"location":"api/#authentication","text":"For production, add authentication: from fastapi import Depends, HTTPException, status from fastapi.security import HTTPBearer security = HTTPBearer() async def verify_token(token: str = Depends(security)): # Implement token verification if not verify_jwt_token(token.credentials): raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid authentication token\" )","title":"Authentication"},{"location":"api/#openapi-documentation_1","text":"The API automatically generates OpenAPI documentation available at: Swagger UI : http://localhost:8000/docs ReDoc : http://localhost:8000/redoc OpenAPI JSON : http://localhost:8000/openapi.json","title":"OpenAPI Documentation"},{"location":"api/#configuration","text":"Configure the API through environment variables or api/config.py : # api/config.py class Settings: api_host: str = \"0.0.0.0\" api_port: int = 8000 model_path: str = \"models/dqn_model_best.pt\" log_level: str = \"INFO\" cors_origins: List[str] = [\"*\"]","title":"Configuration"},{"location":"api/#docker-deployment","text":"Deploy using Docker: FROM python:3.9-slim WORKDIR /app COPY requirements.txt . RUN pip install -r requirements.txt COPY . . EXPOSE 8000 CMD [\"python\", \"run_api.py\"] # Build and run docker build -t rl-ids-api . docker run -p 8000:8000 rl-ids-api","title":"Docker Deployment"},{"location":"api/examples/","text":"API Examples This page provides comprehensive examples of using the RL-IDS API for various integration scenarios. Basic Usage Examples 1. Service Health Check # Check if the API service is running curl -X GET http://localhost:8000/health # Expected response { \"status\": \"healthy\", \"timestamp\": \"2024-01-15T10:30:00Z\", \"details\": { \"model_loaded\": true, \"api_version\": \"1.2.0\", \"memory_usage\": 45.6, \"uptime\": 3600 } } 2. Get Model Information # Get information about the loaded model curl -X GET http://localhost:8000/model/info # Expected response { \"model_name\": \"DQN_IDS_Model\", \"model_type\": \"Deep Q-Network\", \"version\": \"1.2.0\", \"input_features\": 78, \"output_classes\": 2, \"class_names\": [\"BENIGN\", \"ATTACK\"], \"training_date\": \"2024-01-10T15:30:00Z\", \"accuracy\": 0.953, \"false_positive_rate\": 0.047 } 3. Single Prediction # Make a single prediction curl -X POST http://localhost:8000/predict \\ -H \"Content-Type: application/json\" \\ -d '{ \"features\": [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8 ] }' # Expected response { \"prediction\": { \"class\": \"BENIGN\", \"confidence\": 0.87, \"probability\": { \"BENIGN\": 0.87, \"ATTACK\": 0.13 } }, \"timestamp\": \"2024-01-15T10:35:00Z\", \"processing_time\": 0.045 } 4. Batch Predictions # Make multiple predictions at once curl -X POST http://localhost:8000/predict/batch \\ -H \"Content-Type: application/json\" \\ -d '{ \"features\": [ [0.1, 0.2, 0.3, ..., 7.8], [1.1, 1.2, 1.3, ..., 8.8], [2.1, 2.2, 2.3, ..., 9.8] ] }' # Expected response { \"predictions\": [ { \"class\": \"BENIGN\", \"confidence\": 0.87, \"probability\": {\"BENIGN\": 0.87, \"ATTACK\": 0.13} }, { \"class\": \"ATTACK\", \"confidence\": 0.92, \"probability\": {\"BENIGN\": 0.08, \"ATTACK\": 0.92} }, { \"class\": \"BENIGN\", \"confidence\": 0.79, \"probability\": {\"BENIGN\": 0.79, \"ATTACK\": 0.21} } ], \"timestamp\": \"2024-01-15T10:40:00Z\", \"processing_time\": 0.098, \"total_predictions\": 3 } Python Integration Examples 1. Using Requests Library import requests import json # API configuration API_BASE_URL = \"http://localhost:8000\" def check_api_health(): \"\"\"Check if the API is healthy and ready.\"\"\" try: response = requests.get(f\"{API_BASE_URL}/health\") response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Health check failed: {e}\") return None def get_model_info(): \"\"\"Get information about the loaded model.\"\"\" try: response = requests.get(f\"{API_BASE_URL}/model/info\") response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Failed to get model info: {e}\") return None def make_prediction(features): \"\"\"Make a single prediction.\"\"\" try: payload = {\"features\": features} response = requests.post( f\"{API_BASE_URL}/predict\", json=payload, headers={\"Content-Type\": \"application/json\"} ) response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Prediction failed: {e}\") return None def make_batch_predictions(features_list): \"\"\"Make multiple predictions at once.\"\"\" try: payload = {\"features\": features_list} response = requests.post( f\"{API_BASE_URL}/predict/batch\", json=payload, headers={\"Content-Type\": \"application/json\"} ) response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Batch prediction failed: {e}\") return None # Example usage if __name__ == \"__main__\": # Check API health health = check_api_health() print(f\"API Health: {health}\") # Get model information model_info = get_model_info() print(f\"Model Info: {model_info}\") # Sample feature vector (78 features) sample_features = [0.1] * 78 # Make single prediction prediction = make_prediction(sample_features) print(f\"Prediction: {prediction}\") # Make batch predictions batch_features = [sample_features] * 3 batch_predictions = make_batch_predictions(batch_features) print(f\"Batch Predictions: {batch_predictions}\") 2. Using the Official Python Client from api.client import RLIDSClient import asyncio async def main(): # Initialize client client = RLIDSClient(\"http://localhost:8000\") try: # Check health health = await client.health_check() print(f\"Health: {health}\") # Get model info model_info = await client.get_model_info() print(f\"Model: {model_info}\") # Sample features features = [0.1] * 78 # Single prediction prediction = await client.predict(features) print(f\"Prediction: {prediction}\") # Batch prediction batch_features = [features] * 5 batch_predictions = await client.predict_batch(batch_features) print(f\"Batch Results: {batch_predictions}\") except Exception as e: print(f\"Error: {e}\") finally: await client.close() # Run the example if __name__ == \"__main__\": asyncio.run(main()) 3. Real-time Network Monitoring Integration import asyncio import time from api.client import RLIDSClient from rl_ids.make_dataset import extract_features class NetworkMonitor: def __init__(self, api_url=\"http://localhost:8000\"): self.client = RLIDSClient(api_url) self.running = False async def start_monitoring(self): \"\"\"Start real-time network monitoring.\"\"\" self.running = True print(\"Starting network monitoring...\") try: while self.running: # Capture network data (simplified) network_data = self.capture_network_data() if network_data: # Extract features features = extract_features(network_data) # Make prediction result = await self.client.predict(features) # Process result await self.process_prediction(result, network_data) # Wait before next capture await asyncio.sleep(1.0) except KeyboardInterrupt: print(\"Monitoring stopped by user\") except Exception as e: print(f\"Monitoring error: {e}\") finally: await self.client.close() def capture_network_data(self): \"\"\"Capture network data (placeholder).\"\"\" # This would contain actual packet capture logic # For demonstration, return mock data return { 'packets': 100, 'bytes': 5000, 'duration': 1.0, 'protocols': ['TCP', 'UDP'] } async def process_prediction(self, prediction, network_data): \"\"\"Process prediction results.\"\"\" if prediction['prediction']['class'] == 'ATTACK': confidence = prediction['prediction']['confidence'] print(f\"ALERT: Attack detected with {confidence:.2%} confidence\") print(f\"Network data: {network_data}\") # Here you would implement alerting logic await self.send_alert(prediction, network_data) else: print(f\"Normal traffic detected ({prediction['prediction']['confidence']:.2%})\") async def send_alert(self, prediction, network_data): \"\"\"Send alert for detected attack.\"\"\" alert = { 'timestamp': time.time(), 'type': 'ATTACK_DETECTED', 'confidence': prediction['prediction']['confidence'], 'network_data': network_data } # Implement your alerting mechanism here print(f\"Sending alert: {alert}\") def stop_monitoring(self): \"\"\"Stop the monitoring process.\"\"\" self.running = False # Usage example if __name__ == \"__main__\": monitor = NetworkMonitor() asyncio.run(monitor.start_monitoring()) Error Handling Examples 1. Handling API Errors import requests from requests.exceptions import RequestException, Timeout, ConnectionError def robust_prediction(features, max_retries=3): \"\"\"Make prediction with error handling and retries.\"\"\" for attempt in range(max_retries): try: response = requests.post( \"http://localhost:8000/predict\", json={\"features\": features}, timeout=30 ) # Check for HTTP errors if response.status_code == 422: print(\"Validation error: Invalid input format\") return None response.raise_for_status() return response.json() except ConnectionError: print(f\"Connection error (attempt {attempt + 1}/{max_retries})\") if attempt < max_retries - 1: time.sleep(2 ** attempt) # Exponential backoff except Timeout: print(f\"Request timeout (attempt {attempt + 1}/{max_retries})\") if attempt < max_retries - 1: time.sleep(1) except RequestException as e: print(f\"Request failed: {e}\") break print(\"All retry attempts failed\") return None # Example usage features = [0.1] * 78 result = robust_prediction(features) if result: print(f\"Prediction successful: {result}\") else: print(\"Prediction failed after all retries\") 2. Input Validation def validate_features(features): \"\"\"Validate feature input before sending to API.\"\"\" if not isinstance(features, list): raise ValueError(\"Features must be a list\") if len(features) != 78: raise ValueError(f\"Expected 78 features, got {len(features)}\") for i, feature in enumerate(features): if not isinstance(feature, (int, float)): raise ValueError(f\"Feature {i} must be numeric, got {type(feature)}\") if not (-1000 <= feature <= 1000): # Reasonable bounds raise ValueError(f\"Feature {i} value {feature} is out of reasonable bounds\") return True def safe_prediction(features): \"\"\"Make prediction with input validation.\"\"\" try: # Validate input validate_features(features) # Make prediction response = requests.post( \"http://localhost:8000/predict\", json={\"features\": features} ) if response.status_code == 200: return response.json() else: print(f\"API error: {response.status_code} - {response.text}\") return None except ValueError as e: print(f\"Input validation error: {e}\") return None except Exception as e: print(f\"Unexpected error: {e}\") return None # Example usage try: features = [0.1] * 78 # Valid input result = safe_prediction(features) invalid_features = [0.1] * 77 # Invalid input result = safe_prediction(invalid_features) # Will show validation error except Exception as e: print(f\"Error: {e}\") Performance Optimization Examples 1. Batch Processing for High Throughput import asyncio import aiohttp from typing import List, Dict class BatchProcessor: def __init__(self, api_url: str, batch_size: int = 100): self.api_url = api_url self.batch_size = batch_size self.session = None async def __aenter__(self): self.session = aiohttp.ClientSession() return self async def __aexit__(self, exc_type, exc_val, exc_tb): if self.session: await self.session.close() async def process_large_dataset(self, features_list: List[List[float]]): \"\"\"Process a large dataset in batches.\"\"\" results = [] for i in range(0, len(features_list), self.batch_size): batch = features_list[i:i + self.batch_size] try: batch_result = await self._process_batch(batch) results.extend(batch_result) print(f\"Processed batch {i//self.batch_size + 1}, \" f\"total processed: {len(results)}\") except Exception as e: print(f\"Batch processing error: {e}\") # Continue with next batch continue return results async def _process_batch(self, batch: List[List[float]]): \"\"\"Process a single batch.\"\"\" async with self.session.post( f\"{self.api_url}/predict/batch\", json={\"features\": batch} ) as response: if response.status == 200: data = await response.json() return data[\"predictions\"] else: raise Exception(f\"API error: {response.status}\") # Usage example async def main(): # Generate sample data large_dataset = [[0.1] * 78 for _ in range(1000)] async with BatchProcessor(\"http://localhost:8000\", batch_size=50) as processor: results = await processor.process_large_dataset(large_dataset) print(f\"Processed {len(results)} predictions\") # Analyze results attack_count = sum(1 for r in results if r[\"class\"] == \"ATTACK\") print(f\"Detected {attack_count} attacks out of {len(results)} samples\") if __name__ == \"__main__\": asyncio.run(main()) 2. Connection Pooling and Reuse import aiohttp import asyncio from typing import Optional class OptimizedClient: def __init__(self, base_url: str): self.base_url = base_url self.session: Optional[aiohttp.ClientSession] = None async def __aenter__(self): # Configure connection pool connector = aiohttp.TCPConnector( limit=100, # Total connection pool size limit_per_host=30, # Per-host connection limit ttl_dns_cache=300, # DNS cache TTL use_dns_cache=True, ) # Configure timeout timeout = aiohttp.ClientTimeout(total=30, connect=5) self.session = aiohttp.ClientSession( connector=connector, timeout=timeout, headers={\"Content-Type\": \"application/json\"} ) return self async def __aexit__(self, exc_type, exc_val, exc_tb): if self.session: await self.session.close() async def predict(self, features: List[float]) -> Dict: \"\"\"Make a single prediction with optimized connection.\"\"\" async with self.session.post( f\"{self.base_url}/predict\", json={\"features\": features} ) as response: return await response.json() async def predict_many(self, features_list: List[List[float]]) -> List[Dict]: \"\"\"Make multiple concurrent predictions.\"\"\" tasks = [] for features in features_list: task = asyncio.create_task(self.predict(features)) tasks.append(task) results = await asyncio.gather(*tasks, return_exceptions=True) # Filter out exceptions valid_results = [r for r in results if not isinstance(r, Exception)] return valid_results # Usage example async def performance_test(): features_list = [[0.1] * 78 for _ in range(100)] async with OptimizedClient(\"http://localhost:8000\") as client: start_time = asyncio.get_event_loop().time() results = await client.predict_many(features_list) end_time = asyncio.get_event_loop().time() print(f\"Processed {len(results)} predictions in {end_time - start_time:.2f} seconds\") print(f\"Average time per prediction: {(end_time - start_time) / len(results):.3f} seconds\") if __name__ == \"__main__\": asyncio.run(performance_test()) Integration with Security Tools 1. SIEM Integration Example import json import logging from datetime import datetime from api.client import RLIDSClient class SIEMIntegration: def __init__(self, api_url: str, siem_endpoint: str): self.client = RLIDSClient(api_url) self.siem_endpoint = siem_endpoint self.logger = logging.getLogger(__name__) async def process_network_event(self, network_event: Dict): \"\"\"Process network event and send to SIEM if threat detected.\"\"\" try: # Extract features from network event features = self.extract_features(network_event) # Get prediction prediction = await self.client.predict(features) # Create enriched event enriched_event = { 'timestamp': datetime.utcnow().isoformat(), 'source_ip': network_event.get('source_ip'), 'dest_ip': network_event.get('dest_ip'), 'protocol': network_event.get('protocol'), 'original_event': network_event, 'ml_prediction': prediction, 'threat_score': prediction['prediction']['confidence'], 'classification': prediction['prediction']['class'] } # Send to SIEM if attack detected if prediction['prediction']['class'] == 'ATTACK': await self.send_to_siem(enriched_event) return enriched_event except Exception as e: self.logger.error(f\"Failed to process network event: {e}\") return None def extract_features(self, network_event: Dict) -> List[float]: \"\"\"Extract ML features from network event.\"\"\" # This would contain actual feature extraction logic # For demonstration, return mock features return [0.1] * 78 async def send_to_siem(self, event: Dict): \"\"\"Send enriched event to SIEM.\"\"\" try: # Format for SIEM (this example uses JSON) siem_event = { 'timestamp': event['timestamp'], 'event_type': 'ML_THREAT_DETECTION', 'severity': self.calculate_severity(event), 'source': 'RL-IDS', 'details': event } # Send to SIEM endpoint # This would be actual SIEM API call self.logger.info(f\"Sending to SIEM: {json.dumps(siem_event, indent=2)}\") except Exception as e: self.logger.error(f\"Failed to send to SIEM: {e}\") def calculate_severity(self, event: Dict) -> str: \"\"\"Calculate event severity based on confidence.\"\"\" confidence = event['threat_score'] if confidence >= 0.9: return 'HIGH' elif confidence >= 0.7: return 'MEDIUM' else: return 'LOW' # Usage example async def main(): siem = SIEMIntegration( api_url=\"http://localhost:8000\", siem_endpoint=\"https://siem.example.com/api/events\" ) # Sample network event network_event = { 'source_ip': '192.168.1.100', 'dest_ip': '10.0.0.1', 'protocol': 'TCP', 'port': 80, 'packet_count': 1000, 'byte_count': 50000 } result = await siem.process_network_event(network_event) print(f\"Processed event: {result}\") if __name__ == \"__main__\": asyncio.run(main()) Monitoring and Alerting Examples 1. Health Monitoring import asyncio import logging from datetime import datetime, timedelta from api.client import RLIDSClient class HealthMonitor: def __init__(self, api_url: str, check_interval: int = 60): self.client = RLIDSClient(api_url) self.check_interval = check_interval self.logger = logging.getLogger(__name__) self.alert_cooldown = {} async def start_monitoring(self): \"\"\"Start continuous health monitoring.\"\"\" self.logger.info(\"Starting health monitoring...\") while True: try: await self.check_health() await asyncio.sleep(self.check_interval) except KeyboardInterrupt: self.logger.info(\"Health monitoring stopped\") break except Exception as e: self.logger.error(f\"Health monitoring error: {e}\") await asyncio.sleep(self.check_interval) async def check_health(self): \"\"\"Perform health check.\"\"\" try: # Get health status health = await self.client.health_check() if health['status'] != 'healthy': await self.send_alert('UNHEALTHY', health) return # Check model status if not health['details'].get('model_loaded', False): await self.send_alert('MODEL_NOT_LOADED', health) return # Check memory usage memory_usage = health['details'].get('memory_usage', 0) if memory_usage > 80: # 80% threshold await self.send_alert('HIGH_MEMORY', health) # Check response time start_time = asyncio.get_event_loop().time() await self.client.get_model_info() response_time = asyncio.get_event_loop().time() - start_time if response_time > 5.0: # 5 second threshold await self.send_alert('SLOW_RESPONSE', { 'response_time': response_time, 'health': health }) self.logger.info(f\"Health check passed: {health['status']}\") except Exception as e: await self.send_alert('API_ERROR', {'error': str(e)}) async def send_alert(self, alert_type: str, details: Dict): \"\"\"Send alert with cooldown to prevent spam.\"\"\" now = datetime.utcnow() # Check cooldown if alert_type in self.alert_cooldown: last_alert = self.alert_cooldown[alert_type] if now - last_alert < timedelta(minutes=5): return # Skip alert due to cooldown # Send alert alert = { 'timestamp': now.isoformat(), 'type': alert_type, 'details': details, 'severity': self.get_alert_severity(alert_type) } self.logger.error(f\"ALERT: {json.dumps(alert, indent=2)}\") # Update cooldown self.alert_cooldown[alert_type] = now def get_alert_severity(self, alert_type: str) -> str: \"\"\"Get alert severity level.\"\"\" severity_map = { 'UNHEALTHY': 'CRITICAL', 'MODEL_NOT_LOADED': 'CRITICAL', 'API_ERROR': 'HIGH', 'HIGH_MEMORY': 'MEDIUM', 'SLOW_RESPONSE': 'LOW' } return severity_map.get(alert_type, 'MEDIUM') # Usage example if __name__ == \"__main__\": monitor = HealthMonitor(\"http://localhost:8000\", check_interval=30) asyncio.run(monitor.start_monitoring()) These examples demonstrate various ways to integrate and use the RL-IDS API effectively in different scenarios, from simple predictions to complex production integrations with security tools and monitoring systems.","title":"Examples"},{"location":"api/examples/#api-examples","text":"This page provides comprehensive examples of using the RL-IDS API for various integration scenarios.","title":"API Examples"},{"location":"api/examples/#basic-usage-examples","text":"","title":"Basic Usage Examples"},{"location":"api/examples/#1-service-health-check","text":"# Check if the API service is running curl -X GET http://localhost:8000/health # Expected response { \"status\": \"healthy\", \"timestamp\": \"2024-01-15T10:30:00Z\", \"details\": { \"model_loaded\": true, \"api_version\": \"1.2.0\", \"memory_usage\": 45.6, \"uptime\": 3600 } }","title":"1. Service Health Check"},{"location":"api/examples/#2-get-model-information","text":"# Get information about the loaded model curl -X GET http://localhost:8000/model/info # Expected response { \"model_name\": \"DQN_IDS_Model\", \"model_type\": \"Deep Q-Network\", \"version\": \"1.2.0\", \"input_features\": 78, \"output_classes\": 2, \"class_names\": [\"BENIGN\", \"ATTACK\"], \"training_date\": \"2024-01-10T15:30:00Z\", \"accuracy\": 0.953, \"false_positive_rate\": 0.047 }","title":"2. Get Model Information"},{"location":"api/examples/#3-single-prediction","text":"# Make a single prediction curl -X POST http://localhost:8000/predict \\ -H \"Content-Type: application/json\" \\ -d '{ \"features\": [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4, 6.5, 6.6, 6.7, 6.8, 6.9, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5, 7.6, 7.7, 7.8 ] }' # Expected response { \"prediction\": { \"class\": \"BENIGN\", \"confidence\": 0.87, \"probability\": { \"BENIGN\": 0.87, \"ATTACK\": 0.13 } }, \"timestamp\": \"2024-01-15T10:35:00Z\", \"processing_time\": 0.045 }","title":"3. Single Prediction"},{"location":"api/examples/#4-batch-predictions","text":"# Make multiple predictions at once curl -X POST http://localhost:8000/predict/batch \\ -H \"Content-Type: application/json\" \\ -d '{ \"features\": [ [0.1, 0.2, 0.3, ..., 7.8], [1.1, 1.2, 1.3, ..., 8.8], [2.1, 2.2, 2.3, ..., 9.8] ] }' # Expected response { \"predictions\": [ { \"class\": \"BENIGN\", \"confidence\": 0.87, \"probability\": {\"BENIGN\": 0.87, \"ATTACK\": 0.13} }, { \"class\": \"ATTACK\", \"confidence\": 0.92, \"probability\": {\"BENIGN\": 0.08, \"ATTACK\": 0.92} }, { \"class\": \"BENIGN\", \"confidence\": 0.79, \"probability\": {\"BENIGN\": 0.79, \"ATTACK\": 0.21} } ], \"timestamp\": \"2024-01-15T10:40:00Z\", \"processing_time\": 0.098, \"total_predictions\": 3 }","title":"4. Batch Predictions"},{"location":"api/examples/#python-integration-examples","text":"","title":"Python Integration Examples"},{"location":"api/examples/#1-using-requests-library","text":"import requests import json # API configuration API_BASE_URL = \"http://localhost:8000\" def check_api_health(): \"\"\"Check if the API is healthy and ready.\"\"\" try: response = requests.get(f\"{API_BASE_URL}/health\") response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Health check failed: {e}\") return None def get_model_info(): \"\"\"Get information about the loaded model.\"\"\" try: response = requests.get(f\"{API_BASE_URL}/model/info\") response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Failed to get model info: {e}\") return None def make_prediction(features): \"\"\"Make a single prediction.\"\"\" try: payload = {\"features\": features} response = requests.post( f\"{API_BASE_URL}/predict\", json=payload, headers={\"Content-Type\": \"application/json\"} ) response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Prediction failed: {e}\") return None def make_batch_predictions(features_list): \"\"\"Make multiple predictions at once.\"\"\" try: payload = {\"features\": features_list} response = requests.post( f\"{API_BASE_URL}/predict/batch\", json=payload, headers={\"Content-Type\": \"application/json\"} ) response.raise_for_status() return response.json() except requests.exceptions.RequestException as e: print(f\"Batch prediction failed: {e}\") return None # Example usage if __name__ == \"__main__\": # Check API health health = check_api_health() print(f\"API Health: {health}\") # Get model information model_info = get_model_info() print(f\"Model Info: {model_info}\") # Sample feature vector (78 features) sample_features = [0.1] * 78 # Make single prediction prediction = make_prediction(sample_features) print(f\"Prediction: {prediction}\") # Make batch predictions batch_features = [sample_features] * 3 batch_predictions = make_batch_predictions(batch_features) print(f\"Batch Predictions: {batch_predictions}\")","title":"1. Using Requests Library"},{"location":"api/examples/#2-using-the-official-python-client","text":"from api.client import RLIDSClient import asyncio async def main(): # Initialize client client = RLIDSClient(\"http://localhost:8000\") try: # Check health health = await client.health_check() print(f\"Health: {health}\") # Get model info model_info = await client.get_model_info() print(f\"Model: {model_info}\") # Sample features features = [0.1] * 78 # Single prediction prediction = await client.predict(features) print(f\"Prediction: {prediction}\") # Batch prediction batch_features = [features] * 5 batch_predictions = await client.predict_batch(batch_features) print(f\"Batch Results: {batch_predictions}\") except Exception as e: print(f\"Error: {e}\") finally: await client.close() # Run the example if __name__ == \"__main__\": asyncio.run(main())","title":"2. Using the Official Python Client"},{"location":"api/examples/#3-real-time-network-monitoring-integration","text":"import asyncio import time from api.client import RLIDSClient from rl_ids.make_dataset import extract_features class NetworkMonitor: def __init__(self, api_url=\"http://localhost:8000\"): self.client = RLIDSClient(api_url) self.running = False async def start_monitoring(self): \"\"\"Start real-time network monitoring.\"\"\" self.running = True print(\"Starting network monitoring...\") try: while self.running: # Capture network data (simplified) network_data = self.capture_network_data() if network_data: # Extract features features = extract_features(network_data) # Make prediction result = await self.client.predict(features) # Process result await self.process_prediction(result, network_data) # Wait before next capture await asyncio.sleep(1.0) except KeyboardInterrupt: print(\"Monitoring stopped by user\") except Exception as e: print(f\"Monitoring error: {e}\") finally: await self.client.close() def capture_network_data(self): \"\"\"Capture network data (placeholder).\"\"\" # This would contain actual packet capture logic # For demonstration, return mock data return { 'packets': 100, 'bytes': 5000, 'duration': 1.0, 'protocols': ['TCP', 'UDP'] } async def process_prediction(self, prediction, network_data): \"\"\"Process prediction results.\"\"\" if prediction['prediction']['class'] == 'ATTACK': confidence = prediction['prediction']['confidence'] print(f\"ALERT: Attack detected with {confidence:.2%} confidence\") print(f\"Network data: {network_data}\") # Here you would implement alerting logic await self.send_alert(prediction, network_data) else: print(f\"Normal traffic detected ({prediction['prediction']['confidence']:.2%})\") async def send_alert(self, prediction, network_data): \"\"\"Send alert for detected attack.\"\"\" alert = { 'timestamp': time.time(), 'type': 'ATTACK_DETECTED', 'confidence': prediction['prediction']['confidence'], 'network_data': network_data } # Implement your alerting mechanism here print(f\"Sending alert: {alert}\") def stop_monitoring(self): \"\"\"Stop the monitoring process.\"\"\" self.running = False # Usage example if __name__ == \"__main__\": monitor = NetworkMonitor() asyncio.run(monitor.start_monitoring())","title":"3. Real-time Network Monitoring Integration"},{"location":"api/examples/#error-handling-examples","text":"","title":"Error Handling Examples"},{"location":"api/examples/#1-handling-api-errors","text":"import requests from requests.exceptions import RequestException, Timeout, ConnectionError def robust_prediction(features, max_retries=3): \"\"\"Make prediction with error handling and retries.\"\"\" for attempt in range(max_retries): try: response = requests.post( \"http://localhost:8000/predict\", json={\"features\": features}, timeout=30 ) # Check for HTTP errors if response.status_code == 422: print(\"Validation error: Invalid input format\") return None response.raise_for_status() return response.json() except ConnectionError: print(f\"Connection error (attempt {attempt + 1}/{max_retries})\") if attempt < max_retries - 1: time.sleep(2 ** attempt) # Exponential backoff except Timeout: print(f\"Request timeout (attempt {attempt + 1}/{max_retries})\") if attempt < max_retries - 1: time.sleep(1) except RequestException as e: print(f\"Request failed: {e}\") break print(\"All retry attempts failed\") return None # Example usage features = [0.1] * 78 result = robust_prediction(features) if result: print(f\"Prediction successful: {result}\") else: print(\"Prediction failed after all retries\")","title":"1. Handling API Errors"},{"location":"api/examples/#2-input-validation","text":"def validate_features(features): \"\"\"Validate feature input before sending to API.\"\"\" if not isinstance(features, list): raise ValueError(\"Features must be a list\") if len(features) != 78: raise ValueError(f\"Expected 78 features, got {len(features)}\") for i, feature in enumerate(features): if not isinstance(feature, (int, float)): raise ValueError(f\"Feature {i} must be numeric, got {type(feature)}\") if not (-1000 <= feature <= 1000): # Reasonable bounds raise ValueError(f\"Feature {i} value {feature} is out of reasonable bounds\") return True def safe_prediction(features): \"\"\"Make prediction with input validation.\"\"\" try: # Validate input validate_features(features) # Make prediction response = requests.post( \"http://localhost:8000/predict\", json={\"features\": features} ) if response.status_code == 200: return response.json() else: print(f\"API error: {response.status_code} - {response.text}\") return None except ValueError as e: print(f\"Input validation error: {e}\") return None except Exception as e: print(f\"Unexpected error: {e}\") return None # Example usage try: features = [0.1] * 78 # Valid input result = safe_prediction(features) invalid_features = [0.1] * 77 # Invalid input result = safe_prediction(invalid_features) # Will show validation error except Exception as e: print(f\"Error: {e}\")","title":"2. Input Validation"},{"location":"api/examples/#performance-optimization-examples","text":"","title":"Performance Optimization Examples"},{"location":"api/examples/#1-batch-processing-for-high-throughput","text":"import asyncio import aiohttp from typing import List, Dict class BatchProcessor: def __init__(self, api_url: str, batch_size: int = 100): self.api_url = api_url self.batch_size = batch_size self.session = None async def __aenter__(self): self.session = aiohttp.ClientSession() return self async def __aexit__(self, exc_type, exc_val, exc_tb): if self.session: await self.session.close() async def process_large_dataset(self, features_list: List[List[float]]): \"\"\"Process a large dataset in batches.\"\"\" results = [] for i in range(0, len(features_list), self.batch_size): batch = features_list[i:i + self.batch_size] try: batch_result = await self._process_batch(batch) results.extend(batch_result) print(f\"Processed batch {i//self.batch_size + 1}, \" f\"total processed: {len(results)}\") except Exception as e: print(f\"Batch processing error: {e}\") # Continue with next batch continue return results async def _process_batch(self, batch: List[List[float]]): \"\"\"Process a single batch.\"\"\" async with self.session.post( f\"{self.api_url}/predict/batch\", json={\"features\": batch} ) as response: if response.status == 200: data = await response.json() return data[\"predictions\"] else: raise Exception(f\"API error: {response.status}\") # Usage example async def main(): # Generate sample data large_dataset = [[0.1] * 78 for _ in range(1000)] async with BatchProcessor(\"http://localhost:8000\", batch_size=50) as processor: results = await processor.process_large_dataset(large_dataset) print(f\"Processed {len(results)} predictions\") # Analyze results attack_count = sum(1 for r in results if r[\"class\"] == \"ATTACK\") print(f\"Detected {attack_count} attacks out of {len(results)} samples\") if __name__ == \"__main__\": asyncio.run(main())","title":"1. Batch Processing for High Throughput"},{"location":"api/examples/#2-connection-pooling-and-reuse","text":"import aiohttp import asyncio from typing import Optional class OptimizedClient: def __init__(self, base_url: str): self.base_url = base_url self.session: Optional[aiohttp.ClientSession] = None async def __aenter__(self): # Configure connection pool connector = aiohttp.TCPConnector( limit=100, # Total connection pool size limit_per_host=30, # Per-host connection limit ttl_dns_cache=300, # DNS cache TTL use_dns_cache=True, ) # Configure timeout timeout = aiohttp.ClientTimeout(total=30, connect=5) self.session = aiohttp.ClientSession( connector=connector, timeout=timeout, headers={\"Content-Type\": \"application/json\"} ) return self async def __aexit__(self, exc_type, exc_val, exc_tb): if self.session: await self.session.close() async def predict(self, features: List[float]) -> Dict: \"\"\"Make a single prediction with optimized connection.\"\"\" async with self.session.post( f\"{self.base_url}/predict\", json={\"features\": features} ) as response: return await response.json() async def predict_many(self, features_list: List[List[float]]) -> List[Dict]: \"\"\"Make multiple concurrent predictions.\"\"\" tasks = [] for features in features_list: task = asyncio.create_task(self.predict(features)) tasks.append(task) results = await asyncio.gather(*tasks, return_exceptions=True) # Filter out exceptions valid_results = [r for r in results if not isinstance(r, Exception)] return valid_results # Usage example async def performance_test(): features_list = [[0.1] * 78 for _ in range(100)] async with OptimizedClient(\"http://localhost:8000\") as client: start_time = asyncio.get_event_loop().time() results = await client.predict_many(features_list) end_time = asyncio.get_event_loop().time() print(f\"Processed {len(results)} predictions in {end_time - start_time:.2f} seconds\") print(f\"Average time per prediction: {(end_time - start_time) / len(results):.3f} seconds\") if __name__ == \"__main__\": asyncio.run(performance_test())","title":"2. Connection Pooling and Reuse"},{"location":"api/examples/#integration-with-security-tools","text":"","title":"Integration with Security Tools"},{"location":"api/examples/#1-siem-integration-example","text":"import json import logging from datetime import datetime from api.client import RLIDSClient class SIEMIntegration: def __init__(self, api_url: str, siem_endpoint: str): self.client = RLIDSClient(api_url) self.siem_endpoint = siem_endpoint self.logger = logging.getLogger(__name__) async def process_network_event(self, network_event: Dict): \"\"\"Process network event and send to SIEM if threat detected.\"\"\" try: # Extract features from network event features = self.extract_features(network_event) # Get prediction prediction = await self.client.predict(features) # Create enriched event enriched_event = { 'timestamp': datetime.utcnow().isoformat(), 'source_ip': network_event.get('source_ip'), 'dest_ip': network_event.get('dest_ip'), 'protocol': network_event.get('protocol'), 'original_event': network_event, 'ml_prediction': prediction, 'threat_score': prediction['prediction']['confidence'], 'classification': prediction['prediction']['class'] } # Send to SIEM if attack detected if prediction['prediction']['class'] == 'ATTACK': await self.send_to_siem(enriched_event) return enriched_event except Exception as e: self.logger.error(f\"Failed to process network event: {e}\") return None def extract_features(self, network_event: Dict) -> List[float]: \"\"\"Extract ML features from network event.\"\"\" # This would contain actual feature extraction logic # For demonstration, return mock features return [0.1] * 78 async def send_to_siem(self, event: Dict): \"\"\"Send enriched event to SIEM.\"\"\" try: # Format for SIEM (this example uses JSON) siem_event = { 'timestamp': event['timestamp'], 'event_type': 'ML_THREAT_DETECTION', 'severity': self.calculate_severity(event), 'source': 'RL-IDS', 'details': event } # Send to SIEM endpoint # This would be actual SIEM API call self.logger.info(f\"Sending to SIEM: {json.dumps(siem_event, indent=2)}\") except Exception as e: self.logger.error(f\"Failed to send to SIEM: {e}\") def calculate_severity(self, event: Dict) -> str: \"\"\"Calculate event severity based on confidence.\"\"\" confidence = event['threat_score'] if confidence >= 0.9: return 'HIGH' elif confidence >= 0.7: return 'MEDIUM' else: return 'LOW' # Usage example async def main(): siem = SIEMIntegration( api_url=\"http://localhost:8000\", siem_endpoint=\"https://siem.example.com/api/events\" ) # Sample network event network_event = { 'source_ip': '192.168.1.100', 'dest_ip': '10.0.0.1', 'protocol': 'TCP', 'port': 80, 'packet_count': 1000, 'byte_count': 50000 } result = await siem.process_network_event(network_event) print(f\"Processed event: {result}\") if __name__ == \"__main__\": asyncio.run(main())","title":"1. SIEM Integration Example"},{"location":"api/examples/#monitoring-and-alerting-examples","text":"","title":"Monitoring and Alerting Examples"},{"location":"api/examples/#1-health-monitoring","text":"import asyncio import logging from datetime import datetime, timedelta from api.client import RLIDSClient class HealthMonitor: def __init__(self, api_url: str, check_interval: int = 60): self.client = RLIDSClient(api_url) self.check_interval = check_interval self.logger = logging.getLogger(__name__) self.alert_cooldown = {} async def start_monitoring(self): \"\"\"Start continuous health monitoring.\"\"\" self.logger.info(\"Starting health monitoring...\") while True: try: await self.check_health() await asyncio.sleep(self.check_interval) except KeyboardInterrupt: self.logger.info(\"Health monitoring stopped\") break except Exception as e: self.logger.error(f\"Health monitoring error: {e}\") await asyncio.sleep(self.check_interval) async def check_health(self): \"\"\"Perform health check.\"\"\" try: # Get health status health = await self.client.health_check() if health['status'] != 'healthy': await self.send_alert('UNHEALTHY', health) return # Check model status if not health['details'].get('model_loaded', False): await self.send_alert('MODEL_NOT_LOADED', health) return # Check memory usage memory_usage = health['details'].get('memory_usage', 0) if memory_usage > 80: # 80% threshold await self.send_alert('HIGH_MEMORY', health) # Check response time start_time = asyncio.get_event_loop().time() await self.client.get_model_info() response_time = asyncio.get_event_loop().time() - start_time if response_time > 5.0: # 5 second threshold await self.send_alert('SLOW_RESPONSE', { 'response_time': response_time, 'health': health }) self.logger.info(f\"Health check passed: {health['status']}\") except Exception as e: await self.send_alert('API_ERROR', {'error': str(e)}) async def send_alert(self, alert_type: str, details: Dict): \"\"\"Send alert with cooldown to prevent spam.\"\"\" now = datetime.utcnow() # Check cooldown if alert_type in self.alert_cooldown: last_alert = self.alert_cooldown[alert_type] if now - last_alert < timedelta(minutes=5): return # Skip alert due to cooldown # Send alert alert = { 'timestamp': now.isoformat(), 'type': alert_type, 'details': details, 'severity': self.get_alert_severity(alert_type) } self.logger.error(f\"ALERT: {json.dumps(alert, indent=2)}\") # Update cooldown self.alert_cooldown[alert_type] = now def get_alert_severity(self, alert_type: str) -> str: \"\"\"Get alert severity level.\"\"\" severity_map = { 'UNHEALTHY': 'CRITICAL', 'MODEL_NOT_LOADED': 'CRITICAL', 'API_ERROR': 'HIGH', 'HIGH_MEMORY': 'MEDIUM', 'SLOW_RESPONSE': 'LOW' } return severity_map.get(alert_type, 'MEDIUM') # Usage example if __name__ == \"__main__\": monitor = HealthMonitor(\"http://localhost:8000\", check_interval=30) asyncio.run(monitor.start_monitoring()) These examples demonstrate various ways to integrate and use the RL-IDS API effectively in different scenarios, from simple predictions to complex production integrations with security tools and monitoring systems.","title":"1. Health Monitoring"},{"location":"api/python-client/","text":"Python Client The RL-IDS Python client ( api/client.py ) provides a convenient interface for interacting with the RL-IDS API from Python applications. Overview The IDSAPIClient class offers async methods for all API endpoints, connection management, and error handling. It's designed for high-performance applications that need to integrate threat detection capabilities. Installation The client is included with the RL-IDS package. No additional installation is required. Basic Usage Initialize Client from api.client import IDSAPIClient # Initialize with default settings client = IDSAPIClient() # Initialize with custom endpoint client = IDSAPIClient(\"http://your-api-server:8000\") Health Check import asyncio from api.client import IDSAPIClient async def check_health(): client = IDSAPIClient() try: health = await client.health_check() print(f\"API Status: {health['status']}\") print(f\"Model Loaded: {health['details']['model_loaded']}\") print(f\"Uptime: {health['details']['uptime_seconds']} seconds\") except Exception as e: print(f\"Health check failed: {e}\") finally: await client.close() # Run the async function asyncio.run(check_health()) Single Prediction import asyncio from api.client import IDSAPIClient async def make_prediction(): client = IDSAPIClient() try: # Example with 78 CICIDS2017 features features = [0.1, 0.2, 0.3] + [0.0] * 75 # 78 features total prediction = await client.predict(features) if prediction['is_attack']: print(f\"\ud83d\udea8 Attack detected: {prediction['predicted_class']}\") print(f\"Confidence: {prediction['confidence']:.1%}\") else: print(\"\u2705 Normal traffic detected\") except Exception as e: print(f\"Prediction failed: {e}\") finally: await client.close() asyncio.run(make_prediction()) Batch Predictions import asyncio from api.client import IDSAPIClient async def batch_predictions(): client = IDSAPIClient() try: # Create batch of feature sets batch_data = [] for i in range(5): features = [0.1 * (i + 1), 0.2 * (i + 1), 0.3 * (i + 1)] + [0.0] * 75 batch_data.append({\"features\": features}) results = await client.predict_batch(batch_data) for i, result in enumerate(results): status = \"Attack\" if result['is_attack'] else \"Normal\" print(f\"Sample {i+1}: {status} ({result['confidence']:.1%})\") except Exception as e: print(f\"Batch prediction failed: {e}\") finally: await client.close() asyncio.run(batch_predictions()) Client Class Reference IDSAPIClient class IDSAPIClient: \"\"\"Client for interacting with the RL-IDS API.\"\"\" def __init__(self, base_url: str = \"http://localhost:8000\"): \"\"\"Initialize the API client.\"\"\" self.base_url = base_url self.session = httpx.AsyncClient(timeout=30.0) Methods health_check() Check API health status. async def health_check(self) -> Dict[str, Any]: \"\"\"Check API health status.\"\"\" Returns: { \"status\": \"healthy\", \"timestamp\": \"2025-06-27T10:30:00.123456\", \"details\": { \"model_loaded\": True, \"predictions_served\": 1234, \"uptime_seconds\": 3600.5, \"memory_usage_mb\": 256.7 } } Raises: - httpx.HTTPError - If API is unreachable - httpx.HTTPStatusError - If API returns error status get_model_info() Get detailed model information. async def get_model_info(self) -> Dict[str, Any]: \"\"\"Get model information.\"\"\" Returns: { \"model_name\": \"DQN_IDS_Model\", \"model_version\": \"1.2.0\", \"model_type\": \"Deep Q-Network\", \"input_features\": 78, \"output_classes\": 15, \"class_names\": [\"BENIGN\", \"Web Attack \u2013 Brute Force\", ...], \"model_size_mb\": 2.5 } predict() Make single prediction. async def predict(self, features: List[float]) -> Dict[str, Any]: \"\"\"Make single prediction.\"\"\" Parameters: - features : List of 78 CICIDS2017 features Returns: { \"prediction\": 9, \"confidence\": 0.87, \"predicted_class\": \"DoS Hulk\", \"is_attack\": True, \"class_probabilities\": [0.01, 0.02, ...], \"processing_time_ms\": 12.5, \"timestamp\": \"2025-06-27T10:30:45.123456\" } Raises: - ValueError - If features list is invalid - httpx.HTTPStatusError - If API returns error predict_batch() Make batch predictions. async def predict_batch(self, batch_data: List[Dict[str, List[float]]]) -> List[Dict[str, Any]]: \"\"\"Make batch predictions.\"\"\" Parameters: - batch_data : List of dictionaries with \"features\" key Returns: List of prediction results (same format as single prediction) Raises: - ValueError - If batch data is invalid - httpx.HTTPStatusError - If API returns error close() Close the HTTP session. async def close(self): \"\"\"Close the HTTP session.\"\"\" await self.session.aclose() Advanced Usage Context Manager Use the client as an async context manager for automatic cleanup: import asyncio from api.client import IDSAPIClient async def main(): async with IDSAPIClient() as client: health = await client.health_check() print(f\"API Status: {health['status']}\") # Client will be automatically closed when exiting context asyncio.run(main()) Custom Timeout Configure request timeouts: import httpx from api.client import IDSAPIClient # Custom timeout settings client = IDSAPIClient() client.session = httpx.AsyncClient(timeout=httpx.Timeout(60.0)) # 60 second timeout Error Handling Comprehensive error handling: import httpx from api.client import IDSAPIClient async def robust_prediction(features): client = IDSAPIClient() try: prediction = await client.predict(features) return prediction except httpx.TimeoutException: print(\"Request timed out - API may be overloaded\") return None except httpx.HTTPStatusError as e: if e.response.status_code == 422: print(\"Invalid input data\") elif e.response.status_code == 503: print(\"API service unavailable\") else: print(f\"HTTP error: {e.response.status_code}\") return None except httpx.ConnectError: print(\"Cannot connect to API server\") return None except Exception as e: print(f\"Unexpected error: {e}\") return None finally: await client.close() Performance Testing Benchmark API performance: import asyncio import time from api.client import IDSAPIClient async def benchmark_api_performance(num_requests=100): \"\"\"Benchmark API prediction performance.\"\"\" client = IDSAPIClient() # Generate test features test_features = [0.1, 0.2, 0.3] + [0.0] * 75 try: start_time = time.time() # Make concurrent requests tasks = [client.predict(test_features) for _ in range(num_requests)] results = await asyncio.gather(*tasks, return_exceptions=True) end_time = time.time() # Calculate metrics successful_requests = sum(1 for r in results if not isinstance(r, Exception)) total_time = end_time - start_time avg_time = total_time / successful_requests if successful_requests > 0 else 0 rps = successful_requests / total_time if total_time > 0 else 0 print(f\"Performance Test Results:\") print(f\" Total Requests: {num_requests}\") print(f\" Successful: {successful_requests}\") print(f\" Total Time: {total_time:.2f}s\") print(f\" Average Time: {avg_time:.3f}s\") print(f\" Requests/Second: {rps:.1f}\") return { \"total_requests\": num_requests, \"successful_requests\": successful_requests, \"total_time\": total_time, \"avg_time\": avg_time, \"rps\": rps } finally: await client.close() # Run benchmark asyncio.run(benchmark_api_performance(100))","title":"Python Client"},{"location":"api/python-client/#python-client","text":"The RL-IDS Python client ( api/client.py ) provides a convenient interface for interacting with the RL-IDS API from Python applications.","title":"Python Client"},{"location":"api/python-client/#overview","text":"The IDSAPIClient class offers async methods for all API endpoints, connection management, and error handling. It's designed for high-performance applications that need to integrate threat detection capabilities.","title":"Overview"},{"location":"api/python-client/#installation","text":"The client is included with the RL-IDS package. No additional installation is required.","title":"Installation"},{"location":"api/python-client/#basic-usage","text":"","title":"Basic Usage"},{"location":"api/python-client/#initialize-client","text":"from api.client import IDSAPIClient # Initialize with default settings client = IDSAPIClient() # Initialize with custom endpoint client = IDSAPIClient(\"http://your-api-server:8000\")","title":"Initialize Client"},{"location":"api/python-client/#health-check","text":"import asyncio from api.client import IDSAPIClient async def check_health(): client = IDSAPIClient() try: health = await client.health_check() print(f\"API Status: {health['status']}\") print(f\"Model Loaded: {health['details']['model_loaded']}\") print(f\"Uptime: {health['details']['uptime_seconds']} seconds\") except Exception as e: print(f\"Health check failed: {e}\") finally: await client.close() # Run the async function asyncio.run(check_health())","title":"Health Check"},{"location":"api/python-client/#single-prediction","text":"import asyncio from api.client import IDSAPIClient async def make_prediction(): client = IDSAPIClient() try: # Example with 78 CICIDS2017 features features = [0.1, 0.2, 0.3] + [0.0] * 75 # 78 features total prediction = await client.predict(features) if prediction['is_attack']: print(f\"\ud83d\udea8 Attack detected: {prediction['predicted_class']}\") print(f\"Confidence: {prediction['confidence']:.1%}\") else: print(\"\u2705 Normal traffic detected\") except Exception as e: print(f\"Prediction failed: {e}\") finally: await client.close() asyncio.run(make_prediction())","title":"Single Prediction"},{"location":"api/python-client/#batch-predictions","text":"import asyncio from api.client import IDSAPIClient async def batch_predictions(): client = IDSAPIClient() try: # Create batch of feature sets batch_data = [] for i in range(5): features = [0.1 * (i + 1), 0.2 * (i + 1), 0.3 * (i + 1)] + [0.0] * 75 batch_data.append({\"features\": features}) results = await client.predict_batch(batch_data) for i, result in enumerate(results): status = \"Attack\" if result['is_attack'] else \"Normal\" print(f\"Sample {i+1}: {status} ({result['confidence']:.1%})\") except Exception as e: print(f\"Batch prediction failed: {e}\") finally: await client.close() asyncio.run(batch_predictions())","title":"Batch Predictions"},{"location":"api/python-client/#client-class-reference","text":"","title":"Client Class Reference"},{"location":"api/python-client/#idsapiclient","text":"class IDSAPIClient: \"\"\"Client for interacting with the RL-IDS API.\"\"\" def __init__(self, base_url: str = \"http://localhost:8000\"): \"\"\"Initialize the API client.\"\"\" self.base_url = base_url self.session = httpx.AsyncClient(timeout=30.0)","title":"IDSAPIClient"},{"location":"api/python-client/#methods","text":"","title":"Methods"},{"location":"api/python-client/#health_check","text":"Check API health status. async def health_check(self) -> Dict[str, Any]: \"\"\"Check API health status.\"\"\" Returns: { \"status\": \"healthy\", \"timestamp\": \"2025-06-27T10:30:00.123456\", \"details\": { \"model_loaded\": True, \"predictions_served\": 1234, \"uptime_seconds\": 3600.5, \"memory_usage_mb\": 256.7 } } Raises: - httpx.HTTPError - If API is unreachable - httpx.HTTPStatusError - If API returns error status","title":"health_check()"},{"location":"api/python-client/#get_model_info","text":"Get detailed model information. async def get_model_info(self) -> Dict[str, Any]: \"\"\"Get model information.\"\"\" Returns: { \"model_name\": \"DQN_IDS_Model\", \"model_version\": \"1.2.0\", \"model_type\": \"Deep Q-Network\", \"input_features\": 78, \"output_classes\": 15, \"class_names\": [\"BENIGN\", \"Web Attack \u2013 Brute Force\", ...], \"model_size_mb\": 2.5 }","title":"get_model_info()"},{"location":"api/python-client/#predict","text":"Make single prediction. async def predict(self, features: List[float]) -> Dict[str, Any]: \"\"\"Make single prediction.\"\"\" Parameters: - features : List of 78 CICIDS2017 features Returns: { \"prediction\": 9, \"confidence\": 0.87, \"predicted_class\": \"DoS Hulk\", \"is_attack\": True, \"class_probabilities\": [0.01, 0.02, ...], \"processing_time_ms\": 12.5, \"timestamp\": \"2025-06-27T10:30:45.123456\" } Raises: - ValueError - If features list is invalid - httpx.HTTPStatusError - If API returns error","title":"predict()"},{"location":"api/python-client/#predict_batch","text":"Make batch predictions. async def predict_batch(self, batch_data: List[Dict[str, List[float]]]) -> List[Dict[str, Any]]: \"\"\"Make batch predictions.\"\"\" Parameters: - batch_data : List of dictionaries with \"features\" key Returns: List of prediction results (same format as single prediction) Raises: - ValueError - If batch data is invalid - httpx.HTTPStatusError - If API returns error","title":"predict_batch()"},{"location":"api/python-client/#close","text":"Close the HTTP session. async def close(self): \"\"\"Close the HTTP session.\"\"\" await self.session.aclose()","title":"close()"},{"location":"api/python-client/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"api/python-client/#context-manager","text":"Use the client as an async context manager for automatic cleanup: import asyncio from api.client import IDSAPIClient async def main(): async with IDSAPIClient() as client: health = await client.health_check() print(f\"API Status: {health['status']}\") # Client will be automatically closed when exiting context asyncio.run(main())","title":"Context Manager"},{"location":"api/python-client/#custom-timeout","text":"Configure request timeouts: import httpx from api.client import IDSAPIClient # Custom timeout settings client = IDSAPIClient() client.session = httpx.AsyncClient(timeout=httpx.Timeout(60.0)) # 60 second timeout","title":"Custom Timeout"},{"location":"api/python-client/#error-handling","text":"Comprehensive error handling: import httpx from api.client import IDSAPIClient async def robust_prediction(features): client = IDSAPIClient() try: prediction = await client.predict(features) return prediction except httpx.TimeoutException: print(\"Request timed out - API may be overloaded\") return None except httpx.HTTPStatusError as e: if e.response.status_code == 422: print(\"Invalid input data\") elif e.response.status_code == 503: print(\"API service unavailable\") else: print(f\"HTTP error: {e.response.status_code}\") return None except httpx.ConnectError: print(\"Cannot connect to API server\") return None except Exception as e: print(f\"Unexpected error: {e}\") return None finally: await client.close()","title":"Error Handling"},{"location":"api/python-client/#performance-testing","text":"Benchmark API performance: import asyncio import time from api.client import IDSAPIClient async def benchmark_api_performance(num_requests=100): \"\"\"Benchmark API prediction performance.\"\"\" client = IDSAPIClient() # Generate test features test_features = [0.1, 0.2, 0.3] + [0.0] * 75 try: start_time = time.time() # Make concurrent requests tasks = [client.predict(test_features) for _ in range(num_requests)] results = await asyncio.gather(*tasks, return_exceptions=True) end_time = time.time() # Calculate metrics successful_requests = sum(1 for r in results if not isinstance(r, Exception)) total_time = end_time - start_time avg_time = total_time / successful_requests if successful_requests > 0 else 0 rps = successful_requests / total_time if total_time > 0 else 0 print(f\"Performance Test Results:\") print(f\" Total Requests: {num_requests}\") print(f\" Successful: {successful_requests}\") print(f\" Total Time: {total_time:.2f}s\") print(f\" Average Time: {avg_time:.3f}s\") print(f\" Requests/Second: {rps:.1f}\") return { \"total_requests\": num_requests, \"successful_requests\": successful_requests, \"total_time\": total_time, \"avg_time\": avg_time, \"rps\": rps } finally: await client.close() # Run benchmark asyncio.run(benchmark_api_performance(100))","title":"Performance Testing"},{"location":"development/architecture/","text":"System Architecture This document provides a comprehensive overview of the RL-IDS system architecture, including its components, data flow, and design principles. Overview RL-IDS is a reinforcement learning-driven adaptive intrusion detection system that combines real-time network monitoring with intelligent threat detection. The system is designed with modularity, scalability, and extensibility in mind. High-Level Architecture \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 RL-IDS System \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Network Data \u2502 \u2502 Web Traffic \u2502 \u2502 \u2502 \u2502 Collection \u2502 \u2502 Generation \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502 \u2502 Feature Extraction Layer \u2502\u2502 \u2502 \u2502 - CICIDS2017 Feature Engineering \u2502\u2502 \u2502 \u2502 - Flow-based Analysis \u2502\u2502 \u2502 \u2502 - Real-time Processing \u2502\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502 \u2502 Reinforcement Learning Core \u2502\u2502 \u2502 \u2502 - DQN Agent (Deep Q-Network) \u2502\u2502 \u2502 \u2502 - Adaptive Decision Making \u2502\u2502 \u2502 \u2502 - Continuous Learning \u2502\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502 \u2502 Detection & Response \u2502\u2502 \u2502 \u2502 - Real-time Threat Classification \u2502\u2502 \u2502 \u2502 - Alert Generation \u2502\u2502 \u2502 \u2502 - API Interface \u2502\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Core Components 1. Data Collection Layer Network Monitor ( network_monitor.py ) Purpose : Real-time network packet capture and analysis Key Features : Live packet capture using raw sockets Protocol analysis (TCP, UDP, HTTP, HTTPS) Flow-based traffic aggregation Statistical feature computation Website Monitor ( website_monitor.py ) Purpose : Website-specific traffic generation and monitoring Key Features : Automated web requests Traffic pattern simulation Packet capture for generated traffic Integration with network monitor 2. Feature Engineering Layer ( rl_ids/make_dataset.py ) CICIDS2017 Feature Extraction 78 Network Flow Features : Flow duration and packet timing Packet size statistics (min, max, mean, std) Flow flags and protocol information Inter-arrival time analysis Forward/backward flow characteristics Data Processing Pipeline Raw Packets \u2192 Flow Aggregation \u2192 Feature Extraction \u2192 Normalization \u2192 Model Input 3. Reinforcement Learning Core DQN Agent ( rl_ids/agents/dqn_agent.py ) Architecture : Deep Q-Network with experience replay Components : Q-Network: Neural network for action-value estimation Target Network: Stable target for Q-learning updates Experience Replay Buffer: Storage for training experiences Exploration Strategy: Epsilon-greedy with decay Network Architecture Input Layer (78 features) \u2192 Hidden Layers (256, 128, 64) \u2192 Output Layer (Action Space) Training Process ( rl_ids/modeling/train.py ) Environment Interaction : Agent observes network states Action Selection : Choose detection strategy based on Q-values Experience Collection : Store (state, action, reward, next_state) tuples Batch Learning : Update Q-network using sampled experiences Target Network Update : Periodic synchronization for stability 4. Environment Layer ( rl_ids/environments/ids_env.py ) IDS Gym Environment State Space : 78-dimensional feature vectors Action Space : Detection decisions (normal/attack classification) Reward Function : Based on detection accuracy and false positive rates Episode Structure : Configurable episode length for training 5. API Layer ( api/ ) FastAPI Service ( api/main.py ) Endpoints : / : Service information /health : Health check /model/info : Model metadata /predict : Single prediction /predict/batch : Batch predictions Models and Validation ( api/models.py ) Pydantic models for request/response validation Type safety and automatic documentation Error handling and status codes Client Library ( api/client.py ) Python client for API interaction Asynchronous support Built-in error handling and retries Data Flow Architecture Training Phase Historical Data (CICIDS2017) \u2192 Feature Extraction \u2192 Environment \u2192 DQN Agent \u2192 Model Training \u2192 Saved Model Inference Phase Live Network Traffic \u2192 Feature Extraction \u2192 Trained Model \u2192 Prediction \u2192 Alert/Response API Integration External Request \u2192 API Validation \u2192 Model Inference \u2192 Response \u2192 Client Application Design Principles 1. Modularity Separation of Concerns : Each component has a single responsibility Loose Coupling : Components interact through well-defined interfaces Plugin Architecture : Easy to extend with new features 2. Scalability Horizontal Scaling : API can be deployed across multiple instances Asynchronous Processing : Non-blocking operations for better throughput Batch Processing : Efficient handling of multiple requests 3. Adaptability Continuous Learning : Model can adapt to new threat patterns Configuration-Driven : Behavior controlled through configuration files Environment Flexibility : Works across different network environments 4. Reliability Error Handling : Comprehensive error handling and logging Graceful Degradation : System continues operating during partial failures Health Monitoring : Built-in health checks and status reporting Configuration Management Configuration Files rl_ids/config.py : Core system configuration api/config.py : API-specific settings .env : Environment variables for deployment Key Configuration Areas Model Parameters : Network architecture, training hyperparameters Environment Settings : Reward functions, episode configuration API Configuration : Server settings, security parameters Monitoring Settings : Logging levels, capture interfaces Security Considerations Network Access Privilege Management : Minimal required permissions Interface Isolation : Secure packet capture Data Privacy : No sensitive data logging API Security Input Validation : Strict type checking and sanitization Rate Limiting : Protection against abuse Error Handling : No sensitive information leakage Performance Characteristics Training Performance GPU Acceleration : CUDA support for faster training Memory Efficiency : Optimized data structures and batch processing Convergence Speed : Typically 200-500 episodes for convergence Inference Performance Real-time Processing : Sub-second response times Throughput : Handles thousands of predictions per second Resource Usage : Optimized for production deployment Deployment Architecture Development Deployment Local Machine \u2192 Python Virtual Environment \u2192 Direct Execution Production Deployment Load Balancer \u2192 API Instances \u2192 Model Inference \u2192 Database/Logging Monitoring Deployment Network Interface \u2192 Packet Capture \u2192 Feature Extraction \u2192 Real-time Detection Extension Points Adding New Features Extend feature extraction in make_dataset.py Update model input dimensions Retrain with enhanced feature set New Detection Algorithms Implement new agent in rl_ids/agents/ Create corresponding environment Update training pipeline API Extensions Add new endpoints in api/main.py Define request/response models Update client library Dependencies and Libraries Core Dependencies PyTorch : Deep learning framework Gymnasium : RL environment interface Pandas : Data manipulation Scikit-learn : Machine learning utilities FastAPI : Web API framework Monitoring Dependencies Scapy : Packet capture and analysis Psutil : System and network utilities Loguru : Advanced logging Development Dependencies Pytest : Testing framework Ruff : Code formatting and linting MkDocs : Documentation generation Future Architecture Considerations Planned Enhancements Distributed Training : Multi-node training support Stream Processing : Kafka/Redis integration Model Versioning : MLflow integration Container Orchestration : Kubernetes deployment Scalability Improvements Microservices : Service decomposition Event-Driven Architecture : Asynchronous event processing Caching Layer : Redis for improved performance Database Integration : Persistent storage for alerts and metrics","title":"Architecture"},{"location":"development/architecture/#system-architecture","text":"This document provides a comprehensive overview of the RL-IDS system architecture, including its components, data flow, and design principles.","title":"System Architecture"},{"location":"development/architecture/#overview","text":"RL-IDS is a reinforcement learning-driven adaptive intrusion detection system that combines real-time network monitoring with intelligent threat detection. The system is designed with modularity, scalability, and extensibility in mind.","title":"Overview"},{"location":"development/architecture/#high-level-architecture","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 RL-IDS System \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Network Data \u2502 \u2502 Web Traffic \u2502 \u2502 \u2502 \u2502 Collection \u2502 \u2502 Generation \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502 \u2502 Feature Extraction Layer \u2502\u2502 \u2502 \u2502 - CICIDS2017 Feature Engineering \u2502\u2502 \u2502 \u2502 - Flow-based Analysis \u2502\u2502 \u2502 \u2502 - Real-time Processing \u2502\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502 \u2502 Reinforcement Learning Core \u2502\u2502 \u2502 \u2502 - DQN Agent (Deep Q-Network) \u2502\u2502 \u2502 \u2502 - Adaptive Decision Making \u2502\u2502 \u2502 \u2502 - Continuous Learning \u2502\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502 \u2502 \u2502 Detection & Response \u2502\u2502 \u2502 \u2502 - Real-time Threat Classification \u2502\u2502 \u2502 \u2502 - Alert Generation \u2502\u2502 \u2502 \u2502 - API Interface \u2502\u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"High-Level Architecture"},{"location":"development/architecture/#core-components","text":"","title":"Core Components"},{"location":"development/architecture/#1-data-collection-layer","text":"","title":"1. Data Collection Layer"},{"location":"development/architecture/#network-monitor-network_monitorpy","text":"Purpose : Real-time network packet capture and analysis Key Features : Live packet capture using raw sockets Protocol analysis (TCP, UDP, HTTP, HTTPS) Flow-based traffic aggregation Statistical feature computation","title":"Network Monitor (network_monitor.py)"},{"location":"development/architecture/#website-monitor-website_monitorpy","text":"Purpose : Website-specific traffic generation and monitoring Key Features : Automated web requests Traffic pattern simulation Packet capture for generated traffic Integration with network monitor","title":"Website Monitor (website_monitor.py)"},{"location":"development/architecture/#2-feature-engineering-layer-rl_idsmake_datasetpy","text":"","title":"2. Feature Engineering Layer (rl_ids/make_dataset.py)"},{"location":"development/architecture/#cicids2017-feature-extraction","text":"78 Network Flow Features : Flow duration and packet timing Packet size statistics (min, max, mean, std) Flow flags and protocol information Inter-arrival time analysis Forward/backward flow characteristics","title":"CICIDS2017 Feature Extraction"},{"location":"development/architecture/#data-processing-pipeline","text":"Raw Packets \u2192 Flow Aggregation \u2192 Feature Extraction \u2192 Normalization \u2192 Model Input","title":"Data Processing Pipeline"},{"location":"development/architecture/#3-reinforcement-learning-core","text":"","title":"3. Reinforcement Learning Core"},{"location":"development/architecture/#dqn-agent-rl_idsagentsdqn_agentpy","text":"Architecture : Deep Q-Network with experience replay Components : Q-Network: Neural network for action-value estimation Target Network: Stable target for Q-learning updates Experience Replay Buffer: Storage for training experiences Exploration Strategy: Epsilon-greedy with decay","title":"DQN Agent (rl_ids/agents/dqn_agent.py)"},{"location":"development/architecture/#network-architecture","text":"Input Layer (78 features) \u2192 Hidden Layers (256, 128, 64) \u2192 Output Layer (Action Space)","title":"Network Architecture"},{"location":"development/architecture/#training-process-rl_idsmodelingtrainpy","text":"Environment Interaction : Agent observes network states Action Selection : Choose detection strategy based on Q-values Experience Collection : Store (state, action, reward, next_state) tuples Batch Learning : Update Q-network using sampled experiences Target Network Update : Periodic synchronization for stability","title":"Training Process (rl_ids/modeling/train.py)"},{"location":"development/architecture/#4-environment-layer-rl_idsenvironmentsids_envpy","text":"","title":"4. Environment Layer (rl_ids/environments/ids_env.py)"},{"location":"development/architecture/#ids-gym-environment","text":"State Space : 78-dimensional feature vectors Action Space : Detection decisions (normal/attack classification) Reward Function : Based on detection accuracy and false positive rates Episode Structure : Configurable episode length for training","title":"IDS Gym Environment"},{"location":"development/architecture/#5-api-layer-api","text":"","title":"5. API Layer (api/)"},{"location":"development/architecture/#fastapi-service-apimainpy","text":"Endpoints : / : Service information /health : Health check /model/info : Model metadata /predict : Single prediction /predict/batch : Batch predictions","title":"FastAPI Service (api/main.py)"},{"location":"development/architecture/#models-and-validation-apimodelspy","text":"Pydantic models for request/response validation Type safety and automatic documentation Error handling and status codes","title":"Models and Validation (api/models.py)"},{"location":"development/architecture/#client-library-apiclientpy","text":"Python client for API interaction Asynchronous support Built-in error handling and retries","title":"Client Library (api/client.py)"},{"location":"development/architecture/#data-flow-architecture","text":"","title":"Data Flow Architecture"},{"location":"development/architecture/#training-phase","text":"Historical Data (CICIDS2017) \u2192 Feature Extraction \u2192 Environment \u2192 DQN Agent \u2192 Model Training \u2192 Saved Model","title":"Training Phase"},{"location":"development/architecture/#inference-phase","text":"Live Network Traffic \u2192 Feature Extraction \u2192 Trained Model \u2192 Prediction \u2192 Alert/Response","title":"Inference Phase"},{"location":"development/architecture/#api-integration","text":"External Request \u2192 API Validation \u2192 Model Inference \u2192 Response \u2192 Client Application","title":"API Integration"},{"location":"development/architecture/#design-principles","text":"","title":"Design Principles"},{"location":"development/architecture/#1-modularity","text":"Separation of Concerns : Each component has a single responsibility Loose Coupling : Components interact through well-defined interfaces Plugin Architecture : Easy to extend with new features","title":"1. Modularity"},{"location":"development/architecture/#2-scalability","text":"Horizontal Scaling : API can be deployed across multiple instances Asynchronous Processing : Non-blocking operations for better throughput Batch Processing : Efficient handling of multiple requests","title":"2. Scalability"},{"location":"development/architecture/#3-adaptability","text":"Continuous Learning : Model can adapt to new threat patterns Configuration-Driven : Behavior controlled through configuration files Environment Flexibility : Works across different network environments","title":"3. Adaptability"},{"location":"development/architecture/#4-reliability","text":"Error Handling : Comprehensive error handling and logging Graceful Degradation : System continues operating during partial failures Health Monitoring : Built-in health checks and status reporting","title":"4. Reliability"},{"location":"development/architecture/#configuration-management","text":"","title":"Configuration Management"},{"location":"development/architecture/#configuration-files","text":"rl_ids/config.py : Core system configuration api/config.py : API-specific settings .env : Environment variables for deployment","title":"Configuration Files"},{"location":"development/architecture/#key-configuration-areas","text":"Model Parameters : Network architecture, training hyperparameters Environment Settings : Reward functions, episode configuration API Configuration : Server settings, security parameters Monitoring Settings : Logging levels, capture interfaces","title":"Key Configuration Areas"},{"location":"development/architecture/#security-considerations","text":"","title":"Security Considerations"},{"location":"development/architecture/#network-access","text":"Privilege Management : Minimal required permissions Interface Isolation : Secure packet capture Data Privacy : No sensitive data logging","title":"Network Access"},{"location":"development/architecture/#api-security","text":"Input Validation : Strict type checking and sanitization Rate Limiting : Protection against abuse Error Handling : No sensitive information leakage","title":"API Security"},{"location":"development/architecture/#performance-characteristics","text":"","title":"Performance Characteristics"},{"location":"development/architecture/#training-performance","text":"GPU Acceleration : CUDA support for faster training Memory Efficiency : Optimized data structures and batch processing Convergence Speed : Typically 200-500 episodes for convergence","title":"Training Performance"},{"location":"development/architecture/#inference-performance","text":"Real-time Processing : Sub-second response times Throughput : Handles thousands of predictions per second Resource Usage : Optimized for production deployment","title":"Inference Performance"},{"location":"development/architecture/#deployment-architecture","text":"","title":"Deployment Architecture"},{"location":"development/architecture/#development-deployment","text":"Local Machine \u2192 Python Virtual Environment \u2192 Direct Execution","title":"Development Deployment"},{"location":"development/architecture/#production-deployment","text":"Load Balancer \u2192 API Instances \u2192 Model Inference \u2192 Database/Logging","title":"Production Deployment"},{"location":"development/architecture/#monitoring-deployment","text":"Network Interface \u2192 Packet Capture \u2192 Feature Extraction \u2192 Real-time Detection","title":"Monitoring Deployment"},{"location":"development/architecture/#extension-points","text":"","title":"Extension Points"},{"location":"development/architecture/#adding-new-features","text":"Extend feature extraction in make_dataset.py Update model input dimensions Retrain with enhanced feature set","title":"Adding New Features"},{"location":"development/architecture/#new-detection-algorithms","text":"Implement new agent in rl_ids/agents/ Create corresponding environment Update training pipeline","title":"New Detection Algorithms"},{"location":"development/architecture/#api-extensions","text":"Add new endpoints in api/main.py Define request/response models Update client library","title":"API Extensions"},{"location":"development/architecture/#dependencies-and-libraries","text":"","title":"Dependencies and Libraries"},{"location":"development/architecture/#core-dependencies","text":"PyTorch : Deep learning framework Gymnasium : RL environment interface Pandas : Data manipulation Scikit-learn : Machine learning utilities FastAPI : Web API framework","title":"Core Dependencies"},{"location":"development/architecture/#monitoring-dependencies","text":"Scapy : Packet capture and analysis Psutil : System and network utilities Loguru : Advanced logging","title":"Monitoring Dependencies"},{"location":"development/architecture/#development-dependencies","text":"Pytest : Testing framework Ruff : Code formatting and linting MkDocs : Documentation generation","title":"Development Dependencies"},{"location":"development/architecture/#future-architecture-considerations","text":"","title":"Future Architecture Considerations"},{"location":"development/architecture/#planned-enhancements","text":"Distributed Training : Multi-node training support Stream Processing : Kafka/Redis integration Model Versioning : MLflow integration Container Orchestration : Kubernetes deployment","title":"Planned Enhancements"},{"location":"development/architecture/#scalability-improvements","text":"Microservices : Service decomposition Event-Driven Architecture : Asynchronous event processing Caching Layer : Redis for improved performance Database Integration : Persistent storage for alerts and metrics","title":"Scalability Improvements"},{"location":"development/contributing/","text":"Contributing to RL-IDS We welcome contributions from the community! This document provides guidelines for contributing to the RL-IDS Adaptive System. Quick Start Fork the repository Create a feature branch: git checkout -b feature/amazing-feature Make your changes and add tests Run the test suite: pytest tests/ -v --cov=rl_ids Update documentation if needed Submit a pull request Development Setup Prerequisites Python 3.8+ Git Docker (optional) Local Development # Clone your fork git clone https://github.com/yashpotdar-py/rl-ids.git cd rl-ids # Create virtual environment python -m venv venv source venv/bin/activate # Linux/Mac # or venv\\Scripts\\activate # Windows # Install development dependencies pip install -r requirements-dev.txt # Install pre-commit hooks pre-commit install Testing Running Tests # Run all tests pytest tests/ -v # Run with coverage pytest tests/ -v --cov=rl_ids --cov-report=html # Run specific test categories pytest tests/unit/ -v pytest tests/integration/ -v Writing Tests Place unit tests in tests/unit/ Place integration tests in tests/integration/ Follow the naming convention: test_*.py Use descriptive test names: test_dqn_agent_training_convergence Code Style We use several tools to maintain code quality: # Format code black rl_ids/ tests/ isort rl_ids/ tests/ # Type checking mypy rl_ids/ # Linting flake8 rl_ids/ tests/ Style Guidelines Follow PEP 8 for Python code style Use type hints for all public functions Write docstrings for modules, classes, and functions Keep line length under 88 characters (Black default) Documentation Building Documentation # Install documentation dependencies pip install mkdocs-material # Serve locally with hot reload cd docs/ mkdocs serve # Build static documentation mkdocs build Documentation Guidelines Update relevant documentation for any API changes Add new tutorials for significant features Keep examples up-to-date and tested Use clear, concise language Bug Reports When reporting bugs, please include: Environment Information Python version Operating system Package versions ( pip freeze ) Reproduction Steps Minimal code example Expected vs. actual behavior Error messages and stack traces Additional Context Configuration files Log outputs Screenshots (if applicable) Feature Requests For new features: Check existing issues to avoid duplicates Describe the problem the feature would solve Propose a solution or implementation approach Consider backwards compatibility Pull Request Process Before Submitting [ ] Tests pass: pytest tests/ -v [ ] Code is formatted: black rl_ids/ tests/ [ ] Imports are sorted: isort rl_ids/ tests/ [ ] Type checks pass: mypy rl_ids/ [ ] Documentation is updated [ ] CHANGELOG.md is updated Pull Request Template ## Description Brief description of changes ## Type of Change - [ ] Bug fix - [ ] New feature - [ ] Breaking change - [ ] Documentation update ## Testing - [ ] New tests added - [ ] All tests pass - [ ] Manual testing completed ## Checklist - [ ] Code follows style guidelines - [ ] Self-review completed - [ ] Documentation updated - [ ] CHANGELOG.md updated Architecture Guidelines Code Organization rl_ids/ \u251c\u2500\u2500 agents/ # RL agents (DQN, etc.) \u251c\u2500\u2500 environments/ # Gymnasium environments \u251c\u2500\u2500 modeling/ # Training and evaluation \u251c\u2500\u2500 make_dataset.py # Data processing \u251c\u2500\u2500 plots.py # Visualization \u2514\u2500\u2500 config.py # Configuration api/ \u251c\u2500\u2500 main.py # FastAPI application \u251c\u2500\u2500 models/ # Pydantic models \u251c\u2500\u2500 routes/ # API endpoints \u2514\u2500\u2500 services/ # Business logic Design Principles Modularity : Keep components loosely coupled Testability : Design for easy testing Configuration : Make components configurable Documentation : Document public APIs Contribution Areas High Priority Performance optimizations Additional RL algorithms Enhanced monitoring and alerting Security improvements Medium Priority Additional dataset support Visualization enhancements API feature additions Documentation improvements Welcome Contributions Bug fixes and improvements Test coverage expansion Example scripts and tutorials Typo fixes and documentation clarity Getting Help Documentation : Check docs/ for comprehensive guides Issues : Search existing issues or create a new one Discussions : Use GitHub Discussions for questions Email : Contact maintainers for security issues Recognition Contributors will be recognized in: - README.md contributors section - Release notes - Documentation credits Thank you for contributing to RL-IDS!","title":"Contributing"},{"location":"development/contributing/#contributing-to-rl-ids","text":"We welcome contributions from the community! This document provides guidelines for contributing to the RL-IDS Adaptive System.","title":"Contributing to RL-IDS"},{"location":"development/contributing/#quick-start","text":"Fork the repository Create a feature branch: git checkout -b feature/amazing-feature Make your changes and add tests Run the test suite: pytest tests/ -v --cov=rl_ids Update documentation if needed Submit a pull request","title":"Quick Start"},{"location":"development/contributing/#development-setup","text":"","title":"Development Setup"},{"location":"development/contributing/#prerequisites","text":"Python 3.8+ Git Docker (optional)","title":"Prerequisites"},{"location":"development/contributing/#local-development","text":"# Clone your fork git clone https://github.com/yashpotdar-py/rl-ids.git cd rl-ids # Create virtual environment python -m venv venv source venv/bin/activate # Linux/Mac # or venv\\Scripts\\activate # Windows # Install development dependencies pip install -r requirements-dev.txt # Install pre-commit hooks pre-commit install","title":"Local Development"},{"location":"development/contributing/#testing","text":"","title":"Testing"},{"location":"development/contributing/#running-tests","text":"# Run all tests pytest tests/ -v # Run with coverage pytest tests/ -v --cov=rl_ids --cov-report=html # Run specific test categories pytest tests/unit/ -v pytest tests/integration/ -v","title":"Running Tests"},{"location":"development/contributing/#writing-tests","text":"Place unit tests in tests/unit/ Place integration tests in tests/integration/ Follow the naming convention: test_*.py Use descriptive test names: test_dqn_agent_training_convergence","title":"Writing Tests"},{"location":"development/contributing/#code-style","text":"We use several tools to maintain code quality: # Format code black rl_ids/ tests/ isort rl_ids/ tests/ # Type checking mypy rl_ids/ # Linting flake8 rl_ids/ tests/","title":"Code Style"},{"location":"development/contributing/#style-guidelines","text":"Follow PEP 8 for Python code style Use type hints for all public functions Write docstrings for modules, classes, and functions Keep line length under 88 characters (Black default)","title":"Style Guidelines"},{"location":"development/contributing/#documentation","text":"","title":"Documentation"},{"location":"development/contributing/#building-documentation","text":"# Install documentation dependencies pip install mkdocs-material # Serve locally with hot reload cd docs/ mkdocs serve # Build static documentation mkdocs build","title":"Building Documentation"},{"location":"development/contributing/#documentation-guidelines","text":"Update relevant documentation for any API changes Add new tutorials for significant features Keep examples up-to-date and tested Use clear, concise language","title":"Documentation Guidelines"},{"location":"development/contributing/#bug-reports","text":"When reporting bugs, please include: Environment Information Python version Operating system Package versions ( pip freeze ) Reproduction Steps Minimal code example Expected vs. actual behavior Error messages and stack traces Additional Context Configuration files Log outputs Screenshots (if applicable)","title":"Bug Reports"},{"location":"development/contributing/#feature-requests","text":"For new features: Check existing issues to avoid duplicates Describe the problem the feature would solve Propose a solution or implementation approach Consider backwards compatibility","title":"Feature Requests"},{"location":"development/contributing/#pull-request-process","text":"","title":"Pull Request Process"},{"location":"development/contributing/#before-submitting","text":"[ ] Tests pass: pytest tests/ -v [ ] Code is formatted: black rl_ids/ tests/ [ ] Imports are sorted: isort rl_ids/ tests/ [ ] Type checks pass: mypy rl_ids/ [ ] Documentation is updated [ ] CHANGELOG.md is updated","title":"Before Submitting"},{"location":"development/contributing/#pull-request-template","text":"## Description Brief description of changes ## Type of Change - [ ] Bug fix - [ ] New feature - [ ] Breaking change - [ ] Documentation update ## Testing - [ ] New tests added - [ ] All tests pass - [ ] Manual testing completed ## Checklist - [ ] Code follows style guidelines - [ ] Self-review completed - [ ] Documentation updated - [ ] CHANGELOG.md updated","title":"Pull Request Template"},{"location":"development/contributing/#architecture-guidelines","text":"","title":"Architecture Guidelines"},{"location":"development/contributing/#code-organization","text":"rl_ids/ \u251c\u2500\u2500 agents/ # RL agents (DQN, etc.) \u251c\u2500\u2500 environments/ # Gymnasium environments \u251c\u2500\u2500 modeling/ # Training and evaluation \u251c\u2500\u2500 make_dataset.py # Data processing \u251c\u2500\u2500 plots.py # Visualization \u2514\u2500\u2500 config.py # Configuration api/ \u251c\u2500\u2500 main.py # FastAPI application \u251c\u2500\u2500 models/ # Pydantic models \u251c\u2500\u2500 routes/ # API endpoints \u2514\u2500\u2500 services/ # Business logic","title":"Code Organization"},{"location":"development/contributing/#design-principles","text":"Modularity : Keep components loosely coupled Testability : Design for easy testing Configuration : Make components configurable Documentation : Document public APIs","title":"Design Principles"},{"location":"development/contributing/#contribution-areas","text":"","title":"Contribution Areas"},{"location":"development/contributing/#high-priority","text":"Performance optimizations Additional RL algorithms Enhanced monitoring and alerting Security improvements","title":"High Priority"},{"location":"development/contributing/#medium-priority","text":"Additional dataset support Visualization enhancements API feature additions Documentation improvements","title":"Medium Priority"},{"location":"development/contributing/#welcome-contributions","text":"Bug fixes and improvements Test coverage expansion Example scripts and tutorials Typo fixes and documentation clarity","title":"Welcome Contributions"},{"location":"development/contributing/#getting-help","text":"Documentation : Check docs/ for comprehensive guides Issues : Search existing issues or create a new one Discussions : Use GitHub Discussions for questions Email : Contact maintainers for security issues","title":"Getting Help"},{"location":"development/contributing/#recognition","text":"Contributors will be recognized in: - README.md contributors section - Release notes - Documentation credits Thank you for contributing to RL-IDS!","title":"Recognition"},{"location":"development/testing/","text":"Testing Guide This guide covers the testing strategy, setup, and execution for the RL-IDS project. Testing Overview RL-IDS uses a comprehensive testing approach that includes unit tests, integration tests, and performance tests to ensure reliability and correctness. Test Structure tests/ \u251c\u2500\u2500 test_api.py # API endpoint tests \u251c\u2500\u2500 conftest.py # Test configuration and fixtures (if exists) \u251c\u2500\u2500 unit/ # Unit tests (if exists) \u251c\u2500\u2500 integration/ # Integration tests (if exists) \u2514\u2500\u2500 performance/ # Performance tests (if exists) Current Test Suite API Tests ( tests/test_api.py ) The main test suite focuses on API functionality and includes: Test Classes and Methods TestRLIDSAPI Class - test_root_endpoint() : Tests the main API endpoint - test_health_check() : Verifies health check functionality - test_model_info() : Tests model information endpoint - test_single_prediction() : Tests individual prediction requests - Additional prediction and batch processing tests Key Test Areas Endpoint Validation Response status codes Response data structure Required fields presence Model Integration Model loading verification Prediction accuracy Input validation Error Handling Invalid input handling Exception responses Graceful degradation Running Tests Prerequisites Ensure you have the test dependencies installed: # Install test dependencies pip install pytest httpx # For coverage reports pip install pytest-cov # For async testing pip install pytest-asyncio Basic Test Execution # Run all tests pytest tests/ # Run with verbose output pytest tests/ -v # Run specific test file pytest tests/test_api.py -v # Run specific test method pytest tests/test_api.py::TestRLIDSAPI::test_root_endpoint -v Coverage Analysis # Run tests with coverage pytest tests/ --cov=rl_ids --cov=api # Generate HTML coverage report pytest tests/ --cov=rl_ids --cov=api --cov-report=html # View coverage report open htmlcov/index.html # macOS xdg-open htmlcov/index.html # Linux Test Output Examples Successful Test Run tests/test_api.py::TestRLIDSAPI::test_root_endpoint PASSED tests/test_api.py::TestRLIDSAPI::test_health_check PASSED tests/test_api.py::TestRLIDSAPI::test_model_info PASSED tests/test_api.py::TestRLIDSAPI::test_single_prediction PASSED ================= 4 passed in 2.34s ================= Coverage Report Name Stmts Miss Cover -------------------------------------------- api/__init__.py 0 0 100% api/main.py 45 5 89% api/models.py 23 2 91% api/services.py 34 8 76% -------------------------------------------- TOTAL 102 15 85% Test Configuration Environment Setup Create a test-specific environment configuration: # Create .env.test file cat > .env.test << EOF # Test Environment Configuration API_HOST=localhost API_PORT=8001 DEBUG=true LOG_LEVEL=DEBUG # Use test models/data MODEL_PATH=tests/fixtures/test_model.pt TEST_MODE=true EOF Test Fixtures Example test fixtures for consistent testing: # conftest.py (if exists) import pytest from fastapi.testclient import TestClient from api.main import app @pytest.fixture def client(): \"\"\"Create test client.\"\"\" return TestClient(app) @pytest.fixture def sample_features(): \"\"\"Sample feature vector for testing.\"\"\" return [0.1] * 78 # 78 features as expected by model @pytest.fixture def mock_model_response(): \"\"\"Mock model prediction response.\"\"\" return { \"prediction\": \"BENIGN\", \"confidence\": 0.95, \"attack_type\": None } Writing Tests Test Structure Guidelines def test_function_name(): \"\"\" Test description explaining what is being tested. This test verifies that [specific functionality] works correctly when [specific conditions] are met. \"\"\" # Arrange - Set up test data and conditions # Act - Execute the functionality being tested # Assert - Verify the results API Test Example def test_health_check(self): \"\"\"Test the health check endpoint returns correct status.\"\"\" # Arrange expected_keys = [\"status\", \"timestamp\", \"details\"] # Act response = self.client.get(\"/health\") data = response.json() # Assert assert response.status_code == 200 assert data[\"status\"] == \"healthy\" assert all(key in data for key in expected_keys) assert data[\"details\"][\"model_loaded\"] is True Model Test Example def test_model_prediction_format(): \"\"\"Test that model predictions return expected format.\"\"\" # Arrange from rl_ids.agents.dqn_agent import DQNAgent agent = DQNAgent() sample_input = torch.randn(1, 78) # Act prediction = agent.predict(sample_input) # Assert assert isinstance(prediction, dict) assert \"class\" in prediction assert \"confidence\" in prediction assert 0 <= prediction[\"confidence\"] <= 1 Test Categories Unit Tests Test individual components in isolation: # Test feature extraction def test_feature_extraction(): from rl_ids.make_dataset import extract_features # Test individual feature extraction functions # Test agent components def test_dqn_network(): from rl_ids.agents.dqn_agent import DQNNetwork # Test neural network components # Test utility functions def test_config_loading(): from rl_ids.config import load_config # Test configuration management Integration Tests Test component interactions: # Test full prediction pipeline def test_end_to_end_prediction(): # Test: Raw data \u2192 Features \u2192 Model \u2192 Prediction # Test API with real model def test_api_model_integration(): # Test API endpoints with actual model loading # Test monitoring pipeline def test_network_monitoring_flow(): # Test: Packet capture \u2192 Feature extraction \u2192 Detection Performance Tests Test system performance characteristics: import time import pytest def test_prediction_latency(): \"\"\"Test that predictions complete within acceptable time.\"\"\" start_time = time.time() # Make prediction response = client.post(\"/predict\", json=sample_data) end_time = time.time() latency = end_time - start_time assert response.status_code == 200 assert latency < 0.1 # Less than 100ms @pytest.mark.performance def test_batch_prediction_throughput(): \"\"\"Test batch prediction throughput.\"\"\" batch_size = 100 batch_data = [sample_features] * batch_size start_time = time.time() response = client.post(\"/predict/batch\", json=batch_data) end_time = time.time() throughput = batch_size / (end_time - start_time) assert throughput > 50 # At least 50 predictions per second Continuous Integration GitHub Actions Example # .github/workflows/test.yml name: Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest strategy: matrix: python-version: [3.11, 3.12, 3.13] steps: - uses: actions/checkout@v3 - name: Set up Python ${{ matrix.python-version }} uses: actions/setup-python@v3 with: python-version: ${{ matrix.python-version }} - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt pip install pytest pytest-cov - name: Run tests run: | pytest tests/ -v --cov=rl_ids --cov=api - name: Upload coverage uses: codecov/codecov-action@v3 Test Data Management Mock Data Generation def generate_test_features(attack_type=\"benign\", count=100): \"\"\"Generate realistic test features.\"\"\" if attack_type == \"benign\": # Generate normal traffic patterns features = np.random.normal(0, 1, (count, 78)) elif attack_type == \"ddos\": # Generate DDoS-like patterns features = np.random.normal(2, 0.5, (count, 78)) return features.tolist() Test Dataset Management # tests/fixtures/data.py import pandas as pd def load_test_dataset(): \"\"\"Load small test dataset for consistent testing.\"\"\" return pd.read_csv(\"tests/fixtures/test_sample.csv\") def create_test_environment_data(): \"\"\"Create test data for RL environment.\"\"\" states = generate_test_features(\"benign\", 50) actions = [0] * 25 + [1] * 25 # Mix of normal and attack classifications rewards = [1.0] * 50 # All correct classifications return states, actions, rewards Debugging Tests Common Issues and Solutions Test Failures # Run failed tests only pytest --lf # Run tests with detailed output pytest -vv --tb=long # Drop into debugger on failure pytest --pdb Model Loading Issues # Skip tests if model not available @pytest.mark.skipif(not os.path.exists(\"models/dqn_model_best.pt\"), reason=\"Model file not found\") def test_model_prediction(): # Test code here Async Test Issues # For testing async endpoints @pytest.mark.asyncio async def test_async_endpoint(): async with httpx.AsyncClient(app=app, base_url=\"http://test\") as ac: response = await ac.get(\"/async-endpoint\") assert response.status_code == 200 Test Metrics and Quality Coverage Goals Minimum Coverage : 80% overall Critical Components : 95% coverage API Endpoints : 100% coverage Quality Metrics Test Execution Time : < 30 seconds for full suite Flaky Test Rate : < 5% Test Maintenance : Regular updates with code changes Best Practices Test Writing Clear Test Names : Describe what is being tested Single Responsibility : One assertion per test concept Arrange-Act-Assert : Clear test structure Independent Tests : No dependencies between tests Test Maintenance Regular Updates : Keep tests updated with code changes Cleanup : Remove obsolete tests Documentation : Document complex test scenarios Performance : Monitor test execution time Test Data Deterministic : Use fixed seeds for reproducible results Realistic : Use data that represents real-world scenarios Isolated : Don't depend on external resources Clean : Clean up test artifacts after execution Running Specific Test Scenarios API Testing # Test only API endpoints pytest tests/test_api.py -v # Test specific API functionality pytest tests/test_api.py -k \"prediction\" -v Model Testing # Test model-related functionality pytest tests/ -k \"model\" -v # Test with different model configurations MODEL_CONFIG=test pytest tests/ -v Performance Testing # Run performance tests pytest tests/ -m performance -v # Run with performance profiling pytest tests/ --profile -v","title":"Testing"},{"location":"development/testing/#testing-guide","text":"This guide covers the testing strategy, setup, and execution for the RL-IDS project.","title":"Testing Guide"},{"location":"development/testing/#testing-overview","text":"RL-IDS uses a comprehensive testing approach that includes unit tests, integration tests, and performance tests to ensure reliability and correctness.","title":"Testing Overview"},{"location":"development/testing/#test-structure","text":"tests/ \u251c\u2500\u2500 test_api.py # API endpoint tests \u251c\u2500\u2500 conftest.py # Test configuration and fixtures (if exists) \u251c\u2500\u2500 unit/ # Unit tests (if exists) \u251c\u2500\u2500 integration/ # Integration tests (if exists) \u2514\u2500\u2500 performance/ # Performance tests (if exists)","title":"Test Structure"},{"location":"development/testing/#current-test-suite","text":"","title":"Current Test Suite"},{"location":"development/testing/#api-tests-teststest_apipy","text":"The main test suite focuses on API functionality and includes:","title":"API Tests (tests/test_api.py)"},{"location":"development/testing/#test-classes-and-methods","text":"TestRLIDSAPI Class - test_root_endpoint() : Tests the main API endpoint - test_health_check() : Verifies health check functionality - test_model_info() : Tests model information endpoint - test_single_prediction() : Tests individual prediction requests - Additional prediction and batch processing tests","title":"Test Classes and Methods"},{"location":"development/testing/#key-test-areas","text":"Endpoint Validation Response status codes Response data structure Required fields presence Model Integration Model loading verification Prediction accuracy Input validation Error Handling Invalid input handling Exception responses Graceful degradation","title":"Key Test Areas"},{"location":"development/testing/#running-tests","text":"","title":"Running Tests"},{"location":"development/testing/#prerequisites","text":"Ensure you have the test dependencies installed: # Install test dependencies pip install pytest httpx # For coverage reports pip install pytest-cov # For async testing pip install pytest-asyncio","title":"Prerequisites"},{"location":"development/testing/#basic-test-execution","text":"# Run all tests pytest tests/ # Run with verbose output pytest tests/ -v # Run specific test file pytest tests/test_api.py -v # Run specific test method pytest tests/test_api.py::TestRLIDSAPI::test_root_endpoint -v","title":"Basic Test Execution"},{"location":"development/testing/#coverage-analysis","text":"# Run tests with coverage pytest tests/ --cov=rl_ids --cov=api # Generate HTML coverage report pytest tests/ --cov=rl_ids --cov=api --cov-report=html # View coverage report open htmlcov/index.html # macOS xdg-open htmlcov/index.html # Linux","title":"Coverage Analysis"},{"location":"development/testing/#test-output-examples","text":"Successful Test Run tests/test_api.py::TestRLIDSAPI::test_root_endpoint PASSED tests/test_api.py::TestRLIDSAPI::test_health_check PASSED tests/test_api.py::TestRLIDSAPI::test_model_info PASSED tests/test_api.py::TestRLIDSAPI::test_single_prediction PASSED ================= 4 passed in 2.34s ================= Coverage Report Name Stmts Miss Cover -------------------------------------------- api/__init__.py 0 0 100% api/main.py 45 5 89% api/models.py 23 2 91% api/services.py 34 8 76% -------------------------------------------- TOTAL 102 15 85%","title":"Test Output Examples"},{"location":"development/testing/#test-configuration","text":"","title":"Test Configuration"},{"location":"development/testing/#environment-setup","text":"Create a test-specific environment configuration: # Create .env.test file cat > .env.test << EOF # Test Environment Configuration API_HOST=localhost API_PORT=8001 DEBUG=true LOG_LEVEL=DEBUG # Use test models/data MODEL_PATH=tests/fixtures/test_model.pt TEST_MODE=true EOF","title":"Environment Setup"},{"location":"development/testing/#test-fixtures","text":"Example test fixtures for consistent testing: # conftest.py (if exists) import pytest from fastapi.testclient import TestClient from api.main import app @pytest.fixture def client(): \"\"\"Create test client.\"\"\" return TestClient(app) @pytest.fixture def sample_features(): \"\"\"Sample feature vector for testing.\"\"\" return [0.1] * 78 # 78 features as expected by model @pytest.fixture def mock_model_response(): \"\"\"Mock model prediction response.\"\"\" return { \"prediction\": \"BENIGN\", \"confidence\": 0.95, \"attack_type\": None }","title":"Test Fixtures"},{"location":"development/testing/#writing-tests","text":"","title":"Writing Tests"},{"location":"development/testing/#test-structure-guidelines","text":"def test_function_name(): \"\"\" Test description explaining what is being tested. This test verifies that [specific functionality] works correctly when [specific conditions] are met. \"\"\" # Arrange - Set up test data and conditions # Act - Execute the functionality being tested # Assert - Verify the results","title":"Test Structure Guidelines"},{"location":"development/testing/#api-test-example","text":"def test_health_check(self): \"\"\"Test the health check endpoint returns correct status.\"\"\" # Arrange expected_keys = [\"status\", \"timestamp\", \"details\"] # Act response = self.client.get(\"/health\") data = response.json() # Assert assert response.status_code == 200 assert data[\"status\"] == \"healthy\" assert all(key in data for key in expected_keys) assert data[\"details\"][\"model_loaded\"] is True","title":"API Test Example"},{"location":"development/testing/#model-test-example","text":"def test_model_prediction_format(): \"\"\"Test that model predictions return expected format.\"\"\" # Arrange from rl_ids.agents.dqn_agent import DQNAgent agent = DQNAgent() sample_input = torch.randn(1, 78) # Act prediction = agent.predict(sample_input) # Assert assert isinstance(prediction, dict) assert \"class\" in prediction assert \"confidence\" in prediction assert 0 <= prediction[\"confidence\"] <= 1","title":"Model Test Example"},{"location":"development/testing/#test-categories","text":"","title":"Test Categories"},{"location":"development/testing/#unit-tests","text":"Test individual components in isolation: # Test feature extraction def test_feature_extraction(): from rl_ids.make_dataset import extract_features # Test individual feature extraction functions # Test agent components def test_dqn_network(): from rl_ids.agents.dqn_agent import DQNNetwork # Test neural network components # Test utility functions def test_config_loading(): from rl_ids.config import load_config # Test configuration management","title":"Unit Tests"},{"location":"development/testing/#integration-tests","text":"Test component interactions: # Test full prediction pipeline def test_end_to_end_prediction(): # Test: Raw data \u2192 Features \u2192 Model \u2192 Prediction # Test API with real model def test_api_model_integration(): # Test API endpoints with actual model loading # Test monitoring pipeline def test_network_monitoring_flow(): # Test: Packet capture \u2192 Feature extraction \u2192 Detection","title":"Integration Tests"},{"location":"development/testing/#performance-tests","text":"Test system performance characteristics: import time import pytest def test_prediction_latency(): \"\"\"Test that predictions complete within acceptable time.\"\"\" start_time = time.time() # Make prediction response = client.post(\"/predict\", json=sample_data) end_time = time.time() latency = end_time - start_time assert response.status_code == 200 assert latency < 0.1 # Less than 100ms @pytest.mark.performance def test_batch_prediction_throughput(): \"\"\"Test batch prediction throughput.\"\"\" batch_size = 100 batch_data = [sample_features] * batch_size start_time = time.time() response = client.post(\"/predict/batch\", json=batch_data) end_time = time.time() throughput = batch_size / (end_time - start_time) assert throughput > 50 # At least 50 predictions per second","title":"Performance Tests"},{"location":"development/testing/#continuous-integration","text":"","title":"Continuous Integration"},{"location":"development/testing/#github-actions-example","text":"# .github/workflows/test.yml name: Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest strategy: matrix: python-version: [3.11, 3.12, 3.13] steps: - uses: actions/checkout@v3 - name: Set up Python ${{ matrix.python-version }} uses: actions/setup-python@v3 with: python-version: ${{ matrix.python-version }} - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt pip install pytest pytest-cov - name: Run tests run: | pytest tests/ -v --cov=rl_ids --cov=api - name: Upload coverage uses: codecov/codecov-action@v3","title":"GitHub Actions Example"},{"location":"development/testing/#test-data-management","text":"","title":"Test Data Management"},{"location":"development/testing/#mock-data-generation","text":"def generate_test_features(attack_type=\"benign\", count=100): \"\"\"Generate realistic test features.\"\"\" if attack_type == \"benign\": # Generate normal traffic patterns features = np.random.normal(0, 1, (count, 78)) elif attack_type == \"ddos\": # Generate DDoS-like patterns features = np.random.normal(2, 0.5, (count, 78)) return features.tolist()","title":"Mock Data Generation"},{"location":"development/testing/#test-dataset-management","text":"# tests/fixtures/data.py import pandas as pd def load_test_dataset(): \"\"\"Load small test dataset for consistent testing.\"\"\" return pd.read_csv(\"tests/fixtures/test_sample.csv\") def create_test_environment_data(): \"\"\"Create test data for RL environment.\"\"\" states = generate_test_features(\"benign\", 50) actions = [0] * 25 + [1] * 25 # Mix of normal and attack classifications rewards = [1.0] * 50 # All correct classifications return states, actions, rewards","title":"Test Dataset Management"},{"location":"development/testing/#debugging-tests","text":"","title":"Debugging Tests"},{"location":"development/testing/#common-issues-and-solutions","text":"","title":"Common Issues and Solutions"},{"location":"development/testing/#test-failures","text":"# Run failed tests only pytest --lf # Run tests with detailed output pytest -vv --tb=long # Drop into debugger on failure pytest --pdb","title":"Test Failures"},{"location":"development/testing/#model-loading-issues","text":"# Skip tests if model not available @pytest.mark.skipif(not os.path.exists(\"models/dqn_model_best.pt\"), reason=\"Model file not found\") def test_model_prediction(): # Test code here","title":"Model Loading Issues"},{"location":"development/testing/#async-test-issues","text":"# For testing async endpoints @pytest.mark.asyncio async def test_async_endpoint(): async with httpx.AsyncClient(app=app, base_url=\"http://test\") as ac: response = await ac.get(\"/async-endpoint\") assert response.status_code == 200","title":"Async Test Issues"},{"location":"development/testing/#test-metrics-and-quality","text":"","title":"Test Metrics and Quality"},{"location":"development/testing/#coverage-goals","text":"Minimum Coverage : 80% overall Critical Components : 95% coverage API Endpoints : 100% coverage","title":"Coverage Goals"},{"location":"development/testing/#quality-metrics","text":"Test Execution Time : < 30 seconds for full suite Flaky Test Rate : < 5% Test Maintenance : Regular updates with code changes","title":"Quality Metrics"},{"location":"development/testing/#best-practices","text":"","title":"Best Practices"},{"location":"development/testing/#test-writing","text":"Clear Test Names : Describe what is being tested Single Responsibility : One assertion per test concept Arrange-Act-Assert : Clear test structure Independent Tests : No dependencies between tests","title":"Test Writing"},{"location":"development/testing/#test-maintenance","text":"Regular Updates : Keep tests updated with code changes Cleanup : Remove obsolete tests Documentation : Document complex test scenarios Performance : Monitor test execution time","title":"Test Maintenance"},{"location":"development/testing/#test-data","text":"Deterministic : Use fixed seeds for reproducible results Realistic : Use data that represents real-world scenarios Isolated : Don't depend on external resources Clean : Clean up test artifacts after execution","title":"Test Data"},{"location":"development/testing/#running-specific-test-scenarios","text":"","title":"Running Specific Test Scenarios"},{"location":"development/testing/#api-testing","text":"# Test only API endpoints pytest tests/test_api.py -v # Test specific API functionality pytest tests/test_api.py -k \"prediction\" -v","title":"API Testing"},{"location":"development/testing/#model-testing","text":"# Test model-related functionality pytest tests/ -k \"model\" -v # Test with different model configurations MODEL_CONFIG=test pytest tests/ -v","title":"Model Testing"},{"location":"development/testing/#performance-testing","text":"# Run performance tests pytest tests/ -m performance -v # Run with performance profiling pytest tests/ --profile -v","title":"Performance Testing"},{"location":"modules/","text":"RL-IDS Modules This section provides detailed documentation for all RL-IDS system modules, covering their architecture, functionality, and usage. Overview The RL-IDS system is built with a modular architecture that separates concerns and enables easy extension and maintenance. Each module handles specific aspects of the intrusion detection pipeline. Core Modules Reinforcement Learning Agents The heart of the RL-IDS system, implementing Deep Q-Network (DQN) algorithms for adaptive threat detection. Key Components: - DQN Agent : Main reinforcement learning agent - Neural Networks : Q-network and target network architectures - Experience Replay : Memory management for training - Training Pipeline : Complete training and evaluation framework Features: - Adaptive learning from network traffic patterns - Real-time decision making for threat classification - Continuous improvement through reward-based learning - Support for various network environments Feature Extraction Comprehensive feature engineering module that transforms raw network data into meaningful representations for machine learning. Key Components: - CICIDS2017 Features : 78 standardized network flow features - Flow Tracking : Bidirectional flow analysis and aggregation - Statistical Analysis : Time-series and distribution analysis - Data Preprocessing : Normalization and validation Features: - Real-time feature extraction from network packets - Statistical traffic characterization - Protocol-specific feature analysis - Scalable processing pipeline Supporting Modules Configuration Module ( rl_ids/config.py ) Central configuration management for all system components. Features: - Environment-based configuration - Model and training parameters - Network monitoring settings - API and service configuration Key Settings: # Model Configuration MODEL_CONFIG = { 'input_size': 78, 'hidden_layers': [256, 128, 64], 'output_size': 2, # Binary classification 'learning_rate': 0.001, 'gamma': 0.99, 'epsilon': 0.1 } # Training Configuration TRAINING_CONFIG = { 'batch_size': 32, 'episodes': 500, 'memory_size': 10000, 'target_update': 100 } # Environment Configuration ENV_CONFIG = { 'max_steps': 1000, 'reward_function': 'balanced', 'state_normalization': True } Environment Module ( rl_ids/environments/ids_env.py ) Gymnasium-compatible environment for training RL agents on intrusion detection tasks. Features: - CICIDS2017 dataset integration - Configurable reward functions - Episode management - State space normalization Environment Specifications: - State Space : 78-dimensional continuous space (network features) - Action Space : Discrete space for classification decisions - Reward Structure : Based on detection accuracy and false positive rates - Episode Length : Configurable based on dataset size Plotting and Visualization ( rl_ids/plots.py ) Comprehensive visualization tools for training analysis and model evaluation. Features: - Training progress visualization - Performance metrics plotting - Confusion matrix generation - Feature importance analysis Available Plots: - Training loss and reward curves - Episode performance trends - Classification performance metrics - Feature distribution analysis Module Dependencies \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Module Dependency Graph \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Config \u2502 \u2502 Features \u2502 \u2502 Agents \u2502 \u2502 \u2502 \u2502 \u2502\u25c4\u2500\u2500\u2500\u2524 \u2502\u25c4\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u25b2 \u25b2 \u25b2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Plots \u2502 \u2502Environment \u2502 \u2502 Training \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Integration Patterns Data Flow Integration # Example: Complete detection pipeline from rl_ids.make_dataset import extract_features from rl_ids.agents.dqn_agent import DQNAgent from rl_ids.config import MODEL_CONFIG # Initialize components agent = DQNAgent(MODEL_CONFIG) raw_data = capture_network_traffic() # Process data through pipeline features = extract_features(raw_data) prediction = agent.predict(features) Training Integration # Example: Training pipeline integration from rl_ids.environments.ids_env import IDSEnvironment from rl_ids.modeling.train import train_dqn_agent from rl_ids.plots import plot_training_metrics # Set up training environment env = IDSEnvironment() agent = train_dqn_agent(env, episodes=500) # Visualize results plot_training_metrics(agent.training_history) API Integration # Example: API service integration from api.main import app from rl_ids.agents.dqn_agent import DQNAgent # Load trained model for API agent = DQNAgent.load_model('models/dqn_model_best.pt') # API automatically uses loaded model for predictions Module Configuration Environment Variables # Model Configuration MODEL_PATH=models/dqn_model_best.pt FEATURE_SCALER_PATH=models/feature_scaler.pkl # Training Configuration EPISODES=500 BATCH_SIZE=32 LEARNING_RATE=0.001 # Environment Configuration MAX_STEPS=1000 REWARD_TYPE=balanced Configuration Files # config.py structure class Config: # Model settings MODEL_CONFIG = {...} # Training settings TRAINING_CONFIG = {...} # Environment settings ENV_CONFIG = {...} # API settings API_CONFIG = {...} Performance Considerations Memory Usage Feature Extraction : O(n) where n is the number of flows Agent Memory : Configurable replay buffer size Training : Batch processing for memory efficiency Inference : Single-pass processing for real-time performance Computational Complexity Feature Calculation : Linear with packet count Model Inference : Constant time per prediction Training : Depends on dataset size and architecture Environment : Minimal overhead for state transitions Scalability Horizontal : Multiple agent instances for load distribution Vertical : GPU acceleration for training and inference Memory : Configurable buffer sizes and batch processing Storage : Efficient model checkpointing and data handling Best Practices Module Usage Initialize Configuration First : Load configuration before other modules Use Consistent Interfaces : Follow established patterns for integration Handle Errors Gracefully : Implement proper error handling and logging Monitor Performance : Use built-in metrics and monitoring tools Extension Guidelines Follow Module Structure : Maintain consistent organization Document Interfaces : Provide clear API documentation Write Tests : Include unit tests for new functionality Maintain Compatibility : Ensure backward compatibility when possible Development Workflow Local Testing : Use provided test datasets and configurations Integration Testing : Test module interactions thoroughly Performance Testing : Benchmark against expected performance Documentation : Update documentation for any changes Getting Started Quick Start Read Module Documentation : Start with Agents and Features Review Configuration : Check rl_ids/config.py for available settings Run Examples : Execute provided example scripts Explore API : Use the API documentation for integration examples Development Setup Install Dependencies : Follow installation guide Configure Environment : Set up configuration files Run Tests : Verify module functionality Start Development : Begin with existing examples and extend For detailed information about each module, please refer to their individual documentation pages.","title":"Core Components"},{"location":"modules/#rl-ids-modules","text":"This section provides detailed documentation for all RL-IDS system modules, covering their architecture, functionality, and usage.","title":"RL-IDS Modules"},{"location":"modules/#overview","text":"The RL-IDS system is built with a modular architecture that separates concerns and enables easy extension and maintenance. Each module handles specific aspects of the intrusion detection pipeline.","title":"Overview"},{"location":"modules/#core-modules","text":"","title":"Core Modules"},{"location":"modules/#reinforcement-learning-agents","text":"The heart of the RL-IDS system, implementing Deep Q-Network (DQN) algorithms for adaptive threat detection. Key Components: - DQN Agent : Main reinforcement learning agent - Neural Networks : Q-network and target network architectures - Experience Replay : Memory management for training - Training Pipeline : Complete training and evaluation framework Features: - Adaptive learning from network traffic patterns - Real-time decision making for threat classification - Continuous improvement through reward-based learning - Support for various network environments","title":"Reinforcement Learning Agents"},{"location":"modules/#feature-extraction","text":"Comprehensive feature engineering module that transforms raw network data into meaningful representations for machine learning. Key Components: - CICIDS2017 Features : 78 standardized network flow features - Flow Tracking : Bidirectional flow analysis and aggregation - Statistical Analysis : Time-series and distribution analysis - Data Preprocessing : Normalization and validation Features: - Real-time feature extraction from network packets - Statistical traffic characterization - Protocol-specific feature analysis - Scalable processing pipeline","title":"Feature Extraction"},{"location":"modules/#supporting-modules","text":"","title":"Supporting Modules"},{"location":"modules/#configuration-module-rl_idsconfigpy","text":"Central configuration management for all system components. Features: - Environment-based configuration - Model and training parameters - Network monitoring settings - API and service configuration Key Settings: # Model Configuration MODEL_CONFIG = { 'input_size': 78, 'hidden_layers': [256, 128, 64], 'output_size': 2, # Binary classification 'learning_rate': 0.001, 'gamma': 0.99, 'epsilon': 0.1 } # Training Configuration TRAINING_CONFIG = { 'batch_size': 32, 'episodes': 500, 'memory_size': 10000, 'target_update': 100 } # Environment Configuration ENV_CONFIG = { 'max_steps': 1000, 'reward_function': 'balanced', 'state_normalization': True }","title":"Configuration Module (rl_ids/config.py)"},{"location":"modules/#environment-module-rl_idsenvironmentsids_envpy","text":"Gymnasium-compatible environment for training RL agents on intrusion detection tasks. Features: - CICIDS2017 dataset integration - Configurable reward functions - Episode management - State space normalization Environment Specifications: - State Space : 78-dimensional continuous space (network features) - Action Space : Discrete space for classification decisions - Reward Structure : Based on detection accuracy and false positive rates - Episode Length : Configurable based on dataset size","title":"Environment Module (rl_ids/environments/ids_env.py)"},{"location":"modules/#plotting-and-visualization-rl_idsplotspy","text":"Comprehensive visualization tools for training analysis and model evaluation. Features: - Training progress visualization - Performance metrics plotting - Confusion matrix generation - Feature importance analysis Available Plots: - Training loss and reward curves - Episode performance trends - Classification performance metrics - Feature distribution analysis","title":"Plotting and Visualization (rl_ids/plots.py)"},{"location":"modules/#module-dependencies","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Module Dependency Graph \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Config \u2502 \u2502 Features \u2502 \u2502 Agents \u2502 \u2502 \u2502 \u2502 \u2502\u25c4\u2500\u2500\u2500\u2524 \u2502\u25c4\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u25b2 \u25b2 \u25b2 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 Plots \u2502 \u2502Environment \u2502 \u2502 Training \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Module Dependencies"},{"location":"modules/#integration-patterns","text":"","title":"Integration Patterns"},{"location":"modules/#data-flow-integration","text":"# Example: Complete detection pipeline from rl_ids.make_dataset import extract_features from rl_ids.agents.dqn_agent import DQNAgent from rl_ids.config import MODEL_CONFIG # Initialize components agent = DQNAgent(MODEL_CONFIG) raw_data = capture_network_traffic() # Process data through pipeline features = extract_features(raw_data) prediction = agent.predict(features)","title":"Data Flow Integration"},{"location":"modules/#training-integration","text":"# Example: Training pipeline integration from rl_ids.environments.ids_env import IDSEnvironment from rl_ids.modeling.train import train_dqn_agent from rl_ids.plots import plot_training_metrics # Set up training environment env = IDSEnvironment() agent = train_dqn_agent(env, episodes=500) # Visualize results plot_training_metrics(agent.training_history)","title":"Training Integration"},{"location":"modules/#api-integration","text":"# Example: API service integration from api.main import app from rl_ids.agents.dqn_agent import DQNAgent # Load trained model for API agent = DQNAgent.load_model('models/dqn_model_best.pt') # API automatically uses loaded model for predictions","title":"API Integration"},{"location":"modules/#module-configuration","text":"","title":"Module Configuration"},{"location":"modules/#environment-variables","text":"# Model Configuration MODEL_PATH=models/dqn_model_best.pt FEATURE_SCALER_PATH=models/feature_scaler.pkl # Training Configuration EPISODES=500 BATCH_SIZE=32 LEARNING_RATE=0.001 # Environment Configuration MAX_STEPS=1000 REWARD_TYPE=balanced","title":"Environment Variables"},{"location":"modules/#configuration-files","text":"# config.py structure class Config: # Model settings MODEL_CONFIG = {...} # Training settings TRAINING_CONFIG = {...} # Environment settings ENV_CONFIG = {...} # API settings API_CONFIG = {...}","title":"Configuration Files"},{"location":"modules/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"modules/#memory-usage","text":"Feature Extraction : O(n) where n is the number of flows Agent Memory : Configurable replay buffer size Training : Batch processing for memory efficiency Inference : Single-pass processing for real-time performance","title":"Memory Usage"},{"location":"modules/#computational-complexity","text":"Feature Calculation : Linear with packet count Model Inference : Constant time per prediction Training : Depends on dataset size and architecture Environment : Minimal overhead for state transitions","title":"Computational Complexity"},{"location":"modules/#scalability","text":"Horizontal : Multiple agent instances for load distribution Vertical : GPU acceleration for training and inference Memory : Configurable buffer sizes and batch processing Storage : Efficient model checkpointing and data handling","title":"Scalability"},{"location":"modules/#best-practices","text":"","title":"Best Practices"},{"location":"modules/#module-usage","text":"Initialize Configuration First : Load configuration before other modules Use Consistent Interfaces : Follow established patterns for integration Handle Errors Gracefully : Implement proper error handling and logging Monitor Performance : Use built-in metrics and monitoring tools","title":"Module Usage"},{"location":"modules/#extension-guidelines","text":"Follow Module Structure : Maintain consistent organization Document Interfaces : Provide clear API documentation Write Tests : Include unit tests for new functionality Maintain Compatibility : Ensure backward compatibility when possible","title":"Extension Guidelines"},{"location":"modules/#development-workflow","text":"Local Testing : Use provided test datasets and configurations Integration Testing : Test module interactions thoroughly Performance Testing : Benchmark against expected performance Documentation : Update documentation for any changes","title":"Development Workflow"},{"location":"modules/#getting-started","text":"","title":"Getting Started"},{"location":"modules/#quick-start","text":"Read Module Documentation : Start with Agents and Features Review Configuration : Check rl_ids/config.py for available settings Run Examples : Execute provided example scripts Explore API : Use the API documentation for integration examples","title":"Quick Start"},{"location":"modules/#development-setup","text":"Install Dependencies : Follow installation guide Configure Environment : Set up configuration files Run Tests : Verify module functionality Start Development : Begin with existing examples and extend For detailed information about each module, please refer to their individual documentation pages.","title":"Development Setup"},{"location":"modules/agents/","text":"RL Agents This document covers the reinforcement learning agents used in RL-IDS, specifically the Deep Q-Network (DQN) implementation. Overview The RL-IDS system uses Deep Q-Network (DQN) agents for network intrusion detection. The implementation is located in rl_ids/agents/dqn_agent.py and provides a configurable, high-performance agent suitable for multi-class classification tasks. DQN Agent Architecture Core Components The DQN agent consists of several key components: Deep Q-Network Model - Neural network for Q-value estimation Target Network - Stabilized target for Q-learning updates Replay Buffer - Experience replay for stable learning Epsilon-Greedy Policy - Exploration vs exploitation strategy Network Architecture class DQN(nn.Module): \"\"\"Deep Q-Network model.\"\"\" def __init__(self, input_dim: int, output_dim: int, hidden_dims: List[int] = [256, 128]): super(DQN, self).__init__() layers = [] prev_dim = input_dim for hidden_dim in hidden_dims: layers.extend([nn.Linear(prev_dim, hidden_dim), nn.ReLU()]) prev_dim = hidden_dim layers.append(nn.Linear(prev_dim, output_dim)) self.fc = nn.Sequential(*layers) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\"Forward pass through the network.\"\"\" return self.fc(x) Default Architecture: - Input Layer: 78 features (CICIDS2017 feature set) - Hidden Layers: Configurable (default: [1024, 512, 256, 128]) - Output Layer: 15 classes (attack types + benign) - Activation: ReLU between layers - Output: Raw Q-values (no activation) DQN Configuration Configuration Class class DQNConfig(BaseModel): \"\"\"Configuration for DQN Agent.\"\"\" state_dim: int = Field(..., description=\"State space dimension\") action_dim: int = Field(..., description=\"Action space dimension\") lr: float = Field(1e-4, description=\"Learning rate\") gamma: float = Field(0.99, description=\"Discount factor\") epsilon: float = Field(1.0, description=\"Initial exploration rate\") eps_decay: float = Field(0.995, description=\"Epsilon decay rate\") eps_min: float = Field(0.1, description=\"Minimum epsilon value\") memory_size: int = Field(3000000, description=\"Replay buffer size\") batch_size: int = Field(64, description=\"Training batch size\") hidden_dims: List[int] = Field([256, 128], description=\"Hidden layer dimensions\") Training Configuration The training script ( rl_ids/modeling/train.py ) provides extensive configuration options: # Core training parameters num_episodes: int = 500 # Number of training episodes target_update_interval: int = 10 # Target network update frequency lr: float = 1e-4 # Learning rate gamma: float = 0.995 # Discount factor # Exploration parameters epsilon: float = 1.0 # Initial exploration rate eps_decay: float = 0.9995 # Epsilon decay rate eps_min: float = 0.01 # Minimum epsilon value # Network architecture hidden_dims: str = \"1024,512,256,128\" # Hidden layer dimensions dropout_rate: float = 0.2 # Dropout rate for regularization use_layer_norm: bool = True # Use layer normalization # Training optimization memory_size: int = 100000 # Replay buffer size batch_size: int = 256 # Training batch size warmup_steps: int = 1000 # Warmup steps before training # Advanced features curriculum_learning: bool = True # Use curriculum learning double_dqn: bool = True # Use Double DQN dueling: bool = True # Use Dueling DQN architecture prioritized_replay: bool = False # Use prioritized experience replay DQN Agent Implementation Agent Class class DQNAgent: \"\"\"Deep Q-Network Agent for reinforcement learning.\"\"\" def __init__(self, config: DQNConfig): \"\"\"Initialize DQN Agent with configuration.\"\"\" self.config = config self.state_dim = config.state_dim self.action_dim = config.action_dim self.gamma = config.gamma self.epsilon = config.epsilon self.eps_decay = config.eps_decay self.eps_min = config.eps_min self.batch_size = config.batch_size # Device selection self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Initialize networks self.model = DQN(config.state_dim, config.action_dim, config.hidden_dims).to(self.device) self.target_model = DQN(config.state_dim, config.action_dim, config.hidden_dims).to(self.device) self.update_target() # Training components self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr) self.memory = deque(maxlen=config.memory_size) # Training metrics self.training_step = 0 self.episode_count = 0 Key Methods Action Selection def act(self, state: np.ndarray, training: bool = True) -> int: \"\"\"Choose action using epsilon-greedy policy.\"\"\" if training and random.random() < self.epsilon: action = random.randint(0, self.action_dim - 1) return action state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) with torch.no_grad(): q_values = self.model(state_tensor) action = q_values.argmax().item() return action Experience Storage def remember(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool) -> None: \"\"\"Store experience in replay buffer.\"\"\" self.memory.append((state, action, reward, next_state, done)) Training Step def replay(self) -> Optional[float]: \"\"\"Train the model on a batch of experiences.\"\"\" if len(self.memory) < self.batch_size: return None batch = random.sample(self.memory, self.batch_size) states, actions, rewards, next_states, dones = zip(*batch) # Convert to tensors states = torch.FloatTensor(np.array(states)).to(self.device) actions = torch.LongTensor(actions).unsqueeze(1).to(self.device) rewards = torch.FloatTensor(rewards).to(self.device) next_states = torch.FloatTensor(np.array(next_states)).to(self.device) dones = torch.BoolTensor(dones).to(self.device) # Compute Q-values curr_Q = self.model(states).gather(1, actions).squeeze() next_Q = self.target_model(next_states).max(1)[0] target_Q = rewards + self.gamma * next_Q * (~dones) # Compute loss and update loss = self.criterion(curr_Q, target_Q.detach()) self.optimizer.zero_grad() loss.backward() self.optimizer.step() # Update epsilon if self.epsilon > self.eps_min: self.epsilon *= self.eps_decay self.training_step += 1 return loss.item() Training Process Enhanced Training Pipeline The training process ( rl_ids/modeling/train.py ) includes several advanced features: 1. Curriculum Learning Training progresses through stages of increasing difficulty: if curriculum_learning: stage_size = num_episodes // curriculum_stages for i in range(curriculum_stages): start_ep = i * stage_size end_ep = min((i + 1) * stage_size, num_episodes) curriculum_episodes.append((start_ep, end_ep, i + 1)) 2. Learning Rate Scheduling Adaptive learning rate adjustment: if lr_scheduler == \"cosine\": scheduler = torch.optim.lr_scheduler.CosineAnnealingLR( agent.optimizer, T_max=num_episodes, eta_min=lr * 0.01 ) elif lr_scheduler == \"step\": scheduler = torch.optim.lr_scheduler.StepLR( agent.optimizer, step_size=num_episodes // 4, gamma=0.5 ) 3. Early Stopping Prevents overfitting with validation-based early stopping: if val_accuracy > best_val_accuracy: best_val_accuracy = val_accuracy patience_counter = 0 agent.save_model(best_model_path) else: patience_counter += 1 if patience_counter >= early_stopping_patience: logger.info(f\"Early stopping triggered at episode {episode}\") break 4. Gradient Clipping Stabilizes training with gradient clipping: if grad_clip > 0: torch.nn.utils.clip_grad_norm_(agent.model.parameters(), grad_clip) Training Metrics The system tracks comprehensive training metrics: # Core metrics train_rewards = [] # Episode rewards train_losses = [] # Training losses train_accuracies = [] # Episode accuracies val_accuracies = [] # Validation accuracies learning_rates = [] # Learning rate progression episode_lengths = [] # Steps per episode # Advanced metrics reward_stability = np.std(recent_rewards) / (np.mean(recent_rewards) + 1e-8) accuracy_stability = np.std(recent_accuracies) avg_episode_length = np.mean(episode_lengths) Model Evaluation Evaluation Pipeline The evaluation script ( rl_ids/modeling/evaluate.py ) provides comprehensive model assessment: Performance Metrics # Classification metrics accuracy = accuracy_score(all_true_labels, all_predictions) precision, recall, f1, _ = precision_recall_fscore_support( all_true_labels, all_predictions, average='weighted' ) # Confusion matrix cm = confusion_matrix(all_true_labels, all_predictions) # Classification report report = classification_report( all_true_labels, all_predictions, target_names=class_names, output_dict=True ) Confidence Analysis # High confidence predictions high_conf_mask = np.array(all_confidences) >= confidence_threshold high_conf_accuracy = accuracy_score( np.array(all_true_labels)[high_conf_mask], np.array(all_predictions)[high_conf_mask] ) # Confidence distribution by class confidence_by_class = {} for class_idx, class_name in enumerate(class_names): class_mask = np.array(all_true_labels) == class_idx if np.any(class_mask): confidence_by_class[class_name] = np.mean(np.array(all_confidences)[class_mask]) Prediction Timing # Prediction performance avg_prediction_time = np.mean(prediction_times) * 1000 # Convert to ms predictions_per_second = 1.0 / np.mean(prediction_times) Usage Examples Basic Agent Training from rl_ids.agents.dqn_agent import DQNAgent, DQNConfig # Create configuration config = DQNConfig( state_dim=78, action_dim=15, lr=1e-4, gamma=0.995, hidden_dims=[1024, 512, 256, 128] ) # Initialize agent agent = DQNAgent(config) # Training loop for episode in range(num_episodes): state, _ = env.reset() done = False while not done: action = agent.act(state, training=True) next_state, reward, done, _, info = env.step(action) agent.remember(state, action, reward, next_state, done) if len(agent.memory) >= warmup_steps: loss = agent.replay() state = next_state # Update target network periodically if episode % target_update_interval == 0: agent.update_target() Model Inference # Load trained model agent = DQNAgent(config) agent.load_model(\"models/dqn_model_best.pt\") agent.epsilon = 0.0 # Disable exploration # Make predictions features = extract_features(network_packet) action = agent.act(features, training=False) attack_type = class_names[action]","title":"RL Agents"},{"location":"modules/agents/#rl-agents","text":"This document covers the reinforcement learning agents used in RL-IDS, specifically the Deep Q-Network (DQN) implementation.","title":"RL Agents"},{"location":"modules/agents/#overview","text":"The RL-IDS system uses Deep Q-Network (DQN) agents for network intrusion detection. The implementation is located in rl_ids/agents/dqn_agent.py and provides a configurable, high-performance agent suitable for multi-class classification tasks.","title":"Overview"},{"location":"modules/agents/#dqn-agent-architecture","text":"","title":"DQN Agent Architecture"},{"location":"modules/agents/#core-components","text":"The DQN agent consists of several key components: Deep Q-Network Model - Neural network for Q-value estimation Target Network - Stabilized target for Q-learning updates Replay Buffer - Experience replay for stable learning Epsilon-Greedy Policy - Exploration vs exploitation strategy","title":"Core Components"},{"location":"modules/agents/#network-architecture","text":"class DQN(nn.Module): \"\"\"Deep Q-Network model.\"\"\" def __init__(self, input_dim: int, output_dim: int, hidden_dims: List[int] = [256, 128]): super(DQN, self).__init__() layers = [] prev_dim = input_dim for hidden_dim in hidden_dims: layers.extend([nn.Linear(prev_dim, hidden_dim), nn.ReLU()]) prev_dim = hidden_dim layers.append(nn.Linear(prev_dim, output_dim)) self.fc = nn.Sequential(*layers) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\"Forward pass through the network.\"\"\" return self.fc(x) Default Architecture: - Input Layer: 78 features (CICIDS2017 feature set) - Hidden Layers: Configurable (default: [1024, 512, 256, 128]) - Output Layer: 15 classes (attack types + benign) - Activation: ReLU between layers - Output: Raw Q-values (no activation)","title":"Network Architecture"},{"location":"modules/agents/#dqn-configuration","text":"","title":"DQN Configuration"},{"location":"modules/agents/#configuration-class","text":"class DQNConfig(BaseModel): \"\"\"Configuration for DQN Agent.\"\"\" state_dim: int = Field(..., description=\"State space dimension\") action_dim: int = Field(..., description=\"Action space dimension\") lr: float = Field(1e-4, description=\"Learning rate\") gamma: float = Field(0.99, description=\"Discount factor\") epsilon: float = Field(1.0, description=\"Initial exploration rate\") eps_decay: float = Field(0.995, description=\"Epsilon decay rate\") eps_min: float = Field(0.1, description=\"Minimum epsilon value\") memory_size: int = Field(3000000, description=\"Replay buffer size\") batch_size: int = Field(64, description=\"Training batch size\") hidden_dims: List[int] = Field([256, 128], description=\"Hidden layer dimensions\")","title":"Configuration Class"},{"location":"modules/agents/#training-configuration","text":"The training script ( rl_ids/modeling/train.py ) provides extensive configuration options: # Core training parameters num_episodes: int = 500 # Number of training episodes target_update_interval: int = 10 # Target network update frequency lr: float = 1e-4 # Learning rate gamma: float = 0.995 # Discount factor # Exploration parameters epsilon: float = 1.0 # Initial exploration rate eps_decay: float = 0.9995 # Epsilon decay rate eps_min: float = 0.01 # Minimum epsilon value # Network architecture hidden_dims: str = \"1024,512,256,128\" # Hidden layer dimensions dropout_rate: float = 0.2 # Dropout rate for regularization use_layer_norm: bool = True # Use layer normalization # Training optimization memory_size: int = 100000 # Replay buffer size batch_size: int = 256 # Training batch size warmup_steps: int = 1000 # Warmup steps before training # Advanced features curriculum_learning: bool = True # Use curriculum learning double_dqn: bool = True # Use Double DQN dueling: bool = True # Use Dueling DQN architecture prioritized_replay: bool = False # Use prioritized experience replay","title":"Training Configuration"},{"location":"modules/agents/#dqn-agent-implementation","text":"","title":"DQN Agent Implementation"},{"location":"modules/agents/#agent-class","text":"class DQNAgent: \"\"\"Deep Q-Network Agent for reinforcement learning.\"\"\" def __init__(self, config: DQNConfig): \"\"\"Initialize DQN Agent with configuration.\"\"\" self.config = config self.state_dim = config.state_dim self.action_dim = config.action_dim self.gamma = config.gamma self.epsilon = config.epsilon self.eps_decay = config.eps_decay self.eps_min = config.eps_min self.batch_size = config.batch_size # Device selection self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Initialize networks self.model = DQN(config.state_dim, config.action_dim, config.hidden_dims).to(self.device) self.target_model = DQN(config.state_dim, config.action_dim, config.hidden_dims).to(self.device) self.update_target() # Training components self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr) self.memory = deque(maxlen=config.memory_size) # Training metrics self.training_step = 0 self.episode_count = 0","title":"Agent Class"},{"location":"modules/agents/#key-methods","text":"","title":"Key Methods"},{"location":"modules/agents/#action-selection","text":"def act(self, state: np.ndarray, training: bool = True) -> int: \"\"\"Choose action using epsilon-greedy policy.\"\"\" if training and random.random() < self.epsilon: action = random.randint(0, self.action_dim - 1) return action state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device) with torch.no_grad(): q_values = self.model(state_tensor) action = q_values.argmax().item() return action","title":"Action Selection"},{"location":"modules/agents/#experience-storage","text":"def remember(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool) -> None: \"\"\"Store experience in replay buffer.\"\"\" self.memory.append((state, action, reward, next_state, done))","title":"Experience Storage"},{"location":"modules/agents/#training-step","text":"def replay(self) -> Optional[float]: \"\"\"Train the model on a batch of experiences.\"\"\" if len(self.memory) < self.batch_size: return None batch = random.sample(self.memory, self.batch_size) states, actions, rewards, next_states, dones = zip(*batch) # Convert to tensors states = torch.FloatTensor(np.array(states)).to(self.device) actions = torch.LongTensor(actions).unsqueeze(1).to(self.device) rewards = torch.FloatTensor(rewards).to(self.device) next_states = torch.FloatTensor(np.array(next_states)).to(self.device) dones = torch.BoolTensor(dones).to(self.device) # Compute Q-values curr_Q = self.model(states).gather(1, actions).squeeze() next_Q = self.target_model(next_states).max(1)[0] target_Q = rewards + self.gamma * next_Q * (~dones) # Compute loss and update loss = self.criterion(curr_Q, target_Q.detach()) self.optimizer.zero_grad() loss.backward() self.optimizer.step() # Update epsilon if self.epsilon > self.eps_min: self.epsilon *= self.eps_decay self.training_step += 1 return loss.item()","title":"Training Step"},{"location":"modules/agents/#training-process","text":"","title":"Training Process"},{"location":"modules/agents/#enhanced-training-pipeline","text":"The training process ( rl_ids/modeling/train.py ) includes several advanced features:","title":"Enhanced Training Pipeline"},{"location":"modules/agents/#1-curriculum-learning","text":"Training progresses through stages of increasing difficulty: if curriculum_learning: stage_size = num_episodes // curriculum_stages for i in range(curriculum_stages): start_ep = i * stage_size end_ep = min((i + 1) * stage_size, num_episodes) curriculum_episodes.append((start_ep, end_ep, i + 1))","title":"1. Curriculum Learning"},{"location":"modules/agents/#2-learning-rate-scheduling","text":"Adaptive learning rate adjustment: if lr_scheduler == \"cosine\": scheduler = torch.optim.lr_scheduler.CosineAnnealingLR( agent.optimizer, T_max=num_episodes, eta_min=lr * 0.01 ) elif lr_scheduler == \"step\": scheduler = torch.optim.lr_scheduler.StepLR( agent.optimizer, step_size=num_episodes // 4, gamma=0.5 )","title":"2. Learning Rate Scheduling"},{"location":"modules/agents/#3-early-stopping","text":"Prevents overfitting with validation-based early stopping: if val_accuracy > best_val_accuracy: best_val_accuracy = val_accuracy patience_counter = 0 agent.save_model(best_model_path) else: patience_counter += 1 if patience_counter >= early_stopping_patience: logger.info(f\"Early stopping triggered at episode {episode}\") break","title":"3. Early Stopping"},{"location":"modules/agents/#4-gradient-clipping","text":"Stabilizes training with gradient clipping: if grad_clip > 0: torch.nn.utils.clip_grad_norm_(agent.model.parameters(), grad_clip)","title":"4. Gradient Clipping"},{"location":"modules/agents/#training-metrics","text":"The system tracks comprehensive training metrics: # Core metrics train_rewards = [] # Episode rewards train_losses = [] # Training losses train_accuracies = [] # Episode accuracies val_accuracies = [] # Validation accuracies learning_rates = [] # Learning rate progression episode_lengths = [] # Steps per episode # Advanced metrics reward_stability = np.std(recent_rewards) / (np.mean(recent_rewards) + 1e-8) accuracy_stability = np.std(recent_accuracies) avg_episode_length = np.mean(episode_lengths)","title":"Training Metrics"},{"location":"modules/agents/#model-evaluation","text":"","title":"Model Evaluation"},{"location":"modules/agents/#evaluation-pipeline","text":"The evaluation script ( rl_ids/modeling/evaluate.py ) provides comprehensive model assessment:","title":"Evaluation Pipeline"},{"location":"modules/agents/#performance-metrics","text":"# Classification metrics accuracy = accuracy_score(all_true_labels, all_predictions) precision, recall, f1, _ = precision_recall_fscore_support( all_true_labels, all_predictions, average='weighted' ) # Confusion matrix cm = confusion_matrix(all_true_labels, all_predictions) # Classification report report = classification_report( all_true_labels, all_predictions, target_names=class_names, output_dict=True )","title":"Performance Metrics"},{"location":"modules/agents/#confidence-analysis","text":"# High confidence predictions high_conf_mask = np.array(all_confidences) >= confidence_threshold high_conf_accuracy = accuracy_score( np.array(all_true_labels)[high_conf_mask], np.array(all_predictions)[high_conf_mask] ) # Confidence distribution by class confidence_by_class = {} for class_idx, class_name in enumerate(class_names): class_mask = np.array(all_true_labels) == class_idx if np.any(class_mask): confidence_by_class[class_name] = np.mean(np.array(all_confidences)[class_mask])","title":"Confidence Analysis"},{"location":"modules/agents/#prediction-timing","text":"# Prediction performance avg_prediction_time = np.mean(prediction_times) * 1000 # Convert to ms predictions_per_second = 1.0 / np.mean(prediction_times)","title":"Prediction Timing"},{"location":"modules/agents/#usage-examples","text":"","title":"Usage Examples"},{"location":"modules/agents/#basic-agent-training","text":"from rl_ids.agents.dqn_agent import DQNAgent, DQNConfig # Create configuration config = DQNConfig( state_dim=78, action_dim=15, lr=1e-4, gamma=0.995, hidden_dims=[1024, 512, 256, 128] ) # Initialize agent agent = DQNAgent(config) # Training loop for episode in range(num_episodes): state, _ = env.reset() done = False while not done: action = agent.act(state, training=True) next_state, reward, done, _, info = env.step(action) agent.remember(state, action, reward, next_state, done) if len(agent.memory) >= warmup_steps: loss = agent.replay() state = next_state # Update target network periodically if episode % target_update_interval == 0: agent.update_target()","title":"Basic Agent Training"},{"location":"modules/agents/#model-inference","text":"# Load trained model agent = DQNAgent(config) agent.load_model(\"models/dqn_model_best.pt\") agent.epsilon = 0.0 # Disable exploration # Make predictions features = extract_features(network_packet) action = agent.act(features, training=False) attack_type = class_names[action]","title":"Model Inference"},{"location":"modules/features/","text":"Feature Extraction This document covers the CICIDS2017 feature extraction system used in RL-IDS for converting network packets into standardized feature vectors. Overview The feature extraction system ( network_monitor.py - CICIDSFeatureExtractor class) converts raw network packets into 78 standardized features compatible with the CICIDS2017 dataset. This enables the DQN models to analyze real-time network traffic using the same feature space they were trained on. CICIDS2017 Feature Set The system extracts 78 features organized into several categories: Flow Duration Features flow_duration - Duration of the network flow in seconds Packet Count Features total_fwd_packets - Total packets in forward direction total_bwd_packets - Total packets in backward direction Byte Count Features total_length_fwd_packets - Total bytes in forward direction total_length_bwd_packets - Total bytes in backward direction Packet Length Statistics Forward direction: - fwd_packet_length_max - Maximum packet length - fwd_packet_length_min - Minimum packet length - fwd_packet_length_mean - Mean packet length - fwd_packet_length_std - Standard deviation of packet lengths Backward direction: - bwd_packet_length_max - Maximum packet length - bwd_packet_length_min - Minimum packet length - bwd_packet_length_mean - Mean packet length - bwd_packet_length_std - Standard deviation of packet lengths Flow Rate Features flow_bytes_per_sec - Bytes per second in the flow flow_packets_per_sec - Packets per second in the flow fwd_packets_per_sec - Forward packets per second bwd_packets_per_sec - Backward packets per second Inter-Arrival Time Features flow_iat_mean - Mean inter-arrival time flow_iat_std - Standard deviation of inter-arrival times flow_iat_max - Maximum inter-arrival time flow_iat_min - Minimum inter-arrival time TCP Flag Features fin_flag_count - Count of FIN flags syn_flag_count - Count of SYN flags rst_flag_count - Count of RST flags psh_flag_count - Count of PSH flags ack_flag_count - Count of ACK flags urg_flag_count - Count of URG flags Additional Statistical Features min_packet_length - Minimum packet length across all packets max_packet_length - Maximum packet length across all packets packet_length_mean - Mean packet length packet_length_std - Standard deviation of packet lengths packet_length_variance - Variance of packet lengths CICIDSFeatureExtractor Implementation Class Structure class CICIDSFeatureExtractor: \"\"\"Extract CICIDS2017-compatible features from network packets\"\"\" def __init__(self): self.feature_names = [ 'flow_duration', 'total_fwd_packets', 'total_bwd_packets', 'total_length_fwd_packets', 'total_length_bwd_packets', 'fwd_packet_length_max', 'fwd_packet_length_min', 'fwd_packet_length_mean', 'fwd_packet_length_std', 'bwd_packet_length_max', 'bwd_packet_length_min', 'bwd_packet_length_mean', 'bwd_packet_length_std', 'flow_bytes_per_sec', 'flow_packets_per_sec', 'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_length', 'bwd_header_length', 'fwd_packets_per_sec', 'bwd_packets_per_sec', 'min_packet_length', 'max_packet_length', 'packet_length_mean', 'packet_length_std', 'packet_length_variance', 'fin_flag_count', 'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count', 'urg_flag_count', 'cwe_flag_count', 'ece_flag_count', 'down_up_ratio', 'average_packet_size', 'avg_fwd_segment_size', 'avg_bwd_segment_size', 'fwd_header_length_2', 'fwd_avg_bytes_per_bulk', 'fwd_avg_packets_per_bulk', 'fwd_avg_bulk_rate', 'bwd_avg_bytes_per_bulk', 'bwd_avg_packets_per_bulk', 'bwd_avg_bulk_rate', 'subflow_fwd_packets', 'subflow_fwd_bytes', 'subflow_bwd_packets', 'subflow_bwd_bytes', 'init_win_bytes_forward', 'init_win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean', 'idle_std', 'idle_max', 'idle_min' ] Feature Extraction Process def extract_features(self, packet, flow_data): \"\"\"Extract 78 CICIDS2017 features from packet and flow data\"\"\" try: features = [0.0] * 78 if IP not in packet: return features current_time = datetime.now() packets = flow_data.get('packets', []) if not packets: return features # Basic flow statistics flow_start = flow_data.get('start_time', current_time) flow_duration = (current_time - flow_start).total_seconds() # Packet counts fwd_packets = flow_data.get('forward_packets', 0) bwd_packets = flow_data.get('backward_packets', 0) total_packets = fwd_packets + bwd_packets # Byte counts fwd_bytes = flow_data.get('forward_bytes', 0) bwd_bytes = flow_data.get('backward_bytes', 0) total_bytes = fwd_bytes + bwd_bytes # Extract packet sizes by direction fwd_sizes = [p['size'] for p in packets if p.get('direction') == 'forward'] bwd_sizes = [p['size'] for p in packets if p.get('direction') == 'backward'] all_sizes = [p['size'] for p in packets] # Feature extraction features[0] = flow_duration features[1] = float(fwd_packets) features[2] = float(bwd_packets) features[3] = float(fwd_bytes) features[4] = float(bwd_bytes) # Forward packet statistics if fwd_sizes: features[5] = float(max(fwd_sizes)) features[6] = float(min(fwd_sizes)) features[7] = float(np.mean(fwd_sizes)) features[8] = float(np.std(fwd_sizes)) # Backward packet statistics if bwd_sizes: features[9] = float(max(bwd_sizes)) features[10] = float(min(bwd_sizes)) features[11] = float(np.mean(bwd_sizes)) features[12] = float(np.std(bwd_sizes)) # Flow rates if flow_duration > 0: features[13] = float(total_bytes / flow_duration) features[14] = float(total_packets / flow_duration) features[35] = float(fwd_packets / flow_duration) features[36] = float(bwd_packets / flow_duration) # Inter-arrival times if len(packets) > 1: iats = [] for i in range(1, len(packets)): iat = (packets[i]['timestamp'] - packets[i-1]['timestamp']).total_seconds() iats.append(iat) if iats: features[15] = float(np.mean(iats)) features[16] = float(np.std(iats)) features[17] = float(max(iats)) features[18] = float(min(iats)) # Packet length statistics if all_sizes: features[37] = float(min(all_sizes)) features[38] = float(max(all_sizes)) features[39] = float(np.mean(all_sizes)) features[40] = float(np.std(all_sizes)) features[41] = float(np.var(all_sizes)) features[49] = float(np.mean(all_sizes)) # TCP flags analysis if TCP in packet: tcp_flags = flow_data.get('tcp_flags', []) if tcp_flags: features[42] = float(sum(1 for f in tcp_flags if int(f) & 0x01)) # fin features[43] = float(sum(1 for f in tcp_flags if int(f) & 0x02)) # syn features[44] = float(sum(1 for f in tcp_flags if int(f) & 0x04)) # rst features[45] = float(sum(1 for f in tcp_flags if int(f) & 0x08)) # psh features[46] = float(sum(1 for f in tcp_flags if int(f) & 0x10)) # ack features[47] = float(sum(1 for f in tcp_flags if int(f) & 0x20)) # urg # Additional features features[50] = float(np.mean(fwd_sizes) if fwd_sizes else 0) features[51] = float(np.mean(bwd_sizes) if bwd_sizes else 0) # Normalize and clean values features = [min(max(f, -1e6), 1e6) for f in features] features = [0.0 if np.isnan(f) or np.isinf(f) else f for f in features] return features except Exception as e: return [0.0] * 78 Flow Tracking Flow Identification Network flows are identified using a 5-tuple: def get_flow_key(self, packet): \"\"\"Generate unique flow key from packet\"\"\" if IP not in packet: return None src_ip = packet[IP].src dst_ip = packet[IP].dst protocol = packet[IP].proto src_port = 0 dst_port = 0 if TCP in packet: src_port = packet[TCP].sport dst_port = packet[TCP].dport protocol_name = \"TCP\" elif UDP in packet: src_port = packet[UDP].sport dst_port = packet[UDP].dport protocol_name = \"UDP\" else: protocol_name = \"OTHER\" # Create bidirectional flow key (normalize direction) if (src_ip, src_port) < (dst_ip, dst_port): flow_key = f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{protocol_name}\" else: flow_key = f\"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{protocol_name}\" return flow_key Flow State Management class FlowTracker: def __init__(self): self.flows = {} self.flow_timeout = 120 # seconds def update_flow(self, packet, flow_key): \"\"\"Update flow statistics with new packet\"\"\" current_time = datetime.now() if flow_key not in self.flows: self.flows[flow_key] = { 'start_time': current_time, 'last_seen': current_time, 'packets': [], 'forward_packets': 0, 'backward_packets': 0, 'forward_bytes': 0, 'backward_bytes': 0, 'tcp_flags': [] } flow = self.flows[flow_key] flow['last_seen'] = current_time # Determine packet direction direction = self.get_packet_direction(packet, flow_key) # Update packet statistics packet_size = len(packet) packet_info = { 'timestamp': current_time, 'size': packet_size, 'direction': direction } flow['packets'].append(packet_info) # Update counters if direction == 'forward': flow['forward_packets'] += 1 flow['forward_bytes'] += packet_size else: flow['backward_packets'] += 1 flow['backward_bytes'] += packet_size # Extract TCP flags if present if TCP in packet: flow['tcp_flags'].append(packet[TCP].flags) return flow def cleanup_expired_flows(self): \"\"\"Remove expired flows to prevent memory leaks\"\"\" current_time = datetime.now() expired_flows = [] for flow_key, flow in self.flows.items(): if (current_time - flow['last_seen']).total_seconds() > self.flow_timeout: expired_flows.append(flow_key) for flow_key in expired_flows: del self.flows[flow_key] return len(expired_flows) Data Preprocessing CICIDS2017 Dataset Processing The rl_ids/make_dataset.py module handles preprocessing of the original CICIDS2017 CSV files: class DataGenerator: \"\"\"Handles loading and initial preprocessing of raw CSV data files\"\"\" def __init__(self): self.label_encoder = None self.processed_data = None def load_and_preprocess_data(self, data_dir: Path = RAW_DATA_DIR) -> pd.DataFrame: \"\"\"Load and preprocess CSV data files from the specified directory.\"\"\" # Find CSV files csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")] # Load CSV files with progress tracking data_frames = [] for csv_file in tqdm(csv_files, desc=\"Loading CSV files\"): file_path = data_dir / csv_file try: df = pd.read_csv(file_path) data_frames.append(df) except Exception as e: logger.error(f\"Failed to load {csv_file}: {e}\") # Combine all dataframes combined_df = pd.concat(data_frames, ignore_index=True) return combined_df Feature Normalization def normalize_features(self, df: pd.DataFrame, scaler_type: str = \"standard\") -> pd.DataFrame: \"\"\"Normalize feature columns using specified scaler\"\"\" feature_columns = [col for col in df.columns if col not in [\"Label\", \"Label_Original\"]] if scaler_type == \"standard\": scaler = StandardScaler() elif scaler_type == \"minmax\": scaler = MinMaxScaler() elif scaler_type == \"robust\": scaler = RobustScaler() else: raise ValueError(f\"Unknown scaler type: {scaler_type}\") # Fit and transform features df_normalized = df.copy() df_normalized[feature_columns] = scaler.fit_transform(df[feature_columns]) return df_normalized, scaler Label Encoding def encode_labels(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\"Encode string labels to numeric values\"\"\" # Store original labels df['Label_Original'] = df['Label'].copy() # Encode labels to numeric values self.label_encoder = LabelEncoder() df['Label'] = self.label_encoder.fit_transform(df['Label']) # Log label mapping label_mapping = dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_))) logger.info(f\"Label mapping: {label_mapping}\") return df Real-time Feature Pipeline Integration with Network Monitor The feature extraction integrates seamlessly with the network monitoring system: class RealTimeNetworkMonitor: def __init__(self, interface=\"wlan0\", api_url=\"http://localhost:8000\"): self.interface = interface self.api_url = api_url self.feature_extractor = CICIDSFeatureExtractor() self.flow_tracker = FlowTracker() def process_packet(self, packet): \"\"\"Process captured packet for threat detection\"\"\" try: # Get flow key flow_key = self.get_flow_key(packet) if not flow_key: return # Update flow tracking flow_data = self.flow_tracker.update_flow(packet, flow_key) # Extract features features = self.feature_extractor.extract_features(packet, flow_data) # Send to API for analysis asyncio.create_task(self.analyze_features(features, flow_key)) except Exception as e: logger.error(f\"Error processing packet: {e}\") Performance Optimizations # Efficient feature caching class FeatureCache: def __init__(self, max_size=1000): self.cache = {} self.max_size = max_size self.access_times = {} def get_features(self, flow_key, flow_data): \"\"\"Get cached features or compute new ones\"\"\" cache_key = self.compute_cache_key(flow_data) if cache_key in self.cache: self.access_times[cache_key] = time.time() return self.cache[cache_key] # Compute new features features = self.feature_extractor.extract_features(None, flow_data) # Cache with LRU eviction if len(self.cache) >= self.max_size: self.evict_lru() self.cache[cache_key] = features self.access_times[cache_key] = time.time() return features Feature Quality and Validation Feature Validation def validate_features(self, features): \"\"\"Validate extracted features for quality and consistency\"\"\" # Check feature count if len(features) != 78: raise ValueError(f\"Expected 78 features, got {len(features)}\") # Check for invalid values for i, feature in enumerate(features): if np.isnan(feature) or np.isinf(feature): logger.warning(f\"Invalid value in feature {i}: {feature}\") features[i] = 0.0 # Check reasonable ranges if abs(feature) > 1e6: logger.warning(f\"Extreme value in feature {i}: {feature}\") features[i] = np.clip(feature, -1e6, 1e6) return features Feature Statistics def compute_feature_statistics(self, feature_batches): \"\"\"Compute statistics for extracted features\"\"\" features_array = np.array(feature_batches) stats = { 'mean': np.mean(features_array, axis=0), 'std': np.std(features_array, axis=0), 'min': np.min(features_array, axis=0), 'max': np.max(features_array, axis=0), 'zero_ratio': np.mean(features_array == 0, axis=0) } return stats","title":"Feature Extraction"},{"location":"modules/features/#feature-extraction","text":"This document covers the CICIDS2017 feature extraction system used in RL-IDS for converting network packets into standardized feature vectors.","title":"Feature Extraction"},{"location":"modules/features/#overview","text":"The feature extraction system ( network_monitor.py - CICIDSFeatureExtractor class) converts raw network packets into 78 standardized features compatible with the CICIDS2017 dataset. This enables the DQN models to analyze real-time network traffic using the same feature space they were trained on.","title":"Overview"},{"location":"modules/features/#cicids2017-feature-set","text":"The system extracts 78 features organized into several categories:","title":"CICIDS2017 Feature Set"},{"location":"modules/features/#flow-duration-features","text":"flow_duration - Duration of the network flow in seconds","title":"Flow Duration Features"},{"location":"modules/features/#packet-count-features","text":"total_fwd_packets - Total packets in forward direction total_bwd_packets - Total packets in backward direction","title":"Packet Count Features"},{"location":"modules/features/#byte-count-features","text":"total_length_fwd_packets - Total bytes in forward direction total_length_bwd_packets - Total bytes in backward direction","title":"Byte Count Features"},{"location":"modules/features/#packet-length-statistics","text":"Forward direction: - fwd_packet_length_max - Maximum packet length - fwd_packet_length_min - Minimum packet length - fwd_packet_length_mean - Mean packet length - fwd_packet_length_std - Standard deviation of packet lengths Backward direction: - bwd_packet_length_max - Maximum packet length - bwd_packet_length_min - Minimum packet length - bwd_packet_length_mean - Mean packet length - bwd_packet_length_std - Standard deviation of packet lengths","title":"Packet Length Statistics"},{"location":"modules/features/#flow-rate-features","text":"flow_bytes_per_sec - Bytes per second in the flow flow_packets_per_sec - Packets per second in the flow fwd_packets_per_sec - Forward packets per second bwd_packets_per_sec - Backward packets per second","title":"Flow Rate Features"},{"location":"modules/features/#inter-arrival-time-features","text":"flow_iat_mean - Mean inter-arrival time flow_iat_std - Standard deviation of inter-arrival times flow_iat_max - Maximum inter-arrival time flow_iat_min - Minimum inter-arrival time","title":"Inter-Arrival Time Features"},{"location":"modules/features/#tcp-flag-features","text":"fin_flag_count - Count of FIN flags syn_flag_count - Count of SYN flags rst_flag_count - Count of RST flags psh_flag_count - Count of PSH flags ack_flag_count - Count of ACK flags urg_flag_count - Count of URG flags","title":"TCP Flag Features"},{"location":"modules/features/#additional-statistical-features","text":"min_packet_length - Minimum packet length across all packets max_packet_length - Maximum packet length across all packets packet_length_mean - Mean packet length packet_length_std - Standard deviation of packet lengths packet_length_variance - Variance of packet lengths","title":"Additional Statistical Features"},{"location":"modules/features/#cicidsfeatureextractor-implementation","text":"","title":"CICIDSFeatureExtractor Implementation"},{"location":"modules/features/#class-structure","text":"class CICIDSFeatureExtractor: \"\"\"Extract CICIDS2017-compatible features from network packets\"\"\" def __init__(self): self.feature_names = [ 'flow_duration', 'total_fwd_packets', 'total_bwd_packets', 'total_length_fwd_packets', 'total_length_bwd_packets', 'fwd_packet_length_max', 'fwd_packet_length_min', 'fwd_packet_length_mean', 'fwd_packet_length_std', 'bwd_packet_length_max', 'bwd_packet_length_min', 'bwd_packet_length_mean', 'bwd_packet_length_std', 'flow_bytes_per_sec', 'flow_packets_per_sec', 'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min', 'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max', 'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std', 'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_length', 'bwd_header_length', 'fwd_packets_per_sec', 'bwd_packets_per_sec', 'min_packet_length', 'max_packet_length', 'packet_length_mean', 'packet_length_std', 'packet_length_variance', 'fin_flag_count', 'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count', 'urg_flag_count', 'cwe_flag_count', 'ece_flag_count', 'down_up_ratio', 'average_packet_size', 'avg_fwd_segment_size', 'avg_bwd_segment_size', 'fwd_header_length_2', 'fwd_avg_bytes_per_bulk', 'fwd_avg_packets_per_bulk', 'fwd_avg_bulk_rate', 'bwd_avg_bytes_per_bulk', 'bwd_avg_packets_per_bulk', 'bwd_avg_bulk_rate', 'subflow_fwd_packets', 'subflow_fwd_bytes', 'subflow_bwd_packets', 'subflow_bwd_bytes', 'init_win_bytes_forward', 'init_win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward', 'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean', 'idle_std', 'idle_max', 'idle_min' ]","title":"Class Structure"},{"location":"modules/features/#feature-extraction-process","text":"def extract_features(self, packet, flow_data): \"\"\"Extract 78 CICIDS2017 features from packet and flow data\"\"\" try: features = [0.0] * 78 if IP not in packet: return features current_time = datetime.now() packets = flow_data.get('packets', []) if not packets: return features # Basic flow statistics flow_start = flow_data.get('start_time', current_time) flow_duration = (current_time - flow_start).total_seconds() # Packet counts fwd_packets = flow_data.get('forward_packets', 0) bwd_packets = flow_data.get('backward_packets', 0) total_packets = fwd_packets + bwd_packets # Byte counts fwd_bytes = flow_data.get('forward_bytes', 0) bwd_bytes = flow_data.get('backward_bytes', 0) total_bytes = fwd_bytes + bwd_bytes # Extract packet sizes by direction fwd_sizes = [p['size'] for p in packets if p.get('direction') == 'forward'] bwd_sizes = [p['size'] for p in packets if p.get('direction') == 'backward'] all_sizes = [p['size'] for p in packets] # Feature extraction features[0] = flow_duration features[1] = float(fwd_packets) features[2] = float(bwd_packets) features[3] = float(fwd_bytes) features[4] = float(bwd_bytes) # Forward packet statistics if fwd_sizes: features[5] = float(max(fwd_sizes)) features[6] = float(min(fwd_sizes)) features[7] = float(np.mean(fwd_sizes)) features[8] = float(np.std(fwd_sizes)) # Backward packet statistics if bwd_sizes: features[9] = float(max(bwd_sizes)) features[10] = float(min(bwd_sizes)) features[11] = float(np.mean(bwd_sizes)) features[12] = float(np.std(bwd_sizes)) # Flow rates if flow_duration > 0: features[13] = float(total_bytes / flow_duration) features[14] = float(total_packets / flow_duration) features[35] = float(fwd_packets / flow_duration) features[36] = float(bwd_packets / flow_duration) # Inter-arrival times if len(packets) > 1: iats = [] for i in range(1, len(packets)): iat = (packets[i]['timestamp'] - packets[i-1]['timestamp']).total_seconds() iats.append(iat) if iats: features[15] = float(np.mean(iats)) features[16] = float(np.std(iats)) features[17] = float(max(iats)) features[18] = float(min(iats)) # Packet length statistics if all_sizes: features[37] = float(min(all_sizes)) features[38] = float(max(all_sizes)) features[39] = float(np.mean(all_sizes)) features[40] = float(np.std(all_sizes)) features[41] = float(np.var(all_sizes)) features[49] = float(np.mean(all_sizes)) # TCP flags analysis if TCP in packet: tcp_flags = flow_data.get('tcp_flags', []) if tcp_flags: features[42] = float(sum(1 for f in tcp_flags if int(f) & 0x01)) # fin features[43] = float(sum(1 for f in tcp_flags if int(f) & 0x02)) # syn features[44] = float(sum(1 for f in tcp_flags if int(f) & 0x04)) # rst features[45] = float(sum(1 for f in tcp_flags if int(f) & 0x08)) # psh features[46] = float(sum(1 for f in tcp_flags if int(f) & 0x10)) # ack features[47] = float(sum(1 for f in tcp_flags if int(f) & 0x20)) # urg # Additional features features[50] = float(np.mean(fwd_sizes) if fwd_sizes else 0) features[51] = float(np.mean(bwd_sizes) if bwd_sizes else 0) # Normalize and clean values features = [min(max(f, -1e6), 1e6) for f in features] features = [0.0 if np.isnan(f) or np.isinf(f) else f for f in features] return features except Exception as e: return [0.0] * 78","title":"Feature Extraction Process"},{"location":"modules/features/#flow-tracking","text":"","title":"Flow Tracking"},{"location":"modules/features/#flow-identification","text":"Network flows are identified using a 5-tuple: def get_flow_key(self, packet): \"\"\"Generate unique flow key from packet\"\"\" if IP not in packet: return None src_ip = packet[IP].src dst_ip = packet[IP].dst protocol = packet[IP].proto src_port = 0 dst_port = 0 if TCP in packet: src_port = packet[TCP].sport dst_port = packet[TCP].dport protocol_name = \"TCP\" elif UDP in packet: src_port = packet[UDP].sport dst_port = packet[UDP].dport protocol_name = \"UDP\" else: protocol_name = \"OTHER\" # Create bidirectional flow key (normalize direction) if (src_ip, src_port) < (dst_ip, dst_port): flow_key = f\"{src_ip}:{src_port}-{dst_ip}:{dst_port}-{protocol_name}\" else: flow_key = f\"{dst_ip}:{dst_port}-{src_ip}:{src_port}-{protocol_name}\" return flow_key","title":"Flow Identification"},{"location":"modules/features/#flow-state-management","text":"class FlowTracker: def __init__(self): self.flows = {} self.flow_timeout = 120 # seconds def update_flow(self, packet, flow_key): \"\"\"Update flow statistics with new packet\"\"\" current_time = datetime.now() if flow_key not in self.flows: self.flows[flow_key] = { 'start_time': current_time, 'last_seen': current_time, 'packets': [], 'forward_packets': 0, 'backward_packets': 0, 'forward_bytes': 0, 'backward_bytes': 0, 'tcp_flags': [] } flow = self.flows[flow_key] flow['last_seen'] = current_time # Determine packet direction direction = self.get_packet_direction(packet, flow_key) # Update packet statistics packet_size = len(packet) packet_info = { 'timestamp': current_time, 'size': packet_size, 'direction': direction } flow['packets'].append(packet_info) # Update counters if direction == 'forward': flow['forward_packets'] += 1 flow['forward_bytes'] += packet_size else: flow['backward_packets'] += 1 flow['backward_bytes'] += packet_size # Extract TCP flags if present if TCP in packet: flow['tcp_flags'].append(packet[TCP].flags) return flow def cleanup_expired_flows(self): \"\"\"Remove expired flows to prevent memory leaks\"\"\" current_time = datetime.now() expired_flows = [] for flow_key, flow in self.flows.items(): if (current_time - flow['last_seen']).total_seconds() > self.flow_timeout: expired_flows.append(flow_key) for flow_key in expired_flows: del self.flows[flow_key] return len(expired_flows)","title":"Flow State Management"},{"location":"modules/features/#data-preprocessing","text":"","title":"Data Preprocessing"},{"location":"modules/features/#cicids2017-dataset-processing","text":"The rl_ids/make_dataset.py module handles preprocessing of the original CICIDS2017 CSV files: class DataGenerator: \"\"\"Handles loading and initial preprocessing of raw CSV data files\"\"\" def __init__(self): self.label_encoder = None self.processed_data = None def load_and_preprocess_data(self, data_dir: Path = RAW_DATA_DIR) -> pd.DataFrame: \"\"\"Load and preprocess CSV data files from the specified directory.\"\"\" # Find CSV files csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")] # Load CSV files with progress tracking data_frames = [] for csv_file in tqdm(csv_files, desc=\"Loading CSV files\"): file_path = data_dir / csv_file try: df = pd.read_csv(file_path) data_frames.append(df) except Exception as e: logger.error(f\"Failed to load {csv_file}: {e}\") # Combine all dataframes combined_df = pd.concat(data_frames, ignore_index=True) return combined_df","title":"CICIDS2017 Dataset Processing"},{"location":"modules/features/#feature-normalization","text":"def normalize_features(self, df: pd.DataFrame, scaler_type: str = \"standard\") -> pd.DataFrame: \"\"\"Normalize feature columns using specified scaler\"\"\" feature_columns = [col for col in df.columns if col not in [\"Label\", \"Label_Original\"]] if scaler_type == \"standard\": scaler = StandardScaler() elif scaler_type == \"minmax\": scaler = MinMaxScaler() elif scaler_type == \"robust\": scaler = RobustScaler() else: raise ValueError(f\"Unknown scaler type: {scaler_type}\") # Fit and transform features df_normalized = df.copy() df_normalized[feature_columns] = scaler.fit_transform(df[feature_columns]) return df_normalized, scaler","title":"Feature Normalization"},{"location":"modules/features/#label-encoding","text":"def encode_labels(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\"Encode string labels to numeric values\"\"\" # Store original labels df['Label_Original'] = df['Label'].copy() # Encode labels to numeric values self.label_encoder = LabelEncoder() df['Label'] = self.label_encoder.fit_transform(df['Label']) # Log label mapping label_mapping = dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_))) logger.info(f\"Label mapping: {label_mapping}\") return df","title":"Label Encoding"},{"location":"modules/features/#real-time-feature-pipeline","text":"","title":"Real-time Feature Pipeline"},{"location":"modules/features/#integration-with-network-monitor","text":"The feature extraction integrates seamlessly with the network monitoring system: class RealTimeNetworkMonitor: def __init__(self, interface=\"wlan0\", api_url=\"http://localhost:8000\"): self.interface = interface self.api_url = api_url self.feature_extractor = CICIDSFeatureExtractor() self.flow_tracker = FlowTracker() def process_packet(self, packet): \"\"\"Process captured packet for threat detection\"\"\" try: # Get flow key flow_key = self.get_flow_key(packet) if not flow_key: return # Update flow tracking flow_data = self.flow_tracker.update_flow(packet, flow_key) # Extract features features = self.feature_extractor.extract_features(packet, flow_data) # Send to API for analysis asyncio.create_task(self.analyze_features(features, flow_key)) except Exception as e: logger.error(f\"Error processing packet: {e}\")","title":"Integration with Network Monitor"},{"location":"modules/features/#performance-optimizations","text":"# Efficient feature caching class FeatureCache: def __init__(self, max_size=1000): self.cache = {} self.max_size = max_size self.access_times = {} def get_features(self, flow_key, flow_data): \"\"\"Get cached features or compute new ones\"\"\" cache_key = self.compute_cache_key(flow_data) if cache_key in self.cache: self.access_times[cache_key] = time.time() return self.cache[cache_key] # Compute new features features = self.feature_extractor.extract_features(None, flow_data) # Cache with LRU eviction if len(self.cache) >= self.max_size: self.evict_lru() self.cache[cache_key] = features self.access_times[cache_key] = time.time() return features","title":"Performance Optimizations"},{"location":"modules/features/#feature-quality-and-validation","text":"","title":"Feature Quality and Validation"},{"location":"modules/features/#feature-validation","text":"def validate_features(self, features): \"\"\"Validate extracted features for quality and consistency\"\"\" # Check feature count if len(features) != 78: raise ValueError(f\"Expected 78 features, got {len(features)}\") # Check for invalid values for i, feature in enumerate(features): if np.isnan(feature) or np.isinf(feature): logger.warning(f\"Invalid value in feature {i}: {feature}\") features[i] = 0.0 # Check reasonable ranges if abs(feature) > 1e6: logger.warning(f\"Extreme value in feature {i}: {feature}\") features[i] = np.clip(feature, -1e6, 1e6) return features","title":"Feature Validation"},{"location":"modules/features/#feature-statistics","text":"def compute_feature_statistics(self, feature_batches): \"\"\"Compute statistics for extracted features\"\"\" features_array = np.array(feature_batches) stats = { 'mean': np.mean(features_array, axis=0), 'std': np.std(features_array, axis=0), 'min': np.min(features_array, axis=0), 'max': np.max(features_array, axis=0), 'zero_ratio': np.mean(features_array == 0, axis=0) } return stats","title":"Feature Statistics"},{"location":"user-guide/","text":"User Guide This guide covers the practical usage of RL-IDS for network monitoring and threat detection. Overview RL-IDS provides three main operational modes: Network Interface Monitoring - Monitor all traffic on a network interface Website-Specific Monitoring - Monitor traffic to/from specific domains API Integration - Use the REST API for custom integrations Network Interface Monitoring Monitor all network traffic on a specific interface: # Monitor default interface (auto-detected) sudo python network_monitor.py # Monitor specific interface sudo python network_monitor.py eth0 # Monitor with custom API endpoint sudo python network_monitor.py wlan0 --api-url http://localhost:8000 Available Interfaces To see available network interfaces: import psutil for interface, addrs in psutil.net_if_addrs().items(): print(f\"Interface: {interface}\") for addr in addrs: if addr.family.name == 'AF_INET': print(f\" IP: {addr.address}\") Website-Specific Monitoring Monitor traffic to/from specific websites: # Monitor a specific domain python website_monitor.py example.com # Monitor with custom settings python website_monitor.py example.com --api-url http://localhost:8000 --interface wlan0 This mode: - Resolves domain names to IP addresses - Generates test traffic to the target - Captures and analyzes responses - Detects potential attacks in the communication Configuration Options Environment Variables Create a .env file based on .env.example: cp .env.example .env Key configuration options: - API endpoint URLs - Model file paths - Logging levels - Network interface preferences .env.example: # RL-IDS API Environment Configuration # Copy this file to .env and modify as needed # API Settings RLIDS_APP_NAME=RL-IDS API RLIDS_APP_VERSION=1.2.0 RLIDS_DEBUG=false # Server Settings RLIDS_HOST=0.0.0.0 RLIDS_PORT=8000 RLIDS_WORKERS=1 # Model Settings RLIDS_MODEL_PATH=models/dqn_model_final.pt RLIDS_DATA_PATH=data/processed/cicids2017_normalised.csv # Performance Settings RLIDS_MAX_BATCH_SIZE=100 RLIDS_PREDICTION_TIMEOUT=30.0 # Logging Settings RLIDS_LOG_LEVEL=INFO RLIDS_LOG_FORMAT={time} | {level} | {message} # CORS Settings (for production, restrict these) RLIDS_CORS_ORIGINS=[\"*\"] RLIDS_CORS_METHODS=[\"*\"] RLIDS_CORS_HEADERS=[\"*\"] # Rate Limiting RLIDS_RATE_LIMIT_ENABLED=false RLIDS_RATE_LIMIT_REQUESTS=100 RLIDS_RATE_LIMIT_WINDOW=60 # Health Check Settings RLIDS_HEALTH_CHECK_TIMEOUT=5.0 Runtime Parameters Most scripts accept command-line arguments: # Network monitor options python network_monitor.py --help # Website monitor options python website_monitor.py --help # API server options python run_api.py --help Understanding Output Real-time Monitor Display The network monitor shows: \ud83d\udee1\ufe0f RL-IDS NETWORK MONITOR \ud83d\udee1\ufe0f ================================================================================ \ud83d\udcc8 STATISTICS: \ud83d\udcca Uptime: 0:05:23 \ud83d\udce6 Packets: 1,247 \u26a1 Rate: 13.2/min \ud83d\udd0d Active Flows: 8 \ud83d\udea8 Attacks: 2 \ud83d\udd07 Ignored: 1 \ud83d\udce4 Queue Size: 0 \ud83d\udd27 CONFIGURATION: \ud83d\udce1 Interface: wlan0 \ud83c\udf10 API URL: http://localhost:8000 \u2699\ufe0f Threshold: 70.0% \ud83d\udd27 Status: Monitoring Active \ud83d\udfe2 \ud83d\udea8 RECENT ALERTS: | Time | Attack Type | Source IP | Confidence | | -------- | ----------- | ------------- | ---------- | | 14:23:15 | DoS Hulk | 192.168.1.100 | 85% | | 14:22:08 | Port Scan | 10.0.0.15 | 92% | Log Files RL-IDS generates several log files in the logs/ directory: network_monitor.log - General monitoring logs intrusion_alerts.log - Detected attack details alerts.json - Machine-readable alert data website_monitor.log - Website monitoring logs ignored_attacks.json - Filtered/ignored attacks Troubleshooting Permission Issues Network monitoring requires elevated privileges: # Run with sudo sudo python network_monitor.py # Or configure capabilities (Linux) sudo setcap cap_net_raw,cap_net_admin=eip /usr/bin/python3 Interface Not Found List available interfaces: python -c \" import psutil interfaces = [iface for iface, addrs in psutil.net_if_addrs().items() if any(addr.family.name == 'AF_INET' for addr in addrs)] print('Available interfaces:', interfaces) \" API Connection Issues Check if the API server is running: # Test API health curl http://localhost:8000/health # Check API documentation curl http://localhost:8000/docs High Memory Usage For long-running monitoring: Monitor the packet queue size in the UI Adjust confidence_threshold to reduce false positives Add more attack types to ignored_attacks list Increase cleanup_interval for flow cleanup Advanced Usage Custom Feature Extraction The system uses CICIDS2017-compatible features. To add custom features: from network_monitor import CICIDSFeatureExtractor extractor = CICIDSFeatureExtractor() features = extractor.extract_features(packet, flow_data) Filtering Traffic Modify packet filters in network_monitor.py : # Example: Monitor only HTTP traffic packet_filter = \"tcp port 80 or tcp port 443\" # Example: Monitor specific subnet packet_filter = \"net 192.168.1.0/24\" Custom Attack Types Add or remove attack types from the ignore list: # In RealTimeNetworkMonitor.__init__() self.ignored_attacks = ['heartbleed', 'portscan', 'benign']","title":"Overview"},{"location":"user-guide/#user-guide","text":"This guide covers the practical usage of RL-IDS for network monitoring and threat detection.","title":"User Guide"},{"location":"user-guide/#overview","text":"RL-IDS provides three main operational modes: Network Interface Monitoring - Monitor all traffic on a network interface Website-Specific Monitoring - Monitor traffic to/from specific domains API Integration - Use the REST API for custom integrations","title":"Overview"},{"location":"user-guide/#network-interface-monitoring","text":"Monitor all network traffic on a specific interface: # Monitor default interface (auto-detected) sudo python network_monitor.py # Monitor specific interface sudo python network_monitor.py eth0 # Monitor with custom API endpoint sudo python network_monitor.py wlan0 --api-url http://localhost:8000","title":"Network Interface Monitoring"},{"location":"user-guide/#available-interfaces","text":"To see available network interfaces: import psutil for interface, addrs in psutil.net_if_addrs().items(): print(f\"Interface: {interface}\") for addr in addrs: if addr.family.name == 'AF_INET': print(f\" IP: {addr.address}\")","title":"Available Interfaces"},{"location":"user-guide/#website-specific-monitoring","text":"Monitor traffic to/from specific websites: # Monitor a specific domain python website_monitor.py example.com # Monitor with custom settings python website_monitor.py example.com --api-url http://localhost:8000 --interface wlan0 This mode: - Resolves domain names to IP addresses - Generates test traffic to the target - Captures and analyzes responses - Detects potential attacks in the communication","title":"Website-Specific Monitoring"},{"location":"user-guide/#configuration-options","text":"","title":"Configuration Options"},{"location":"user-guide/#environment-variables","text":"Create a .env file based on .env.example: cp .env.example .env Key configuration options: - API endpoint URLs - Model file paths - Logging levels - Network interface preferences .env.example: # RL-IDS API Environment Configuration # Copy this file to .env and modify as needed # API Settings RLIDS_APP_NAME=RL-IDS API RLIDS_APP_VERSION=1.2.0 RLIDS_DEBUG=false # Server Settings RLIDS_HOST=0.0.0.0 RLIDS_PORT=8000 RLIDS_WORKERS=1 # Model Settings RLIDS_MODEL_PATH=models/dqn_model_final.pt RLIDS_DATA_PATH=data/processed/cicids2017_normalised.csv # Performance Settings RLIDS_MAX_BATCH_SIZE=100 RLIDS_PREDICTION_TIMEOUT=30.0 # Logging Settings RLIDS_LOG_LEVEL=INFO RLIDS_LOG_FORMAT={time} | {level} | {message} # CORS Settings (for production, restrict these) RLIDS_CORS_ORIGINS=[\"*\"] RLIDS_CORS_METHODS=[\"*\"] RLIDS_CORS_HEADERS=[\"*\"] # Rate Limiting RLIDS_RATE_LIMIT_ENABLED=false RLIDS_RATE_LIMIT_REQUESTS=100 RLIDS_RATE_LIMIT_WINDOW=60 # Health Check Settings RLIDS_HEALTH_CHECK_TIMEOUT=5.0","title":"Environment Variables"},{"location":"user-guide/#runtime-parameters","text":"Most scripts accept command-line arguments: # Network monitor options python network_monitor.py --help # Website monitor options python website_monitor.py --help # API server options python run_api.py --help","title":"Runtime Parameters"},{"location":"user-guide/#understanding-output","text":"","title":"Understanding Output"},{"location":"user-guide/#real-time-monitor-display","text":"The network monitor shows: \ud83d\udee1\ufe0f RL-IDS NETWORK MONITOR \ud83d\udee1\ufe0f ================================================================================ \ud83d\udcc8 STATISTICS: \ud83d\udcca Uptime: 0:05:23 \ud83d\udce6 Packets: 1,247 \u26a1 Rate: 13.2/min \ud83d\udd0d Active Flows: 8 \ud83d\udea8 Attacks: 2 \ud83d\udd07 Ignored: 1 \ud83d\udce4 Queue Size: 0 \ud83d\udd27 CONFIGURATION: \ud83d\udce1 Interface: wlan0 \ud83c\udf10 API URL: http://localhost:8000 \u2699\ufe0f Threshold: 70.0% \ud83d\udd27 Status: Monitoring Active \ud83d\udfe2 \ud83d\udea8 RECENT ALERTS: | Time | Attack Type | Source IP | Confidence | | -------- | ----------- | ------------- | ---------- | | 14:23:15 | DoS Hulk | 192.168.1.100 | 85% | | 14:22:08 | Port Scan | 10.0.0.15 | 92% |","title":"Real-time Monitor Display"},{"location":"user-guide/#log-files","text":"RL-IDS generates several log files in the logs/ directory: network_monitor.log - General monitoring logs intrusion_alerts.log - Detected attack details alerts.json - Machine-readable alert data website_monitor.log - Website monitoring logs ignored_attacks.json - Filtered/ignored attacks","title":"Log Files"},{"location":"user-guide/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/#permission-issues","text":"Network monitoring requires elevated privileges: # Run with sudo sudo python network_monitor.py # Or configure capabilities (Linux) sudo setcap cap_net_raw,cap_net_admin=eip /usr/bin/python3","title":"Permission Issues"},{"location":"user-guide/#interface-not-found","text":"List available interfaces: python -c \" import psutil interfaces = [iface for iface, addrs in psutil.net_if_addrs().items() if any(addr.family.name == 'AF_INET' for addr in addrs)] print('Available interfaces:', interfaces) \"","title":"Interface Not Found"},{"location":"user-guide/#api-connection-issues","text":"Check if the API server is running: # Test API health curl http://localhost:8000/health # Check API documentation curl http://localhost:8000/docs","title":"API Connection Issues"},{"location":"user-guide/#high-memory-usage","text":"For long-running monitoring: Monitor the packet queue size in the UI Adjust confidence_threshold to reduce false positives Add more attack types to ignored_attacks list Increase cleanup_interval for flow cleanup","title":"High Memory Usage"},{"location":"user-guide/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"user-guide/#custom-feature-extraction","text":"The system uses CICIDS2017-compatible features. To add custom features: from network_monitor import CICIDSFeatureExtractor extractor = CICIDSFeatureExtractor() features = extractor.extract_features(packet, flow_data)","title":"Custom Feature Extraction"},{"location":"user-guide/#filtering-traffic","text":"Modify packet filters in network_monitor.py : # Example: Monitor only HTTP traffic packet_filter = \"tcp port 80 or tcp port 443\" # Example: Monitor specific subnet packet_filter = \"net 192.168.1.0/24\"","title":"Filtering Traffic"},{"location":"user-guide/#custom-attack-types","text":"Add or remove attack types from the ignore list: # In RealTimeNetworkMonitor.__init__() self.ignored_attacks = ['heartbleed', 'portscan', 'benign']","title":"Custom Attack Types"},{"location":"user-guide/installation/","text":"Installation This guide covers the installation and setup of the RL-IDS (Reinforcement Learning Intrusion Detection System). System Requirements Minimum Requirements Operating System : Linux (Ubuntu 18.04+), macOS (10.15+), or Windows 10/11 Python : 3.13.0+ (recommended: 3.13.0) Memory : 8GB RAM minimum, 16GB recommended Storage : 10GB free space for datasets and models Network : Administrative privileges for packet capture Hardware Recommendations CPU : Multi-core processor (8+ cores recommended for training) GPU : NVIDIA GPU with CUDA support (optional, for faster training) Network Interface : Ethernet adapter for network monitoring Installation Methods Method 1: pip Installation (Recommended) # Create virtual environment python -m venv rl_ids_env source rl_ids_env/bin/activate # Linux/macOS # or rl_ids_env\\Scripts\\activate # Windows # Install RL-IDS pip install rl_ids # Verify installation python -c \"import rl_ids; print('RL-IDS installed successfully')\" Method 2: Development Installation # Clone the repository git clone https://github.com/yashpotdar-py/rl-ids.git cd rl-ids # Create virtual environment python -m venv venv source venv/bin/activate # Linux/macOS # or venv\\Scripts\\activate # Windows # Install in development mode pip install -e . # Install additional dependencies pip install -r requirements.txt Method 3: Docker Installation (Coming Soon) # Pull the Docker image docker pull yashpotdar/rl-ids:latest # Run the container docker run -p 8000:8000 yashpotdar/rl-ids:latest Post-Installation Setup 1. Verify Network Permissions For network monitoring capabilities, ensure you have the necessary permissions: # Linux: Check if user can capture packets sudo setcap cap_net_raw,cap_net_admin=eip $(which python) # Or run with sudo (not recommended for production) sudo python network_monitor.py 2. Download Pre-trained Models # Download the latest DQN model (if available) python -c \" from rl_ids.modeling.train import download_pretrained_model download_pretrained_model() \" 3. Configure Environment Create a .env file in your project directory: # API Configuration API_HOST=localhost API_PORT=8000 DEBUG=true # Model Configuration MODEL_PATH=models/dqn_model_best.pt FEATURE_SCALER_PATH=models/feature_scaler.pkl # Monitoring Configuration CAPTURE_INTERFACE=eth0 LOG_LEVEL=INFO LOG_FILE=logs/rl_ids.log 4. Test Installation Run the test suite to verify everything is working: # Basic functionality test pytest tests/test_api.py -v # Complete test suite (if available) python -m pytest tests/ -v Configuration Network Interface Configuration Identify available network interfaces: # List network interfaces python -c \" import psutil interfaces = psutil.net_if_addrs() for interface, addresses in interfaces.items(): print(f'Interface: {interface}') for addr in addresses: if addr.family.name == 'AF_INET': print(f' IP: {addr.address}') \" GPU Configuration (Optional) If you have an NVIDIA GPU and want to use CUDA acceleration: # Install PyTorch with CUDA support pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 # Verify CUDA availability python -c \" import torch print(f'CUDA available: {torch.cuda.is_available()}') if torch.cuda.is_available(): print(f'CUDA version: {torch.version.cuda}') print(f'GPU device: {torch.cuda.get_device_name(0)}') \" Troubleshooting Common Issues Permission Denied Error # Error: [Errno 1] Operation not permitted # Solution: Run with appropriate permissions or set capabilities sudo setcap cap_net_raw,cap_net_admin=eip $(which python) Import Error # Error: ModuleNotFoundError: No module named 'rl_ids' # Solution: Ensure virtual environment is activated and package is installed source venv/bin/activate pip install -e . CUDA Out of Memory # Error: RuntimeError: CUDA out of memory # Solution: Reduce batch size or use CPU export CUDA_VISIBLE_DEVICES=\"\" # Force CPU usage Network Interface Not Found # Error: Interface 'eth0' not found # Solution: List available interfaces and update configuration python -c \"import psutil; print(list(psutil.net_if_addrs().keys()))\" Logging Configuration Enable detailed logging for troubleshooting: # Add to your Python script import logging from loguru import logger # Configure logging level logger.add(\"logs/debug.log\", level=\"DEBUG\", rotation=\"10 MB\") Performance Optimization For Training Use GPU acceleration when available Increase batch size if memory permits Use multiple CPU cores for data preprocessing For Monitoring Adjust capture buffer size based on network traffic Use appropriate logging levels to reduce overhead Consider using background processes for real-time monitoring Next Steps After successful installation: Read the Network Monitoring Guide to start monitoring network traffic Explore the API Documentation to integrate with existing systems Check the Agent Configuration to customize the RL model Review Development Setup if you plan to contribute Getting Help Documentation : Browse the complete documentation in this site Issues : Report bugs on the GitHub Issues page Discussions : Join community discussions on the repository Version Information Current Version : 1.2.0 Python Compatibility : 3.10.0+ Last Updated : 2025","title":"Installation"},{"location":"user-guide/installation/#installation","text":"This guide covers the installation and setup of the RL-IDS (Reinforcement Learning Intrusion Detection System).","title":"Installation"},{"location":"user-guide/installation/#system-requirements","text":"","title":"System Requirements"},{"location":"user-guide/installation/#minimum-requirements","text":"Operating System : Linux (Ubuntu 18.04+), macOS (10.15+), or Windows 10/11 Python : 3.13.0+ (recommended: 3.13.0) Memory : 8GB RAM minimum, 16GB recommended Storage : 10GB free space for datasets and models Network : Administrative privileges for packet capture","title":"Minimum Requirements"},{"location":"user-guide/installation/#hardware-recommendations","text":"CPU : Multi-core processor (8+ cores recommended for training) GPU : NVIDIA GPU with CUDA support (optional, for faster training) Network Interface : Ethernet adapter for network monitoring","title":"Hardware Recommendations"},{"location":"user-guide/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"user-guide/installation/#method-1-pip-installation-recommended","text":"# Create virtual environment python -m venv rl_ids_env source rl_ids_env/bin/activate # Linux/macOS # or rl_ids_env\\Scripts\\activate # Windows # Install RL-IDS pip install rl_ids # Verify installation python -c \"import rl_ids; print('RL-IDS installed successfully')\"","title":"Method 1: pip Installation (Recommended)"},{"location":"user-guide/installation/#method-2-development-installation","text":"# Clone the repository git clone https://github.com/yashpotdar-py/rl-ids.git cd rl-ids # Create virtual environment python -m venv venv source venv/bin/activate # Linux/macOS # or venv\\Scripts\\activate # Windows # Install in development mode pip install -e . # Install additional dependencies pip install -r requirements.txt","title":"Method 2: Development Installation"},{"location":"user-guide/installation/#method-3-docker-installation-coming-soon","text":"# Pull the Docker image docker pull yashpotdar/rl-ids:latest # Run the container docker run -p 8000:8000 yashpotdar/rl-ids:latest","title":"Method 3: Docker Installation (Coming Soon)"},{"location":"user-guide/installation/#post-installation-setup","text":"","title":"Post-Installation Setup"},{"location":"user-guide/installation/#1-verify-network-permissions","text":"For network monitoring capabilities, ensure you have the necessary permissions: # Linux: Check if user can capture packets sudo setcap cap_net_raw,cap_net_admin=eip $(which python) # Or run with sudo (not recommended for production) sudo python network_monitor.py","title":"1. Verify Network Permissions"},{"location":"user-guide/installation/#2-download-pre-trained-models","text":"# Download the latest DQN model (if available) python -c \" from rl_ids.modeling.train import download_pretrained_model download_pretrained_model() \"","title":"2. Download Pre-trained Models"},{"location":"user-guide/installation/#3-configure-environment","text":"Create a .env file in your project directory: # API Configuration API_HOST=localhost API_PORT=8000 DEBUG=true # Model Configuration MODEL_PATH=models/dqn_model_best.pt FEATURE_SCALER_PATH=models/feature_scaler.pkl # Monitoring Configuration CAPTURE_INTERFACE=eth0 LOG_LEVEL=INFO LOG_FILE=logs/rl_ids.log","title":"3. Configure Environment"},{"location":"user-guide/installation/#4-test-installation","text":"Run the test suite to verify everything is working: # Basic functionality test pytest tests/test_api.py -v # Complete test suite (if available) python -m pytest tests/ -v","title":"4. Test Installation"},{"location":"user-guide/installation/#configuration","text":"","title":"Configuration"},{"location":"user-guide/installation/#network-interface-configuration","text":"Identify available network interfaces: # List network interfaces python -c \" import psutil interfaces = psutil.net_if_addrs() for interface, addresses in interfaces.items(): print(f'Interface: {interface}') for addr in addresses: if addr.family.name == 'AF_INET': print(f' IP: {addr.address}') \"","title":"Network Interface Configuration"},{"location":"user-guide/installation/#gpu-configuration-optional","text":"If you have an NVIDIA GPU and want to use CUDA acceleration: # Install PyTorch with CUDA support pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 # Verify CUDA availability python -c \" import torch print(f'CUDA available: {torch.cuda.is_available()}') if torch.cuda.is_available(): print(f'CUDA version: {torch.version.cuda}') print(f'GPU device: {torch.cuda.get_device_name(0)}') \"","title":"GPU Configuration (Optional)"},{"location":"user-guide/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/installation/#common-issues","text":"","title":"Common Issues"},{"location":"user-guide/installation/#permission-denied-error","text":"# Error: [Errno 1] Operation not permitted # Solution: Run with appropriate permissions or set capabilities sudo setcap cap_net_raw,cap_net_admin=eip $(which python)","title":"Permission Denied Error"},{"location":"user-guide/installation/#import-error","text":"# Error: ModuleNotFoundError: No module named 'rl_ids' # Solution: Ensure virtual environment is activated and package is installed source venv/bin/activate pip install -e .","title":"Import Error"},{"location":"user-guide/installation/#cuda-out-of-memory","text":"# Error: RuntimeError: CUDA out of memory # Solution: Reduce batch size or use CPU export CUDA_VISIBLE_DEVICES=\"\" # Force CPU usage","title":"CUDA Out of Memory"},{"location":"user-guide/installation/#network-interface-not-found","text":"# Error: Interface 'eth0' not found # Solution: List available interfaces and update configuration python -c \"import psutil; print(list(psutil.net_if_addrs().keys()))\"","title":"Network Interface Not Found"},{"location":"user-guide/installation/#logging-configuration","text":"Enable detailed logging for troubleshooting: # Add to your Python script import logging from loguru import logger # Configure logging level logger.add(\"logs/debug.log\", level=\"DEBUG\", rotation=\"10 MB\")","title":"Logging Configuration"},{"location":"user-guide/installation/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"user-guide/installation/#for-training","text":"Use GPU acceleration when available Increase batch size if memory permits Use multiple CPU cores for data preprocessing","title":"For Training"},{"location":"user-guide/installation/#for-monitoring","text":"Adjust capture buffer size based on network traffic Use appropriate logging levels to reduce overhead Consider using background processes for real-time monitoring","title":"For Monitoring"},{"location":"user-guide/installation/#next-steps","text":"After successful installation: Read the Network Monitoring Guide to start monitoring network traffic Explore the API Documentation to integrate with existing systems Check the Agent Configuration to customize the RL model Review Development Setup if you plan to contribute","title":"Next Steps"},{"location":"user-guide/installation/#getting-help","text":"Documentation : Browse the complete documentation in this site Issues : Report bugs on the GitHub Issues page Discussions : Join community discussions on the repository","title":"Getting Help"},{"location":"user-guide/installation/#version-information","text":"Current Version : 1.2.0 Python Compatibility : 3.10.0+ Last Updated : 2025","title":"Version Information"},{"location":"user-guide/network-monitoring/","text":"Network Monitoring This guide covers real-time network interface monitoring using RL-IDS. Overview The network_monitor.py script provides comprehensive real-time monitoring of network interfaces, analyzing all network traffic and detecting potential intrusions using the trained DQN model. Features Real-time packet capture using Scapy CICIDS2017 feature extraction from network flows DQN-based threat detection via API integration Flow-based analysis with stateful connection tracking Interactive terminal interface with live statistics Configurable attack filtering and thresholds Comprehensive logging with JSON and text formats Basic Usage Start Monitoring # Monitor default interface (auto-detected) sudo python network_monitor.py # Monitor specific interface sudo python network_monitor.py eth0 # Monitor with custom API endpoint sudo python network_monitor.py wlan0 --api-url http://localhost:8000 Command Line Options python network_monitor.py [INTERFACE] [OPTIONS] Arguments: INTERFACE Network interface to monitor (default: auto-detect) Options: --api-url TEXT RL-IDS API endpoint URL [default: http://localhost:8000] --help Show help message and exit User Interface The monitor provides a real-time terminal interface with: Statistics Panel Uptime : How long the monitor has been running Packets : Total packets processed Rate : Packets per minute Active Flows : Number of tracked network flows Attacks : Total attacks detected Ignored : Attacks filtered out based on configuration Queue Size : Async processing queue size Configuration Panel Interface : Currently monitored network interface API URL : RL-IDS API endpoint Threshold : Confidence threshold for attack detection Status : Current monitoring status Recent Alerts Real-time display of detected attacks with: - Timestamp - Attack type - Source IP address - Confidence score Network Flow Analysis The monitor implements stateful flow tracking: Flow Identification Flows are identified by the 5-tuple: - Source IP - Destination IP - Source Port - Destination Port - Protocol Flow Statistics For each flow, the system tracks: - Packet counts (forward/backward direction) - Byte counts (forward/backward direction) - Timing information (start time, inter-arrival times) - TCP flags (if applicable) - Packet sizes and statistical measures Feature Extraction The CICIDSFeatureExtractor class extracts 78 features from each flow: class CICIDSFeatureExtractor: \"\"\"Extract CICIDS2017-compatible features from network packets\"\"\" def __init__(self): self.feature_names = [ 'flow_duration', 'total_fwd_packets', 'total_bwd_packets', 'total_length_fwd_packets', 'total_length_bwd_packets', 'fwd_packet_length_max', 'fwd_packet_length_min', 'fwd_packet_length_mean', 'fwd_packet_length_std', # ... 69 more features ] def extract_features(self, packet, flow_data): \"\"\"Extract 78 CICIDS2017 features from packet and flow data\"\"\" # Implementation extracts statistical features from flow # Returns list of 78 float values Detection Process 1. Packet Capture Captures packets on specified interface using Scapy Filters packets based on configurable criteria Handles multiple protocols (TCP, UDP, ICMP) 2. Flow Processing Groups packets into bidirectional flows Maintains flow state and statistics Calculates timing and size metrics 3. Feature Extraction Extracts 78 CICIDS2017 features per flow Handles edge cases and missing data Normalizes feature values 4. Threat Detection Sends features to RL-IDS API for classification Receives prediction with confidence score Applies configured threshold for alerting 5. Alert Processing Logs detected attacks with full context Applies ignore filters for known false positives Updates real-time interface with new alerts Configuration Attack Filtering Configure which attack types to ignore: self.ignored_attacks = [ 'heartbleed', # Often false positives 'portscan', # Too noisy for some environments 'benign' # Normal traffic ] Confidence Threshold Set minimum confidence for attack alerts: self.confidence_threshold = 0.7 # 70% confidence minimum Network Interface Selection The monitor can auto-detect the primary network interface or use a specified one: def get_available_interfaces(self): \"\"\"Get list of available network interfaces with IP addresses\"\"\" interfaces = [] for iface, addrs in psutil.net_if_addrs().items(): for addr in addrs: if addr.family.name == 'AF_INET' and not addr.address.startswith('127.'): interfaces.append(iface) return list(set(interfaces)) Logging The monitor generates several log files: network_monitor.log General operational logs: 2025-06-27 10:30:15 | INFO | Starting network monitoring on interface wlan0 2025-06-27 10:30:16 | INFO | API connection established: http://localhost:8000 2025-06-27 10:30:45 | WARNING | High packet rate detected: 150 pps intrusion_alerts.log Detailed attack information: 2025-06-27 10:31:22 | CRITICAL | ATTACK DETECTED: DoS Hulk Source: 192.168.1.100:45123 -> 192.168.1.1:80 Confidence: 87.5% Flow Duration: 12.3s Features: [0.123, 0.456, ...] alerts.json Machine-readable alert data: { \"timestamp\": \"2025-06-27T10:31:22.123456\", \"attack_type\": \"DoS Hulk\", \"source_ip\": \"192.168.1.100\", \"dest_ip\": \"192.168.1.1\", \"confidence\": 0.875, \"flow_key\": \"192.168.1.100:45123-192.168.1.1:80-TCP\", \"features\": [0.123, 0.456, ...] } Troubleshooting Common Issues Permission Denied # Solution: Run with sudo or set capabilities sudo python network_monitor.py # OR sudo setcap cap_net_raw,cap_net_admin=eip $(which python3) Interface Not Found # List available interfaces python -c \"import psutil; print(list(psutil.net_if_addrs().keys()))\" API Connection Failed # Check API server status curl http://localhost:8000/health High Memory Usage - Reduce max_flows parameter - Increase flow_timeout for faster cleanup - Add more attack types to ignore list Packet Drops - Increase system buffer sizes - Reduce feature extraction frequency - Use dedicated monitoring hardware Debug Mode Enable verbose logging for troubleshooting: # Set environment variable for debug logging export RLIDS_DEBUG=true export RLIDS_LOG_LEVEL=DEBUG sudo python network_monitor.py wlan0","title":"Network Monitoring"},{"location":"user-guide/network-monitoring/#network-monitoring","text":"This guide covers real-time network interface monitoring using RL-IDS.","title":"Network Monitoring"},{"location":"user-guide/network-monitoring/#overview","text":"The network_monitor.py script provides comprehensive real-time monitoring of network interfaces, analyzing all network traffic and detecting potential intrusions using the trained DQN model.","title":"Overview"},{"location":"user-guide/network-monitoring/#features","text":"Real-time packet capture using Scapy CICIDS2017 feature extraction from network flows DQN-based threat detection via API integration Flow-based analysis with stateful connection tracking Interactive terminal interface with live statistics Configurable attack filtering and thresholds Comprehensive logging with JSON and text formats","title":"Features"},{"location":"user-guide/network-monitoring/#basic-usage","text":"","title":"Basic Usage"},{"location":"user-guide/network-monitoring/#start-monitoring","text":"# Monitor default interface (auto-detected) sudo python network_monitor.py # Monitor specific interface sudo python network_monitor.py eth0 # Monitor with custom API endpoint sudo python network_monitor.py wlan0 --api-url http://localhost:8000","title":"Start Monitoring"},{"location":"user-guide/network-monitoring/#command-line-options","text":"python network_monitor.py [INTERFACE] [OPTIONS] Arguments: INTERFACE Network interface to monitor (default: auto-detect) Options: --api-url TEXT RL-IDS API endpoint URL [default: http://localhost:8000] --help Show help message and exit","title":"Command Line Options"},{"location":"user-guide/network-monitoring/#user-interface","text":"The monitor provides a real-time terminal interface with:","title":"User Interface"},{"location":"user-guide/network-monitoring/#statistics-panel","text":"Uptime : How long the monitor has been running Packets : Total packets processed Rate : Packets per minute Active Flows : Number of tracked network flows Attacks : Total attacks detected Ignored : Attacks filtered out based on configuration Queue Size : Async processing queue size","title":"Statistics Panel"},{"location":"user-guide/network-monitoring/#configuration-panel","text":"Interface : Currently monitored network interface API URL : RL-IDS API endpoint Threshold : Confidence threshold for attack detection Status : Current monitoring status","title":"Configuration Panel"},{"location":"user-guide/network-monitoring/#recent-alerts","text":"Real-time display of detected attacks with: - Timestamp - Attack type - Source IP address - Confidence score","title":"Recent Alerts"},{"location":"user-guide/network-monitoring/#network-flow-analysis","text":"The monitor implements stateful flow tracking:","title":"Network Flow Analysis"},{"location":"user-guide/network-monitoring/#flow-identification","text":"Flows are identified by the 5-tuple: - Source IP - Destination IP - Source Port - Destination Port - Protocol","title":"Flow Identification"},{"location":"user-guide/network-monitoring/#flow-statistics","text":"For each flow, the system tracks: - Packet counts (forward/backward direction) - Byte counts (forward/backward direction) - Timing information (start time, inter-arrival times) - TCP flags (if applicable) - Packet sizes and statistical measures","title":"Flow Statistics"},{"location":"user-guide/network-monitoring/#feature-extraction","text":"The CICIDSFeatureExtractor class extracts 78 features from each flow: class CICIDSFeatureExtractor: \"\"\"Extract CICIDS2017-compatible features from network packets\"\"\" def __init__(self): self.feature_names = [ 'flow_duration', 'total_fwd_packets', 'total_bwd_packets', 'total_length_fwd_packets', 'total_length_bwd_packets', 'fwd_packet_length_max', 'fwd_packet_length_min', 'fwd_packet_length_mean', 'fwd_packet_length_std', # ... 69 more features ] def extract_features(self, packet, flow_data): \"\"\"Extract 78 CICIDS2017 features from packet and flow data\"\"\" # Implementation extracts statistical features from flow # Returns list of 78 float values","title":"Feature Extraction"},{"location":"user-guide/network-monitoring/#detection-process","text":"","title":"Detection Process"},{"location":"user-guide/network-monitoring/#1-packet-capture","text":"Captures packets on specified interface using Scapy Filters packets based on configurable criteria Handles multiple protocols (TCP, UDP, ICMP)","title":"1. Packet Capture"},{"location":"user-guide/network-monitoring/#2-flow-processing","text":"Groups packets into bidirectional flows Maintains flow state and statistics Calculates timing and size metrics","title":"2. Flow Processing"},{"location":"user-guide/network-monitoring/#3-feature-extraction","text":"Extracts 78 CICIDS2017 features per flow Handles edge cases and missing data Normalizes feature values","title":"3. Feature Extraction"},{"location":"user-guide/network-monitoring/#4-threat-detection","text":"Sends features to RL-IDS API for classification Receives prediction with confidence score Applies configured threshold for alerting","title":"4. Threat Detection"},{"location":"user-guide/network-monitoring/#5-alert-processing","text":"Logs detected attacks with full context Applies ignore filters for known false positives Updates real-time interface with new alerts","title":"5. Alert Processing"},{"location":"user-guide/network-monitoring/#configuration","text":"","title":"Configuration"},{"location":"user-guide/network-monitoring/#attack-filtering","text":"Configure which attack types to ignore: self.ignored_attacks = [ 'heartbleed', # Often false positives 'portscan', # Too noisy for some environments 'benign' # Normal traffic ]","title":"Attack Filtering"},{"location":"user-guide/network-monitoring/#confidence-threshold","text":"Set minimum confidence for attack alerts: self.confidence_threshold = 0.7 # 70% confidence minimum","title":"Confidence Threshold"},{"location":"user-guide/network-monitoring/#network-interface-selection","text":"The monitor can auto-detect the primary network interface or use a specified one: def get_available_interfaces(self): \"\"\"Get list of available network interfaces with IP addresses\"\"\" interfaces = [] for iface, addrs in psutil.net_if_addrs().items(): for addr in addrs: if addr.family.name == 'AF_INET' and not addr.address.startswith('127.'): interfaces.append(iface) return list(set(interfaces))","title":"Network Interface Selection"},{"location":"user-guide/network-monitoring/#logging","text":"The monitor generates several log files:","title":"Logging"},{"location":"user-guide/network-monitoring/#network_monitorlog","text":"General operational logs: 2025-06-27 10:30:15 | INFO | Starting network monitoring on interface wlan0 2025-06-27 10:30:16 | INFO | API connection established: http://localhost:8000 2025-06-27 10:30:45 | WARNING | High packet rate detected: 150 pps","title":"network_monitor.log"},{"location":"user-guide/network-monitoring/#intrusion_alertslog","text":"Detailed attack information: 2025-06-27 10:31:22 | CRITICAL | ATTACK DETECTED: DoS Hulk Source: 192.168.1.100:45123 -> 192.168.1.1:80 Confidence: 87.5% Flow Duration: 12.3s Features: [0.123, 0.456, ...]","title":"intrusion_alerts.log"},{"location":"user-guide/network-monitoring/#alertsjson","text":"Machine-readable alert data: { \"timestamp\": \"2025-06-27T10:31:22.123456\", \"attack_type\": \"DoS Hulk\", \"source_ip\": \"192.168.1.100\", \"dest_ip\": \"192.168.1.1\", \"confidence\": 0.875, \"flow_key\": \"192.168.1.100:45123-192.168.1.1:80-TCP\", \"features\": [0.123, 0.456, ...] }","title":"alerts.json"},{"location":"user-guide/network-monitoring/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/network-monitoring/#common-issues","text":"Permission Denied # Solution: Run with sudo or set capabilities sudo python network_monitor.py # OR sudo setcap cap_net_raw,cap_net_admin=eip $(which python3) Interface Not Found # List available interfaces python -c \"import psutil; print(list(psutil.net_if_addrs().keys()))\" API Connection Failed # Check API server status curl http://localhost:8000/health High Memory Usage - Reduce max_flows parameter - Increase flow_timeout for faster cleanup - Add more attack types to ignore list Packet Drops - Increase system buffer sizes - Reduce feature extraction frequency - Use dedicated monitoring hardware","title":"Common Issues"},{"location":"user-guide/network-monitoring/#debug-mode","text":"Enable verbose logging for troubleshooting: # Set environment variable for debug logging export RLIDS_DEBUG=true export RLIDS_LOG_LEVEL=DEBUG sudo python network_monitor.py wlan0","title":"Debug Mode"},{"location":"user-guide/website-monitoring/","text":"Website Monitoring This guide covers website-specific traffic monitoring using RL-IDS to analyze communication with specific domains. Overview The website_monitor.py script provides targeted monitoring of network traffic to and from specific websites. It generates controlled traffic, captures the responses, and analyzes the communication patterns for potential threats. Features Domain-specific monitoring with automatic IP resolution Controlled traffic generation using HTTP requests Packet capture and analysis for target communications Real-time threat detection via RL-IDS API integration Interactive monitoring interface with live statistics Comprehensive logging of traffic and detections Basic Usage Start Website Monitoring # Monitor a specific domain python website_monitor.py example.com # Monitor with custom API endpoint python website_monitor.py example.com --api-url http://localhost:8000 # Monitor with specific network interface python website_monitor.py example.com --interface wlan0 Command Line Options python website_monitor.py TARGET [OPTIONS] Arguments: TARGET Target domain/website to monitor (required) Options: --api-url TEXT RL-IDS API endpoint URL [default: http://localhost:8000] --interface TEXT Network interface to use [default: wlan0] --help Show help message and exit How It Works 1. Domain Resolution The monitor resolves the target domain to IP addresses: def resolve_domain(self): \"\"\"Resolve target domain to IP address\"\"\" try: result = socket.getaddrinfo(self.target_domain, None) ip_addresses = list(set([res[4][0] for res in result if ':' not in res[4][0]])) self.logger.info(f\"Resolved {self.target_domain} to: {ip_addresses}\") return ip_addresses[0] if ip_addresses else None except Exception as e: self.logger.error(f\"Failed to resolve {self.target_domain}: {e}\") return None 2. Traffic Generation Generates HTTP requests to the target domain: async def generate_traffic(self): \"\"\"Generate HTTP traffic to target domain\"\"\" try: async with aiohttp.ClientSession() as session: url = f\"https://{self.target_domain}\" async with session.get(url, timeout=10) as response: await response.text() self.logger.info(f\"Generated traffic to {url} - Status: {response.status}\") except Exception as e: self.logger.warning(f\"Traffic generation failed: {e}\") 3. Packet Capture Captures network packets to/from the target IP: def start_packet_capture(self, target_ip): \"\"\"Start packet capture for target IP\"\"\" packet_filter = f\"host {target_ip}\" def packet_handler(packet): if IP in packet: # Store packet data for analysis packet_data = { 'timestamp': datetime.now(), 'src_ip': packet[IP].src, 'dst_ip': packet[IP].dst, 'protocol': packet[IP].proto, 'size': len(packet) } self.captured_packets.append(packet_data) # Start packet capture sniff(iface=self.interface, filter=packet_filter, prn=packet_handler, timeout=self.capture_duration) 4. Traffic Analysis Analyzes captured traffic using CICIDS2017 features: async def analyze_captured_traffic(self): \"\"\"Analyze captured traffic for threats\"\"\" if not self.captured_packets: return # Group packets into flows flows = self.group_packets_into_flows() for flow_key, flow_data in flows.items(): # Extract features using CICIDSFeatureExtractor features = self.feature_extractor.extract_features(None, flow_data) # Send to API for analysis try: prediction = await self.client.predict(features) if prediction['is_attack']: self.log_attack(prediction, flow_key) except Exception as e: self.logger.error(f\"Analysis failed for flow {flow_key}: {e}\") Monitoring Cycle The website monitor operates in cycles: 1. Traffic Generation Phase Sends HTTP requests to target domain Uses various request types (GET, POST if applicable) Handles SSL/TLS connections 2. Capture Phase Captures packets for configured duration (default: 5 seconds) Filters packets to target IP addresses only Stores packet metadata and payloads 3. Analysis Phase Groups packets into bidirectional flows Extracts CICIDS2017 features from flows Sends features to RL-IDS API for classification 4. Reporting Phase Logs detected attacks with full context Updates monitoring interface Prepares for next cycle Configuration Monitoring Parameters class WebsiteMonitor: def __init__(self, target_domain, api_url=\"http://localhost:8000\", interface=\"wlan0\"): # Traffic generation settings self.request_interval = 10 # seconds between requests self.capture_duration = 5 # seconds to capture after each request Customizable Settings Request Interval : Time between traffic generation cycles self.request_interval = 10 # 10 seconds between requests Capture Duration : How long to capture packets after each request self.capture_duration = 5 # 5 seconds of capture Network Interface : Which interface to monitor self.interface = \"wlan0\" # or eth0, etc. User Interface The website monitor provides a real-time interface showing: Target Information Target domain name Resolved IP addresses Current monitoring status Traffic Statistics Requests generated Packets captured Flows analyzed Attacks detected Recent Activity Last request timestamp Recent packet captures Latest analysis results Detected threats Logging website_monitor.log General monitoring activities: 2025-06-27 10:30:15 | INFO | Starting website monitoring for example.com 2025-06-27 10:30:16 | INFO | Resolved example.com to 93.184.216.34 2025-06-27 10:30:20 | INFO | Generated traffic to https://example.com - Status: 200 2025-06-27 10:30:25 | INFO | Captured 15 packets for analysis Alert Logs Detected threats are logged with context: 2025-06-27 10:31:15 | WARNING | POTENTIAL THREAT DETECTED Target: example.com (93.184.216.34) Attack Type: DoS Hulk Confidence: 78.5% Flow: 192.168.1.100:45123 -> 93.184.216.34:443 JSON Logs Machine-readable monitoring data: { \"timestamp\": \"2025-06-27T10:31:15.123456\", \"target_domain\": \"example.com\", \"target_ip\": \"93.184.216.34\", \"attack_detected\": true, \"attack_type\": \"DoS Hulk\", \"confidence\": 0.785, \"flow_details\": { \"src_ip\": \"192.168.1.100\", \"dst_ip\": \"93.184.216.34\", \"protocol\": \"TCP\", \"packets\": 12, \"bytes\": 8456 } } Use Cases 1. Website Security Assessment Monitor your own websites for attack patterns: python website_monitor.py mywebsite.com 2. Third-party Service Monitoring Analyze communication with external services: python website_monitor.py api.thirdpartyservice.com 3. Suspicious Domain Investigation Investigate potentially malicious domains: python website_monitor.py suspicious-domain.example 4. Network Baseline Establishment Establish normal communication patterns: python website_monitor.py trusted-service.com Advanced Configuration Custom Request Headers Modify traffic generation to include custom headers: headers = { 'User-Agent': 'RL-IDS Website Monitor', 'Accept': 'text/html,application/json', 'Custom-Header': 'monitoring-traffic' } Multiple Target IPs Handle domains with multiple IP addresses: def resolve_all_ips(self, domain): \"\"\"Resolve domain to all IP addresses\"\"\" result = socket.getaddrinfo(domain, None) return list(set([res[4][0] for res in result if ':' not in res[4][0]])) Protocol-Specific Monitoring Monitor specific protocols beyond HTTP: # Monitor HTTPS traffic only packet_filter = f\"host {target_ip} and port 443\" # Monitor all traffic to domain packet_filter = f\"host {target_ip}\" # Monitor specific port ranges packet_filter = f\"host {target_ip} and portrange 80-443\" Troubleshooting Common Issues Domain Resolution Failed # Check DNS resolution nslookup example.com dig example.com No Packets Captured - Verify network interface is correct - Check if target domain is reachable - Ensure sufficient privileges for packet capture High False Positive Rate - Adjust confidence threshold - Add domain to whitelist if needed - Verify baseline traffic patterns Connection Timeouts - Increase request timeout - Check network connectivity - Verify firewall rules Debug Mode Enable verbose logging: export RLIDS_DEBUG=true export RLIDS_LOG_LEVEL=DEBUG python website_monitor.py example.com Testing Network Connectivity # Test basic connectivity ping example.com # Test HTTP connectivity curl -I https://example.com # Test with specific interface curl --interface wlan0 https://example.com","title":"Website Monitoring"},{"location":"user-guide/website-monitoring/#website-monitoring","text":"This guide covers website-specific traffic monitoring using RL-IDS to analyze communication with specific domains.","title":"Website Monitoring"},{"location":"user-guide/website-monitoring/#overview","text":"The website_monitor.py script provides targeted monitoring of network traffic to and from specific websites. It generates controlled traffic, captures the responses, and analyzes the communication patterns for potential threats.","title":"Overview"},{"location":"user-guide/website-monitoring/#features","text":"Domain-specific monitoring with automatic IP resolution Controlled traffic generation using HTTP requests Packet capture and analysis for target communications Real-time threat detection via RL-IDS API integration Interactive monitoring interface with live statistics Comprehensive logging of traffic and detections","title":"Features"},{"location":"user-guide/website-monitoring/#basic-usage","text":"","title":"Basic Usage"},{"location":"user-guide/website-monitoring/#start-website-monitoring","text":"# Monitor a specific domain python website_monitor.py example.com # Monitor with custom API endpoint python website_monitor.py example.com --api-url http://localhost:8000 # Monitor with specific network interface python website_monitor.py example.com --interface wlan0","title":"Start Website Monitoring"},{"location":"user-guide/website-monitoring/#command-line-options","text":"python website_monitor.py TARGET [OPTIONS] Arguments: TARGET Target domain/website to monitor (required) Options: --api-url TEXT RL-IDS API endpoint URL [default: http://localhost:8000] --interface TEXT Network interface to use [default: wlan0] --help Show help message and exit","title":"Command Line Options"},{"location":"user-guide/website-monitoring/#how-it-works","text":"","title":"How It Works"},{"location":"user-guide/website-monitoring/#1-domain-resolution","text":"The monitor resolves the target domain to IP addresses: def resolve_domain(self): \"\"\"Resolve target domain to IP address\"\"\" try: result = socket.getaddrinfo(self.target_domain, None) ip_addresses = list(set([res[4][0] for res in result if ':' not in res[4][0]])) self.logger.info(f\"Resolved {self.target_domain} to: {ip_addresses}\") return ip_addresses[0] if ip_addresses else None except Exception as e: self.logger.error(f\"Failed to resolve {self.target_domain}: {e}\") return None","title":"1. Domain Resolution"},{"location":"user-guide/website-monitoring/#2-traffic-generation","text":"Generates HTTP requests to the target domain: async def generate_traffic(self): \"\"\"Generate HTTP traffic to target domain\"\"\" try: async with aiohttp.ClientSession() as session: url = f\"https://{self.target_domain}\" async with session.get(url, timeout=10) as response: await response.text() self.logger.info(f\"Generated traffic to {url} - Status: {response.status}\") except Exception as e: self.logger.warning(f\"Traffic generation failed: {e}\")","title":"2. Traffic Generation"},{"location":"user-guide/website-monitoring/#3-packet-capture","text":"Captures network packets to/from the target IP: def start_packet_capture(self, target_ip): \"\"\"Start packet capture for target IP\"\"\" packet_filter = f\"host {target_ip}\" def packet_handler(packet): if IP in packet: # Store packet data for analysis packet_data = { 'timestamp': datetime.now(), 'src_ip': packet[IP].src, 'dst_ip': packet[IP].dst, 'protocol': packet[IP].proto, 'size': len(packet) } self.captured_packets.append(packet_data) # Start packet capture sniff(iface=self.interface, filter=packet_filter, prn=packet_handler, timeout=self.capture_duration)","title":"3. Packet Capture"},{"location":"user-guide/website-monitoring/#4-traffic-analysis","text":"Analyzes captured traffic using CICIDS2017 features: async def analyze_captured_traffic(self): \"\"\"Analyze captured traffic for threats\"\"\" if not self.captured_packets: return # Group packets into flows flows = self.group_packets_into_flows() for flow_key, flow_data in flows.items(): # Extract features using CICIDSFeatureExtractor features = self.feature_extractor.extract_features(None, flow_data) # Send to API for analysis try: prediction = await self.client.predict(features) if prediction['is_attack']: self.log_attack(prediction, flow_key) except Exception as e: self.logger.error(f\"Analysis failed for flow {flow_key}: {e}\")","title":"4. Traffic Analysis"},{"location":"user-guide/website-monitoring/#monitoring-cycle","text":"The website monitor operates in cycles:","title":"Monitoring Cycle"},{"location":"user-guide/website-monitoring/#1-traffic-generation-phase","text":"Sends HTTP requests to target domain Uses various request types (GET, POST if applicable) Handles SSL/TLS connections","title":"1. Traffic Generation Phase"},{"location":"user-guide/website-monitoring/#2-capture-phase","text":"Captures packets for configured duration (default: 5 seconds) Filters packets to target IP addresses only Stores packet metadata and payloads","title":"2. Capture Phase"},{"location":"user-guide/website-monitoring/#3-analysis-phase","text":"Groups packets into bidirectional flows Extracts CICIDS2017 features from flows Sends features to RL-IDS API for classification","title":"3. Analysis Phase"},{"location":"user-guide/website-monitoring/#4-reporting-phase","text":"Logs detected attacks with full context Updates monitoring interface Prepares for next cycle","title":"4. Reporting Phase"},{"location":"user-guide/website-monitoring/#configuration","text":"","title":"Configuration"},{"location":"user-guide/website-monitoring/#monitoring-parameters","text":"class WebsiteMonitor: def __init__(self, target_domain, api_url=\"http://localhost:8000\", interface=\"wlan0\"): # Traffic generation settings self.request_interval = 10 # seconds between requests self.capture_duration = 5 # seconds to capture after each request","title":"Monitoring Parameters"},{"location":"user-guide/website-monitoring/#customizable-settings","text":"Request Interval : Time between traffic generation cycles self.request_interval = 10 # 10 seconds between requests Capture Duration : How long to capture packets after each request self.capture_duration = 5 # 5 seconds of capture Network Interface : Which interface to monitor self.interface = \"wlan0\" # or eth0, etc.","title":"Customizable Settings"},{"location":"user-guide/website-monitoring/#user-interface","text":"The website monitor provides a real-time interface showing:","title":"User Interface"},{"location":"user-guide/website-monitoring/#target-information","text":"Target domain name Resolved IP addresses Current monitoring status","title":"Target Information"},{"location":"user-guide/website-monitoring/#traffic-statistics","text":"Requests generated Packets captured Flows analyzed Attacks detected","title":"Traffic Statistics"},{"location":"user-guide/website-monitoring/#recent-activity","text":"Last request timestamp Recent packet captures Latest analysis results Detected threats","title":"Recent Activity"},{"location":"user-guide/website-monitoring/#logging","text":"","title":"Logging"},{"location":"user-guide/website-monitoring/#website_monitorlog","text":"General monitoring activities: 2025-06-27 10:30:15 | INFO | Starting website monitoring for example.com 2025-06-27 10:30:16 | INFO | Resolved example.com to 93.184.216.34 2025-06-27 10:30:20 | INFO | Generated traffic to https://example.com - Status: 200 2025-06-27 10:30:25 | INFO | Captured 15 packets for analysis","title":"website_monitor.log"},{"location":"user-guide/website-monitoring/#alert-logs","text":"Detected threats are logged with context: 2025-06-27 10:31:15 | WARNING | POTENTIAL THREAT DETECTED Target: example.com (93.184.216.34) Attack Type: DoS Hulk Confidence: 78.5% Flow: 192.168.1.100:45123 -> 93.184.216.34:443","title":"Alert Logs"},{"location":"user-guide/website-monitoring/#json-logs","text":"Machine-readable monitoring data: { \"timestamp\": \"2025-06-27T10:31:15.123456\", \"target_domain\": \"example.com\", \"target_ip\": \"93.184.216.34\", \"attack_detected\": true, \"attack_type\": \"DoS Hulk\", \"confidence\": 0.785, \"flow_details\": { \"src_ip\": \"192.168.1.100\", \"dst_ip\": \"93.184.216.34\", \"protocol\": \"TCP\", \"packets\": 12, \"bytes\": 8456 } }","title":"JSON Logs"},{"location":"user-guide/website-monitoring/#use-cases","text":"","title":"Use Cases"},{"location":"user-guide/website-monitoring/#1-website-security-assessment","text":"Monitor your own websites for attack patterns: python website_monitor.py mywebsite.com","title":"1. Website Security Assessment"},{"location":"user-guide/website-monitoring/#2-third-party-service-monitoring","text":"Analyze communication with external services: python website_monitor.py api.thirdpartyservice.com","title":"2. Third-party Service Monitoring"},{"location":"user-guide/website-monitoring/#3-suspicious-domain-investigation","text":"Investigate potentially malicious domains: python website_monitor.py suspicious-domain.example","title":"3. Suspicious Domain Investigation"},{"location":"user-guide/website-monitoring/#4-network-baseline-establishment","text":"Establish normal communication patterns: python website_monitor.py trusted-service.com","title":"4. Network Baseline Establishment"},{"location":"user-guide/website-monitoring/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"user-guide/website-monitoring/#custom-request-headers","text":"Modify traffic generation to include custom headers: headers = { 'User-Agent': 'RL-IDS Website Monitor', 'Accept': 'text/html,application/json', 'Custom-Header': 'monitoring-traffic' }","title":"Custom Request Headers"},{"location":"user-guide/website-monitoring/#multiple-target-ips","text":"Handle domains with multiple IP addresses: def resolve_all_ips(self, domain): \"\"\"Resolve domain to all IP addresses\"\"\" result = socket.getaddrinfo(domain, None) return list(set([res[4][0] for res in result if ':' not in res[4][0]]))","title":"Multiple Target IPs"},{"location":"user-guide/website-monitoring/#protocol-specific-monitoring","text":"Monitor specific protocols beyond HTTP: # Monitor HTTPS traffic only packet_filter = f\"host {target_ip} and port 443\" # Monitor all traffic to domain packet_filter = f\"host {target_ip}\" # Monitor specific port ranges packet_filter = f\"host {target_ip} and portrange 80-443\"","title":"Protocol-Specific Monitoring"},{"location":"user-guide/website-monitoring/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/website-monitoring/#common-issues","text":"Domain Resolution Failed # Check DNS resolution nslookup example.com dig example.com No Packets Captured - Verify network interface is correct - Check if target domain is reachable - Ensure sufficient privileges for packet capture High False Positive Rate - Adjust confidence threshold - Add domain to whitelist if needed - Verify baseline traffic patterns Connection Timeouts - Increase request timeout - Check network connectivity - Verify firewall rules","title":"Common Issues"},{"location":"user-guide/website-monitoring/#debug-mode","text":"Enable verbose logging: export RLIDS_DEBUG=true export RLIDS_LOG_LEVEL=DEBUG python website_monitor.py example.com","title":"Debug Mode"},{"location":"user-guide/website-monitoring/#testing-network-connectivity","text":"# Test basic connectivity ping example.com # Test HTTP connectivity curl -I https://example.com # Test with specific interface curl --interface wlan0 https://example.com","title":"Testing Network Connectivity"}]}